{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} -c rdkit rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard_logger import configure, log_value\n",
    "import tensorboard\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import glob\n",
    "#import utils import Logger\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part to use my own images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transforms_=None):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.files = sorted(glob.glob('%s/*.*' % folder_path))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "img_shape = (3, 100, 100)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "n_epochs=5000 #number of epochs of training\n",
    "batch_size=400 #size of the batches\n",
    "lr=0.0002 #adam: learning rate\n",
    "b1=0.5  #\"adam: decay of first order momentum of gradient\")\n",
    "b2=0.999 #adam: decay of first order momentum of gradient\")\n",
    "n_cpu=8\n",
    "latent_dim=100\n",
    "img_size=200\n",
    "channels=3\n",
    "n_critic=5\n",
    "clip_value=0.01\n",
    "sample_interval=1000\n",
    "img_shape = (channels, img_size, img_size)\n",
    "crop_size = 400\n",
    "print(img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"./zinc100k500px\"\n",
    "transforms_ = [ transforms.Resize(img_size),\n",
    "                transforms.CenterCrop(img_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "dataloader = DataLoader(ImageDataset(folder_path, transforms_=transforms_),\n",
    "                        batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_shape = (3, 100, 100)\n",
    "\n",
    "#folder_path = \"./molecules100\"\n",
    "#transforms_ = [ transforms.Resize(100),\n",
    "#                transforms.CenterCrop(100),\n",
    "#                transforms.ToTensor(),\n",
    "#                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "#dataloader = DataLoader(ImageDataset(folder_path, transforms_=transforms_),\n",
    "#                        batch_size=100, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(100, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Determine optimal learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual training now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b2, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6221,  0.9872,  0.2508,  ..., -0.4239,  0.2535,  0.6143],\n",
      "        [ 0.4697,  1.0298,  0.1345,  ..., -0.1841, -1.0988,  1.6070],\n",
      "        [ 1.1106, -0.5274, -0.1055,  ...,  0.2113,  0.5366, -0.5801],\n",
      "        ...,\n",
      "        [-1.0739,  0.6992, -0.1014,  ..., -0.3990,  1.0972, -0.4691],\n",
      "        [-0.7951,  0.4914, -0.8391,  ..., -0.4018, -0.5528,  0.2405],\n",
      "        [ 0.1755, -0.2522,  0.2104,  ...,  1.2836,  0.4065, -0.2441]],\n",
      "       device='cuda:0')\n",
      "100\n",
      "torch.Size([400, 3, 200, 200])\n",
      "tensor([[[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "# Sample noise as generator input\n",
    "\n",
    "imgs\n",
    "z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "print(z)\n",
    "print(latent_dim)\n",
    "print(imgs.shape)\n",
    "print(imgs)\n",
    "\n",
    "# Generate a batch of images\n",
    "fake_imgs = generator(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/500] [Batch 0/250] [D loss: 8.083148] [G loss: 0.003303]\n",
      "[Epoch 0/500] [Batch 5/250] [D loss: -351.765991] [G loss: -0.149655]\n",
      "[Epoch 0/500] [Batch 10/250] [D loss: -930.247498] [G loss: -1.183127]\n",
      "[Epoch 0/500] [Batch 15/250] [D loss: -1678.090942] [G loss: -4.038185]\n",
      "[Epoch 0/500] [Batch 20/250] [D loss: -2472.244141] [G loss: -10.242505]\n",
      "[Epoch 0/500] [Batch 25/250] [D loss: -3096.625732] [G loss: -22.473108]\n",
      "[Epoch 0/500] [Batch 30/250] [D loss: -3221.287109] [G loss: -42.861942]\n",
      "[Epoch 0/500] [Batch 35/250] [D loss: -2691.095215] [G loss: -70.594467]\n",
      "[Epoch 0/500] [Batch 40/250] [D loss: -2159.705078] [G loss: -98.534950]\n",
      "[Epoch 0/500] [Batch 45/250] [D loss: -2086.342285] [G loss: -124.022202]\n",
      "[Epoch 0/500] [Batch 50/250] [D loss: -2327.064453] [G loss: -142.505280]\n",
      "[Epoch 0/500] [Batch 55/250] [D loss: -2662.669922] [G loss: -156.052536]\n",
      "[Epoch 0/500] [Batch 60/250] [D loss: -2936.402344] [G loss: -167.244370]\n",
      "[Epoch 0/500] [Batch 65/250] [D loss: -3089.248047] [G loss: -170.740616]\n",
      "[Epoch 0/500] [Batch 70/250] [D loss: -3108.814453] [G loss: -171.966446]\n",
      "[Epoch 0/500] [Batch 75/250] [D loss: -3030.431152] [G loss: -169.082748]\n",
      "[Epoch 0/500] [Batch 80/250] [D loss: -2892.396484] [G loss: -166.818649]\n",
      "[Epoch 0/500] [Batch 85/250] [D loss: -2737.729248] [G loss: -165.104767]\n",
      "[Epoch 0/500] [Batch 90/250] [D loss: -2591.490479] [G loss: -165.517395]\n",
      "[Epoch 0/500] [Batch 95/250] [D loss: -2465.950195] [G loss: -172.917709]\n",
      "[Epoch 0/500] [Batch 100/250] [D loss: -2376.720215] [G loss: -178.056610]\n",
      "[Epoch 0/500] [Batch 105/250] [D loss: -2320.305176] [G loss: -186.992462]\n",
      "[Epoch 0/500] [Batch 110/250] [D loss: -2294.244141] [G loss: -199.581207]\n",
      "[Epoch 0/500] [Batch 115/250] [D loss: -2288.645020] [G loss: -222.701462]\n",
      "[Epoch 0/500] [Batch 120/250] [D loss: -2303.951660] [G loss: -252.936874]\n",
      "[Epoch 0/500] [Batch 125/250] [D loss: -2333.739014] [G loss: -292.607208]\n",
      "[Epoch 0/500] [Batch 130/250] [D loss: -2375.326416] [G loss: -338.394836]\n",
      "[Epoch 0/500] [Batch 135/250] [D loss: -2403.128418] [G loss: -409.745422]\n",
      "[Epoch 0/500] [Batch 140/250] [D loss: -2422.298340] [G loss: -497.658905]\n",
      "[Epoch 0/500] [Batch 145/250] [D loss: -2419.141602] [G loss: -605.924194]\n",
      "[Epoch 0/500] [Batch 150/250] [D loss: -2378.140137] [G loss: -744.018555]\n",
      "[Epoch 0/500] [Batch 155/250] [D loss: -2279.399170] [G loss: -925.517700]\n",
      "[Epoch 0/500] [Batch 160/250] [D loss: -2081.422363] [G loss: -1183.347534]\n",
      "[Epoch 0/500] [Batch 165/250] [D loss: -1791.594971] [G loss: -1506.749023]\n",
      "[Epoch 0/500] [Batch 170/250] [D loss: -1501.529541] [G loss: -1805.377319]\n",
      "[Epoch 0/500] [Batch 175/250] [D loss: -1099.308594] [G loss: -2200.521484]\n",
      "[Epoch 0/500] [Batch 180/250] [D loss: -843.005859] [G loss: -2445.129639]\n",
      "[Epoch 0/500] [Batch 185/250] [D loss: -292.964355] [G loss: -2984.756836]\n",
      "[Epoch 0/500] [Batch 190/250] [D loss: -99.284180] [G loss: -3172.952393]\n",
      "[Epoch 0/500] [Batch 195/250] [D loss: 93.377197] [G loss: -3361.247070]\n",
      "[Epoch 0/500] [Batch 200/250] [D loss: 200.093018] [G loss: -3463.437988]\n",
      "[Epoch 0/500] [Batch 205/250] [D loss: 330.538818] [G loss: -3576.125488]\n",
      "[Epoch 0/500] [Batch 210/250] [D loss: 336.828369] [G loss: -3549.598877]\n",
      "[Epoch 0/500] [Batch 215/250] [D loss: 331.132202] [G loss: -3487.564941]\n",
      "[Epoch 0/500] [Batch 220/250] [D loss: 248.580811] [G loss: -3324.134277]\n",
      "[Epoch 0/500] [Batch 225/250] [D loss: 208.328857] [G loss: -3175.304688]\n",
      "[Epoch 0/500] [Batch 230/250] [D loss: 161.174927] [G loss: -2992.279541]\n",
      "[Epoch 0/500] [Batch 235/250] [D loss: 136.272583] [G loss: -2808.417969]\n",
      "[Epoch 0/500] [Batch 240/250] [D loss: -37.137512] [G loss: -2458.423340]\n",
      "[Epoch 0/500] [Batch 245/250] [D loss: -63.928711] [G loss: -2236.123779]\n",
      "[Epoch 1/500] [Batch 0/250] [D loss: -57.397400] [G loss: -2036.028564]\n",
      "[Epoch 1/500] [Batch 5/250] [D loss: -105.510712] [G loss: -1776.616333]\n",
      "[Epoch 1/500] [Batch 10/250] [D loss: -140.644257] [G loss: -1528.869751]\n",
      "[Epoch 1/500] [Batch 15/250] [D loss: -165.546967] [G loss: -1295.338623]\n",
      "[Epoch 1/500] [Batch 20/250] [D loss: -149.674683] [G loss: -1106.044189]\n",
      "[Epoch 1/500] [Batch 25/250] [D loss: -128.740967] [G loss: -929.558044]\n",
      "[Epoch 1/500] [Batch 30/250] [D loss: -106.630905] [G loss: -759.937195]\n",
      "[Epoch 1/500] [Batch 35/250] [D loss: -103.009048] [G loss: -580.499817]\n",
      "[Epoch 1/500] [Batch 40/250] [D loss: -84.154579] [G loss: -434.729828]\n",
      "[Epoch 1/500] [Batch 45/250] [D loss: -62.665974] [G loss: -311.120453]\n",
      "[Epoch 1/500] [Batch 50/250] [D loss: -42.307953] [G loss: -209.771667]\n",
      "[Epoch 1/500] [Batch 55/250] [D loss: -26.467453] [G loss: -128.752426]\n",
      "[Epoch 1/500] [Batch 60/250] [D loss: -10.454284] [G loss: -74.970001]\n",
      "[Epoch 1/500] [Batch 65/250] [D loss: -1.266359] [G loss: -35.978775]\n",
      "[Epoch 1/500] [Batch 70/250] [D loss: 4.199494] [G loss: -13.194040]\n",
      "[Epoch 1/500] [Batch 75/250] [D loss: 6.560061] [G loss: -4.869051]\n",
      "[Epoch 1/500] [Batch 80/250] [D loss: 5.869491] [G loss: -10.707667]\n",
      "[Epoch 1/500] [Batch 85/250] [D loss: 2.927414] [G loss: -28.301432]\n",
      "[Epoch 1/500] [Batch 90/250] [D loss: -2.786485] [G loss: -53.451511]\n",
      "[Epoch 1/500] [Batch 95/250] [D loss: -8.697446] [G loss: -87.028061]\n",
      "[Epoch 1/500] [Batch 100/250] [D loss: -18.180798] [G loss: -124.523056]\n",
      "[Epoch 1/500] [Batch 105/250] [D loss: -27.009674] [G loss: -174.695038]\n",
      "[Epoch 1/500] [Batch 110/250] [D loss: -37.317135] [G loss: -234.088898]\n",
      "[Epoch 1/500] [Batch 115/250] [D loss: -57.415710] [G loss: -294.478699]\n",
      "[Epoch 1/500] [Batch 120/250] [D loss: -63.015221] [G loss: -381.348785]\n",
      "[Epoch 1/500] [Batch 125/250] [D loss: -79.578415] [G loss: -469.343079]\n",
      "[Epoch 1/500] [Batch 130/250] [D loss: -89.681046] [G loss: -575.819946]\n",
      "[Epoch 1/500] [Batch 135/250] [D loss: -93.941414] [G loss: -698.099731]\n",
      "[Epoch 1/500] [Batch 140/250] [D loss: -95.868988] [G loss: -831.790344]\n",
      "[Epoch 1/500] [Batch 145/250] [D loss: -105.461761] [G loss: -963.909729]\n",
      "[Epoch 1/500] [Batch 150/250] [D loss: -94.232346] [G loss: -1121.350708]\n",
      "[Epoch 1/500] [Batch 155/250] [D loss: -94.316475] [G loss: -1267.180908]\n",
      "[Epoch 1/500] [Batch 160/250] [D loss: -59.960068] [G loss: -1438.804077]\n",
      "[Epoch 1/500] [Batch 165/250] [D loss: -41.752090] [G loss: -1578.785400]\n",
      "[Epoch 1/500] [Batch 170/250] [D loss: -1.563019] [G loss: -1721.615601]\n",
      "[Epoch 1/500] [Batch 175/250] [D loss: 19.481995] [G loss: -1824.268433]\n",
      "[Epoch 1/500] [Batch 180/250] [D loss: 24.003693] [G loss: -1886.781860]\n",
      "[Epoch 1/500] [Batch 185/250] [D loss: 52.501740] [G loss: -1951.374634]\n",
      "[Epoch 1/500] [Batch 190/250] [D loss: 67.156372] [G loss: -1982.871460]\n",
      "[Epoch 1/500] [Batch 195/250] [D loss: 79.211914] [G loss: -1992.213135]\n",
      "[Epoch 1/500] [Batch 200/250] [D loss: 83.028625] [G loss: -1975.161865]\n",
      "[Epoch 1/500] [Batch 205/250] [D loss: 103.113861] [G loss: -1958.366699]\n",
      "[Epoch 1/500] [Batch 210/250] [D loss: 78.514008] [G loss: -1881.513550]\n",
      "[Epoch 1/500] [Batch 215/250] [D loss: 73.270798] [G loss: -1810.570923]\n",
      "[Epoch 1/500] [Batch 220/250] [D loss: 67.686249] [G loss: -1726.410400]\n",
      "[Epoch 1/500] [Batch 225/250] [D loss: 43.812256] [G loss: -1613.342163]\n",
      "[Epoch 1/500] [Batch 230/250] [D loss: 24.927841] [G loss: -1496.046997]\n",
      "[Epoch 1/500] [Batch 235/250] [D loss: 7.973328] [G loss: -1372.527344]\n",
      "[Epoch 1/500] [Batch 240/250] [D loss: -1.441933] [G loss: -1250.134155]\n",
      "[Epoch 1/500] [Batch 245/250] [D loss: -9.286934] [G loss: -1124.298096]\n",
      "[Epoch 2/500] [Batch 0/250] [D loss: -16.128979] [G loss: -996.258911]\n",
      "[Epoch 2/500] [Batch 5/250] [D loss: -28.105392] [G loss: -863.012817]\n",
      "[Epoch 2/500] [Batch 10/250] [D loss: -26.231674] [G loss: -742.631653]\n",
      "[Epoch 2/500] [Batch 15/250] [D loss: -26.351482] [G loss: -619.910095]\n",
      "[Epoch 2/500] [Batch 20/250] [D loss: -22.661179] [G loss: -500.884216]\n",
      "[Epoch 2/500] [Batch 25/250] [D loss: -20.430901] [G loss: -381.577209]\n",
      "[Epoch 2/500] [Batch 30/250] [D loss: -17.237431] [G loss: -266.012970]\n",
      "[Epoch 2/500] [Batch 35/250] [D loss: -10.250126] [G loss: -158.088760]\n",
      "[Epoch 2/500] [Batch 40/250] [D loss: -0.554156] [G loss: -57.973324]\n",
      "[Epoch 2/500] [Batch 45/250] [D loss: 6.570713] [G loss: 37.335125]\n",
      "[Epoch 2/500] [Batch 50/250] [D loss: 8.719855] [G loss: 129.406921]\n",
      "[Epoch 2/500] [Batch 55/250] [D loss: 12.184638] [G loss: 217.052551]\n",
      "[Epoch 2/500] [Batch 60/250] [D loss: 14.923555] [G loss: 303.368103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/500] [Batch 65/250] [D loss: 20.854404] [G loss: 383.124207]\n",
      "[Epoch 2/500] [Batch 70/250] [D loss: 24.007080] [G loss: 461.651398]\n",
      "[Epoch 2/500] [Batch 75/250] [D loss: 31.401312] [G loss: 530.231934]\n",
      "[Epoch 2/500] [Batch 80/250] [D loss: 36.679817] [G loss: 594.485779]\n",
      "[Epoch 2/500] [Batch 85/250] [D loss: 40.748421] [G loss: 652.752930]\n",
      "[Epoch 2/500] [Batch 90/250] [D loss: 53.014137] [G loss: 694.534973]\n",
      "[Epoch 2/500] [Batch 95/250] [D loss: 52.534916] [G loss: 739.802002]\n",
      "[Epoch 2/500] [Batch 100/250] [D loss: 59.745953] [G loss: 767.630005]\n",
      "[Epoch 2/500] [Batch 105/250] [D loss: 63.451279] [G loss: 788.779053]\n",
      "[Epoch 2/500] [Batch 110/250] [D loss: 67.600464] [G loss: 799.071472]\n",
      "[Epoch 2/500] [Batch 115/250] [D loss: 74.160706] [G loss: 796.354126]\n",
      "[Epoch 2/500] [Batch 120/250] [D loss: 74.420433] [G loss: 789.849731]\n",
      "[Epoch 2/500] [Batch 125/250] [D loss: 73.220802] [G loss: 775.397095]\n",
      "[Epoch 2/500] [Batch 130/250] [D loss: 68.945419] [G loss: 755.137390]\n",
      "[Epoch 2/500] [Batch 135/250] [D loss: 68.967361] [G loss: 722.237305]\n",
      "[Epoch 2/500] [Batch 140/250] [D loss: 64.538849] [G loss: 687.001465]\n",
      "[Epoch 2/500] [Batch 145/250] [D loss: 59.194019] [G loss: 647.063721]\n",
      "[Epoch 2/500] [Batch 150/250] [D loss: 59.949005] [G loss: 595.874390]\n",
      "[Epoch 2/500] [Batch 155/250] [D loss: 52.157684] [G loss: 550.035889]\n",
      "[Epoch 2/500] [Batch 160/250] [D loss: 42.396053] [G loss: 504.407867]\n",
      "[Epoch 2/500] [Batch 165/250] [D loss: 35.026703] [G loss: 454.528168]\n",
      "[Epoch 2/500] [Batch 170/250] [D loss: 31.800730] [G loss: 399.618195]\n",
      "[Epoch 2/500] [Batch 175/250] [D loss: 30.939423] [G loss: 342.029755]\n",
      "[Epoch 2/500] [Batch 180/250] [D loss: 24.100328] [G loss: 290.818909]\n",
      "[Epoch 2/500] [Batch 185/250] [D loss: 21.868595] [G loss: 235.110428]\n",
      "[Epoch 2/500] [Batch 190/250] [D loss: 17.815123] [G loss: 182.547226]\n",
      "[Epoch 2/500] [Batch 195/250] [D loss: 14.502489] [G loss: 131.091324]\n",
      "[Epoch 2/500] [Batch 200/250] [D loss: 12.209396] [G loss: 79.230026]\n",
      "[Epoch 2/500] [Batch 205/250] [D loss: 9.282686] [G loss: 27.217207]\n",
      "[Epoch 2/500] [Batch 210/250] [D loss: 5.249835] [G loss: -23.884970]\n",
      "[Epoch 2/500] [Batch 215/250] [D loss: 0.645594] [G loss: -74.865738]\n",
      "[Epoch 2/500] [Batch 220/250] [D loss: -4.453730] [G loss: -123.295479]\n",
      "[Epoch 2/500] [Batch 225/250] [D loss: -10.065746] [G loss: -171.523193]\n",
      "[Epoch 2/500] [Batch 230/250] [D loss: -15.748325] [G loss: -220.020386]\n",
      "[Epoch 2/500] [Batch 235/250] [D loss: -20.967035] [G loss: -270.141449]\n",
      "[Epoch 2/500] [Batch 240/250] [D loss: -24.017357] [G loss: -320.651215]\n",
      "[Epoch 2/500] [Batch 245/250] [D loss: -24.989983] [G loss: -369.942871]\n",
      "[Epoch 3/500] [Batch 0/250] [D loss: -31.614849] [G loss: -413.249878]\n",
      "[Epoch 3/500] [Batch 5/250] [D loss: -27.972811] [G loss: -465.771149]\n",
      "[Epoch 3/500] [Batch 10/250] [D loss: -38.799644] [G loss: -501.936127]\n",
      "[Epoch 3/500] [Batch 15/250] [D loss: -39.298840] [G loss: -547.280457]\n",
      "[Epoch 3/500] [Batch 20/250] [D loss: -34.335026] [G loss: -597.914490]\n",
      "[Epoch 3/500] [Batch 25/250] [D loss: -35.130062] [G loss: -641.672180]\n",
      "[Epoch 3/500] [Batch 30/250] [D loss: -34.152527] [G loss: -686.783447]\n",
      "[Epoch 3/500] [Batch 35/250] [D loss: -34.093529] [G loss: -729.297180]\n",
      "[Epoch 3/500] [Batch 40/250] [D loss: -31.549810] [G loss: -772.227722]\n",
      "[Epoch 3/500] [Batch 45/250] [D loss: -25.040344] [G loss: -816.285278]\n",
      "[Epoch 3/500] [Batch 50/250] [D loss: -25.756519] [G loss: -851.295593]\n",
      "[Epoch 3/500] [Batch 55/250] [D loss: -30.552347] [G loss: -881.531311]\n",
      "[Epoch 3/500] [Batch 60/250] [D loss: -27.860073] [G loss: -917.111084]\n",
      "[Epoch 3/500] [Batch 65/250] [D loss: -16.934780] [G loss: -957.380127]\n",
      "[Epoch 3/500] [Batch 70/250] [D loss: -9.110416] [G loss: -991.346924]\n",
      "[Epoch 3/500] [Batch 75/250] [D loss: -8.153847] [G loss: -1013.866089]\n",
      "[Epoch 3/500] [Batch 80/250] [D loss: -15.320194] [G loss: -1024.167236]\n",
      "[Epoch 3/500] [Batch 85/250] [D loss: -6.106991] [G loss: -1046.484741]\n",
      "[Epoch 3/500] [Batch 90/250] [D loss: -6.375324] [G loss: -1055.419067]\n",
      "[Epoch 3/500] [Batch 95/250] [D loss: 1.735752] [G loss: -1069.061646]\n",
      "[Epoch 3/500] [Batch 100/250] [D loss: -1.747494] [G loss: -1066.498169]\n",
      "[Epoch 3/500] [Batch 105/250] [D loss: -0.455212] [G loss: -1064.890381]\n",
      "[Epoch 3/500] [Batch 110/250] [D loss: 5.507057] [G loss: -1064.716187]\n",
      "[Epoch 3/500] [Batch 115/250] [D loss: 7.984695] [G loss: -1056.828125]\n",
      "[Epoch 3/500] [Batch 120/250] [D loss: 8.090656] [G loss: -1043.297241]\n",
      "[Epoch 3/500] [Batch 125/250] [D loss: 5.173389] [G loss: -1023.710632]\n",
      "[Epoch 3/500] [Batch 130/250] [D loss: 1.997807] [G loss: -1000.300659]\n",
      "[Epoch 3/500] [Batch 135/250] [D loss: 5.313484] [G loss: -980.860046]\n",
      "[Epoch 3/500] [Batch 140/250] [D loss: 4.251415] [G loss: -954.610535]\n",
      "[Epoch 3/500] [Batch 145/250] [D loss: 4.614059] [G loss: -927.553345]\n",
      "[Epoch 3/500] [Batch 150/250] [D loss: 0.638632] [G loss: -892.517883]\n",
      "[Epoch 3/500] [Batch 155/250] [D loss: 2.177021] [G loss: -860.853638]\n",
      "[Epoch 3/500] [Batch 160/250] [D loss: -1.466888] [G loss: -821.779541]\n",
      "[Epoch 3/500] [Batch 165/250] [D loss: -6.690170] [G loss: -777.901001]\n",
      "[Epoch 3/500] [Batch 170/250] [D loss: -2.552996] [G loss: -740.710632]\n",
      "[Epoch 3/500] [Batch 175/250] [D loss: -7.261086] [G loss: -692.062927]\n",
      "[Epoch 3/500] [Batch 180/250] [D loss: -7.302674] [G loss: -645.109070]\n",
      "[Epoch 3/500] [Batch 185/250] [D loss: -7.644944] [G loss: -595.391968]\n",
      "[Epoch 3/500] [Batch 190/250] [D loss: -11.337767] [G loss: -540.284729]\n",
      "[Epoch 3/500] [Batch 195/250] [D loss: -8.743437] [G loss: -489.301605]\n",
      "[Epoch 3/500] [Batch 200/250] [D loss: -9.548252] [G loss: -433.187805]\n",
      "[Epoch 3/500] [Batch 205/250] [D loss: -7.401484] [G loss: -378.153320]\n",
      "[Epoch 3/500] [Batch 210/250] [D loss: -7.377783] [G loss: -319.971069]\n",
      "[Epoch 3/500] [Batch 215/250] [D loss: -4.768193] [G loss: -263.645203]\n",
      "[Epoch 3/500] [Batch 220/250] [D loss: -3.448624] [G loss: -205.803223]\n",
      "[Epoch 3/500] [Batch 225/250] [D loss: 0.371149] [G loss: -150.702209]\n",
      "[Epoch 3/500] [Batch 230/250] [D loss: 2.296854] [G loss: -94.233795]\n",
      "[Epoch 3/500] [Batch 235/250] [D loss: 4.501703] [G loss: -38.700802]\n",
      "[Epoch 3/500] [Batch 240/250] [D loss: 6.192089] [G loss: 15.778553]\n",
      "[Epoch 3/500] [Batch 245/250] [D loss: 6.849388] [G loss: 70.411339]\n",
      "[Epoch 4/500] [Batch 0/250] [D loss: 7.207941] [G loss: 125.130066]\n",
      "[Epoch 4/500] [Batch 5/250] [D loss: 7.855844] [G loss: 179.985794]\n",
      "[Epoch 4/500] [Batch 10/250] [D loss: 8.653993] [G loss: 235.485138]\n",
      "[Epoch 4/500] [Batch 15/250] [D loss: 9.114640] [G loss: 291.779846]\n",
      "[Epoch 4/500] [Batch 20/250] [D loss: 10.438357] [G loss: 347.323364]\n",
      "[Epoch 4/500] [Batch 25/250] [D loss: 12.168068] [G loss: 401.903320]\n",
      "[Epoch 4/500] [Batch 30/250] [D loss: 13.968101] [G loss: 455.144653]\n",
      "[Epoch 4/500] [Batch 35/250] [D loss: 20.789814] [G loss: 501.365417]\n",
      "[Epoch 4/500] [Batch 40/250] [D loss: 20.008484] [G loss: 552.696777]\n",
      "[Epoch 4/500] [Batch 45/250] [D loss: 22.408562] [G loss: 597.565552]\n",
      "[Epoch 4/500] [Batch 50/250] [D loss: 27.846312] [G loss: 635.462036]\n",
      "[Epoch 4/500] [Batch 55/250] [D loss: 34.868824] [G loss: 667.004333]\n",
      "[Epoch 4/500] [Batch 60/250] [D loss: 33.109333] [G loss: 702.440308]\n",
      "[Epoch 4/500] [Batch 65/250] [D loss: 39.394699] [G loss: 724.061646]\n",
      "[Epoch 4/500] [Batch 70/250] [D loss: 42.039955] [G loss: 743.381409]\n",
      "[Epoch 4/500] [Batch 75/250] [D loss: 41.000393] [G loss: 759.969360]\n",
      "[Epoch 4/500] [Batch 80/250] [D loss: 46.691689] [G loss: 763.422119]\n",
      "[Epoch 4/500] [Batch 85/250] [D loss: 47.099998] [G loss: 765.466370]\n",
      "[Epoch 4/500] [Batch 90/250] [D loss: 43.711006] [G loss: 765.252014]\n",
      "[Epoch 4/500] [Batch 95/250] [D loss: 42.295135] [G loss: 758.215210]\n",
      "[Epoch 4/500] [Batch 100/250] [D loss: 47.238049] [G loss: 739.634766]\n",
      "[Epoch 4/500] [Batch 105/250] [D loss: 44.957096] [G loss: 722.402588]\n",
      "[Epoch 4/500] [Batch 110/250] [D loss: 42.999134] [G loss: 699.367737]\n",
      "[Epoch 4/500] [Batch 115/250] [D loss: 36.610039] [G loss: 675.585754]\n",
      "[Epoch 4/500] [Batch 120/250] [D loss: 38.112801] [G loss: 639.217590]\n",
      "[Epoch 4/500] [Batch 125/250] [D loss: 34.770954] [G loss: 602.588318]\n",
      "[Epoch 4/500] [Batch 130/250] [D loss: 27.763645] [G loss: 565.042664]\n",
      "[Epoch 4/500] [Batch 135/250] [D loss: 27.240517] [G loss: 517.125854]\n",
      "[Epoch 4/500] [Batch 140/250] [D loss: 24.225815] [G loss: 468.872681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/500] [Batch 145/250] [D loss: 19.082449] [G loss: 419.848694]\n",
      "[Epoch 4/500] [Batch 150/250] [D loss: 18.133350] [G loss: 364.137024]\n",
      "[Epoch 4/500] [Batch 155/250] [D loss: 14.364824] [G loss: 309.080353]\n",
      "[Epoch 4/500] [Batch 160/250] [D loss: 12.306402] [G loss: 249.864532]\n",
      "[Epoch 4/500] [Batch 165/250] [D loss: 10.522099] [G loss: 188.781174]\n",
      "[Epoch 4/500] [Batch 170/250] [D loss: 10.077324] [G loss: 125.855629]\n",
      "[Epoch 4/500] [Batch 175/250] [D loss: 8.115013] [G loss: 62.740025]\n",
      "[Epoch 4/500] [Batch 180/250] [D loss: 6.805653] [G loss: -2.477991]\n",
      "[Epoch 4/500] [Batch 185/250] [D loss: 2.669103] [G loss: -70.089981]\n",
      "[Epoch 4/500] [Batch 190/250] [D loss: -2.311835] [G loss: -143.936127]\n",
      "[Epoch 4/500] [Batch 195/250] [D loss: -8.295969] [G loss: -220.384109]\n",
      "[Epoch 4/500] [Batch 200/250] [D loss: -12.813758] [G loss: -300.992462]\n",
      "[Epoch 4/500] [Batch 205/250] [D loss: -18.558140] [G loss: -382.746918]\n",
      "[Epoch 4/500] [Batch 210/250] [D loss: -20.028784] [G loss: -470.269897]\n",
      "[Epoch 4/500] [Batch 215/250] [D loss: -20.944057] [G loss: -559.046448]\n",
      "[Epoch 4/500] [Batch 220/250] [D loss: -22.368242] [G loss: -646.312988]\n",
      "[Epoch 4/500] [Batch 225/250] [D loss: -15.185321] [G loss: -738.494568]\n",
      "[Epoch 4/500] [Batch 230/250] [D loss: -9.386604] [G loss: -823.831238]\n",
      "[Epoch 4/500] [Batch 235/250] [D loss: -14.222412] [G loss: -892.966614]\n",
      "[Epoch 4/500] [Batch 240/250] [D loss: -2.815704] [G loss: -972.780823]\n",
      "[Epoch 4/500] [Batch 245/250] [D loss: 2.284637] [G loss: -1039.025024]\n",
      "[Epoch 5/500] [Batch 0/250] [D loss: 15.656780] [G loss: -1105.330566]\n",
      "[Epoch 5/500] [Batch 5/250] [D loss: 15.359844] [G loss: -1149.675415]\n",
      "[Epoch 5/500] [Batch 10/250] [D loss: 26.569672] [G loss: -1196.008423]\n",
      "[Epoch 5/500] [Batch 15/250] [D loss: 30.277527] [G loss: -1226.038330]\n",
      "[Epoch 5/500] [Batch 20/250] [D loss: 40.478485] [G loss: -1253.969727]\n",
      "[Epoch 5/500] [Batch 25/250] [D loss: 38.221123] [G loss: -1260.852539]\n",
      "[Epoch 5/500] [Batch 30/250] [D loss: 36.603729] [G loss: -1259.996338]\n",
      "[Epoch 5/500] [Batch 35/250] [D loss: 41.208603] [G loss: -1257.831909]\n",
      "[Epoch 5/500] [Batch 40/250] [D loss: 40.798698] [G loss: -1243.321167]\n",
      "[Epoch 5/500] [Batch 45/250] [D loss: 40.682465] [G loss: -1222.741577]\n",
      "[Epoch 5/500] [Batch 50/250] [D loss: 37.020073] [G loss: -1192.707031]\n",
      "[Epoch 5/500] [Batch 55/250] [D loss: 35.444454] [G loss: -1159.114624]\n",
      "[Epoch 5/500] [Batch 60/250] [D loss: 27.236004] [G loss: -1112.903198]\n",
      "[Epoch 5/500] [Batch 65/250] [D loss: 26.613510] [G loss: -1068.915283]\n",
      "[Epoch 5/500] [Batch 70/250] [D loss: 20.188332] [G loss: -1013.791992]\n",
      "[Epoch 5/500] [Batch 75/250] [D loss: 15.137272] [G loss: -954.938904]\n",
      "[Epoch 5/500] [Batch 80/250] [D loss: 12.119518] [G loss: -893.275391]\n",
      "[Epoch 5/500] [Batch 85/250] [D loss: 6.771645] [G loss: -825.418396]\n",
      "[Epoch 5/500] [Batch 90/250] [D loss: 2.479336] [G loss: -755.813171]\n",
      "[Epoch 5/500] [Batch 95/250] [D loss: -0.076278] [G loss: -685.824890]\n",
      "[Epoch 5/500] [Batch 100/250] [D loss: -5.561771] [G loss: -609.694519]\n",
      "[Epoch 5/500] [Batch 105/250] [D loss: -6.043183] [G loss: -536.211975]\n",
      "[Epoch 5/500] [Batch 110/250] [D loss: -9.596538] [G loss: -459.404785]\n",
      "[Epoch 5/500] [Batch 115/250] [D loss: -8.616614] [G loss: -386.412140]\n",
      "[Epoch 5/500] [Batch 120/250] [D loss: -7.236740] [G loss: -313.307831]\n",
      "[Epoch 5/500] [Batch 125/250] [D loss: -7.069779] [G loss: -240.611740]\n",
      "[Epoch 5/500] [Batch 130/250] [D loss: -5.124143] [G loss: -170.102325]\n",
      "[Epoch 5/500] [Batch 135/250] [D loss: -3.317386] [G loss: -100.442970]\n",
      "[Epoch 5/500] [Batch 140/250] [D loss: -1.702885] [G loss: -32.104736]\n",
      "[Epoch 5/500] [Batch 145/250] [D loss: -0.673352] [G loss: 34.511925]\n",
      "[Epoch 5/500] [Batch 150/250] [D loss: -0.202080] [G loss: 100.600601]\n",
      "[Epoch 5/500] [Batch 155/250] [D loss: -0.152438] [G loss: 166.836594]\n",
      "[Epoch 5/500] [Batch 160/250] [D loss: 0.271731] [G loss: 232.645172]\n",
      "[Epoch 5/500] [Batch 165/250] [D loss: 1.072575] [G loss: 297.935425]\n",
      "[Epoch 5/500] [Batch 170/250] [D loss: 2.438876] [G loss: 362.136749]\n",
      "[Epoch 5/500] [Batch 175/250] [D loss: 4.728036] [G loss: 424.746826]\n",
      "[Epoch 5/500] [Batch 180/250] [D loss: 6.965490] [G loss: 485.268555]\n",
      "[Epoch 5/500] [Batch 185/250] [D loss: 10.797351] [G loss: 542.029785]\n",
      "[Epoch 5/500] [Batch 190/250] [D loss: 13.987350] [G loss: 596.467468]\n",
      "[Epoch 5/500] [Batch 195/250] [D loss: 17.762363] [G loss: 646.168091]\n",
      "[Epoch 5/500] [Batch 200/250] [D loss: 22.056206] [G loss: 690.942322]\n",
      "[Epoch 5/500] [Batch 205/250] [D loss: 24.597153] [G loss: 732.021790]\n",
      "[Epoch 5/500] [Batch 210/250] [D loss: 30.478214] [G loss: 763.648743]\n",
      "[Epoch 5/500] [Batch 215/250] [D loss: 32.992302] [G loss: 792.005066]\n",
      "[Epoch 5/500] [Batch 220/250] [D loss: 33.929562] [G loss: 814.981262]\n",
      "[Epoch 5/500] [Batch 225/250] [D loss: 36.001617] [G loss: 829.577393]\n",
      "[Epoch 5/500] [Batch 230/250] [D loss: 36.153152] [G loss: 838.463806]\n",
      "[Epoch 5/500] [Batch 235/250] [D loss: 41.335548] [G loss: 834.355469]\n",
      "[Epoch 5/500] [Batch 240/250] [D loss: 35.869942] [G loss: 833.940613]\n",
      "[Epoch 5/500] [Batch 245/250] [D loss: 37.095852] [G loss: 819.194275]\n",
      "[Epoch 6/500] [Batch 0/250] [D loss: 34.113136] [G loss: 801.532532]\n",
      "[Epoch 6/500] [Batch 5/250] [D loss: 35.326080] [G loss: 772.987305]\n",
      "[Epoch 6/500] [Batch 10/250] [D loss: 31.600588] [G loss: 742.494812]\n",
      "[Epoch 6/500] [Batch 15/250] [D loss: 26.917627] [G loss: 707.066162]\n",
      "[Epoch 6/500] [Batch 20/250] [D loss: 25.186346] [G loss: 663.373108]\n",
      "[Epoch 6/500] [Batch 25/250] [D loss: 21.401535] [G loss: 616.596313]\n",
      "[Epoch 6/500] [Batch 30/250] [D loss: 17.043221] [G loss: 565.834534]\n",
      "[Epoch 6/500] [Batch 35/250] [D loss: 16.171165] [G loss: 507.928741]\n",
      "[Epoch 6/500] [Batch 40/250] [D loss: 11.301855] [G loss: 450.507019]\n",
      "[Epoch 6/500] [Batch 45/250] [D loss: 9.699950] [G loss: 387.190735]\n",
      "[Epoch 6/500] [Batch 50/250] [D loss: 6.374190] [G loss: 323.355957]\n",
      "[Epoch 6/500] [Batch 55/250] [D loss: 3.441191] [G loss: 256.977600]\n",
      "[Epoch 6/500] [Batch 60/250] [D loss: 0.794174] [G loss: 188.871582]\n",
      "[Epoch 6/500] [Batch 65/250] [D loss: -0.936857] [G loss: 118.457497]\n",
      "[Epoch 6/500] [Batch 70/250] [D loss: -3.082577] [G loss: 46.947720]\n",
      "[Epoch 6/500] [Batch 75/250] [D loss: -5.001153] [G loss: -26.016340]\n",
      "[Epoch 6/500] [Batch 80/250] [D loss: -7.354970] [G loss: -100.166641]\n",
      "[Epoch 6/500] [Batch 85/250] [D loss: -9.211835] [G loss: -176.058548]\n",
      "[Epoch 6/500] [Batch 90/250] [D loss: -11.418261] [G loss: -252.301071]\n",
      "[Epoch 6/500] [Batch 95/250] [D loss: -11.818979] [G loss: -330.804840]\n",
      "[Epoch 6/500] [Batch 100/250] [D loss: -13.206892] [G loss: -407.845062]\n",
      "[Epoch 6/500] [Batch 105/250] [D loss: -13.355747] [G loss: -485.396912]\n",
      "[Epoch 6/500] [Batch 110/250] [D loss: -8.309715] [G loss: -566.319214]\n",
      "[Epoch 6/500] [Batch 115/250] [D loss: -9.800859] [G loss: -638.586914]\n",
      "[Epoch 6/500] [Batch 120/250] [D loss: -3.422453] [G loss: -715.530945]\n",
      "[Epoch 6/500] [Batch 125/250] [D loss: 2.027466] [G loss: -787.497070]\n",
      "[Epoch 6/500] [Batch 130/250] [D loss: 2.848751] [G loss: -850.459534]\n",
      "[Epoch 6/500] [Batch 135/250] [D loss: 7.138683] [G loss: -911.847412]\n",
      "[Epoch 6/500] [Batch 140/250] [D loss: 16.612553] [G loss: -972.488281]\n",
      "[Epoch 6/500] [Batch 145/250] [D loss: 20.463203] [G loss: -1021.509521]\n",
      "[Epoch 6/500] [Batch 150/250] [D loss: 29.583199] [G loss: -1069.381348]\n",
      "[Epoch 6/500] [Batch 155/250] [D loss: 34.150841] [G loss: -1105.639160]\n",
      "[Epoch 6/500] [Batch 160/250] [D loss: 39.125832] [G loss: -1135.800049]\n",
      "[Epoch 6/500] [Batch 165/250] [D loss: 46.471367] [G loss: -1161.233765]\n",
      "[Epoch 6/500] [Batch 170/250] [D loss: 41.782906] [G loss: -1167.311279]\n",
      "[Epoch 6/500] [Batch 175/250] [D loss: 45.767662] [G loss: -1176.750488]\n",
      "[Epoch 6/500] [Batch 180/250] [D loss: 47.006470] [G loss: -1176.930420]\n",
      "[Epoch 6/500] [Batch 185/250] [D loss: 45.279770] [G loss: -1168.015747]\n",
      "[Epoch 6/500] [Batch 190/250] [D loss: 45.325813] [G loss: -1154.333008]\n",
      "[Epoch 6/500] [Batch 195/250] [D loss: 42.719238] [G loss: -1131.963745]\n",
      "[Epoch 6/500] [Batch 200/250] [D loss: 38.972801] [G loss: -1102.505981]\n",
      "[Epoch 6/500] [Batch 205/250] [D loss: 32.590649] [G loss: -1064.944214]\n",
      "[Epoch 6/500] [Batch 210/250] [D loss: 31.099396] [G loss: -1026.982544]\n",
      "[Epoch 6/500] [Batch 215/250] [D loss: 27.240891] [G loss: -981.407349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/500] [Batch 220/250] [D loss: 21.910301] [G loss: -929.359497]\n",
      "[Epoch 6/500] [Batch 225/250] [D loss: 15.576054] [G loss: -871.729065]\n",
      "[Epoch 6/500] [Batch 230/250] [D loss: 12.649599] [G loss: -812.616943]\n",
      "[Epoch 6/500] [Batch 235/250] [D loss: 8.584141] [G loss: -748.364990]\n",
      "[Epoch 6/500] [Batch 240/250] [D loss: 2.105625] [G loss: -677.723083]\n",
      "[Epoch 6/500] [Batch 245/250] [D loss: 0.819658] [G loss: -608.660156]\n",
      "[Epoch 7/500] [Batch 0/250] [D loss: -1.066672] [G loss: -535.843750]\n",
      "[Epoch 7/500] [Batch 5/250] [D loss: -3.028653] [G loss: -459.507751]\n",
      "[Epoch 7/500] [Batch 10/250] [D loss: -5.558800] [G loss: -380.282013]\n",
      "[Epoch 7/500] [Batch 15/250] [D loss: -5.809355] [G loss: -301.790558]\n",
      "[Epoch 7/500] [Batch 20/250] [D loss: -5.896058] [G loss: -222.479584]\n",
      "[Epoch 7/500] [Batch 25/250] [D loss: -5.654815] [G loss: -143.147659]\n",
      "[Epoch 7/500] [Batch 30/250] [D loss: -4.727274] [G loss: -64.659889]\n",
      "[Epoch 7/500] [Batch 35/250] [D loss: -3.946465] [G loss: 13.474526]\n",
      "[Epoch 7/500] [Batch 40/250] [D loss: -3.075682] [G loss: 90.849037]\n",
      "[Epoch 7/500] [Batch 45/250] [D loss: -1.808754] [G loss: 167.114639]\n",
      "[Epoch 7/500] [Batch 50/250] [D loss: -0.253154] [G loss: 242.432312]\n",
      "[Epoch 7/500] [Batch 55/250] [D loss: 1.118393] [G loss: 316.924500]\n",
      "[Epoch 7/500] [Batch 60/250] [D loss: 2.676116] [G loss: 390.221832]\n",
      "[Epoch 7/500] [Batch 65/250] [D loss: 5.462931] [G loss: 460.589874]\n",
      "[Epoch 7/500] [Batch 70/250] [D loss: 8.686600] [G loss: 528.159668]\n",
      "[Epoch 7/500] [Batch 75/250] [D loss: 11.982813] [G loss: 592.089417]\n",
      "[Epoch 7/500] [Batch 80/250] [D loss: 16.901569] [G loss: 650.668884]\n",
      "[Epoch 7/500] [Batch 85/250] [D loss: 22.045856] [G loss: 704.002380]\n",
      "[Epoch 7/500] [Batch 90/250] [D loss: 26.408432] [G loss: 752.440979]\n",
      "[Epoch 7/500] [Batch 95/250] [D loss: 29.355934] [G loss: 795.956848]\n",
      "[Epoch 7/500] [Batch 100/250] [D loss: 35.051991] [G loss: 829.559448]\n",
      "[Epoch 7/500] [Batch 105/250] [D loss: 35.553387] [G loss: 861.306458]\n",
      "[Epoch 7/500] [Batch 110/250] [D loss: 39.165642] [G loss: 882.149841]\n",
      "[Epoch 7/500] [Batch 115/250] [D loss: 42.182838] [G loss: 895.294067]\n",
      "[Epoch 7/500] [Batch 120/250] [D loss: 45.563160] [G loss: 900.027039]\n",
      "[Epoch 7/500] [Batch 125/250] [D loss: 46.108158] [G loss: 899.881226]\n",
      "[Epoch 7/500] [Batch 130/250] [D loss: 42.385506] [G loss: 895.297974]\n",
      "[Epoch 7/500] [Batch 135/250] [D loss: 46.488991] [G loss: 875.714050]\n",
      "[Epoch 7/500] [Batch 140/250] [D loss: 41.885250] [G loss: 857.549744]\n",
      "[Epoch 7/500] [Batch 145/250] [D loss: 43.141624] [G loss: 826.927979]\n",
      "[Epoch 7/500] [Batch 150/250] [D loss: 38.047340] [G loss: 795.852173]\n",
      "[Epoch 7/500] [Batch 155/250] [D loss: 34.523636] [G loss: 757.422974]\n",
      "[Epoch 7/500] [Batch 160/250] [D loss: 32.458336] [G loss: 712.042725]\n",
      "[Epoch 7/500] [Batch 165/250] [D loss: 27.961287] [G loss: 664.420776]\n",
      "[Epoch 7/500] [Batch 170/250] [D loss: 26.770683] [G loss: 610.208496]\n",
      "[Epoch 7/500] [Batch 175/250] [D loss: 21.871544] [G loss: 557.049377]\n",
      "[Epoch 7/500] [Batch 180/250] [D loss: 17.985924] [G loss: 499.825745]\n",
      "[Epoch 7/500] [Batch 185/250] [D loss: 16.940815] [G loss: 437.607574]\n",
      "[Epoch 7/500] [Batch 190/250] [D loss: 12.850741] [G loss: 376.352448]\n",
      "[Epoch 7/500] [Batch 195/250] [D loss: 11.991167] [G loss: 310.858368]\n",
      "[Epoch 7/500] [Batch 200/250] [D loss: 8.817626] [G loss: 246.798523]\n",
      "[Epoch 7/500] [Batch 205/250] [D loss: 8.499619] [G loss: 179.462006]\n",
      "[Epoch 7/500] [Batch 210/250] [D loss: 7.173267] [G loss: 112.663223]\n",
      "[Epoch 7/500] [Batch 215/250] [D loss: 6.004054] [G loss: 45.090210]\n",
      "[Epoch 7/500] [Batch 220/250] [D loss: 4.682111] [G loss: -23.337528]\n",
      "[Epoch 7/500] [Batch 225/250] [D loss: 2.267957] [G loss: -92.604294]\n",
      "[Epoch 7/500] [Batch 230/250] [D loss: -0.792902] [G loss: -163.330414]\n",
      "[Epoch 7/500] [Batch 235/250] [D loss: -3.564234] [G loss: -236.053802]\n",
      "[Epoch 7/500] [Batch 240/250] [D loss: -7.316183] [G loss: -308.609741]\n",
      "[Epoch 7/500] [Batch 245/250] [D loss: -8.297527] [G loss: -384.474823]\n",
      "[Epoch 8/500] [Batch 0/250] [D loss: -9.630648] [G loss: -459.378754]\n",
      "[Epoch 8/500] [Batch 5/250] [D loss: -10.496977] [G loss: -533.668396]\n",
      "[Epoch 8/500] [Batch 10/250] [D loss: -8.000257] [G loss: -609.503845]\n",
      "[Epoch 8/500] [Batch 15/250] [D loss: -6.452190] [G loss: -681.719299]\n",
      "[Epoch 8/500] [Batch 20/250] [D loss: -2.701338] [G loss: -753.272034]\n",
      "[Epoch 8/500] [Batch 25/250] [D loss: -5.356665] [G loss: -814.513367]\n",
      "[Epoch 8/500] [Batch 30/250] [D loss: 4.118763] [G loss: -883.823975]\n",
      "[Epoch 8/500] [Batch 35/250] [D loss: 7.320221] [G loss: -942.143860]\n",
      "[Epoch 8/500] [Batch 40/250] [D loss: 15.041492] [G loss: -1000.133423]\n",
      "[Epoch 8/500] [Batch 45/250] [D loss: 20.375538] [G loss: -1050.390137]\n",
      "[Epoch 8/500] [Batch 50/250] [D loss: 22.692848] [G loss: -1091.754150]\n",
      "[Epoch 8/500] [Batch 55/250] [D loss: 28.130562] [G loss: -1130.652954]\n",
      "[Epoch 8/500] [Batch 60/250] [D loss: 34.378471] [G loss: -1164.625854]\n",
      "[Epoch 8/500] [Batch 65/250] [D loss: 39.169655] [G loss: -1190.958740]\n",
      "[Epoch 8/500] [Batch 70/250] [D loss: 44.234749] [G loss: -1211.590088]\n",
      "[Epoch 8/500] [Batch 75/250] [D loss: 46.772758] [G loss: -1224.277344]\n",
      "[Epoch 8/500] [Batch 80/250] [D loss: 45.399231] [G loss: -1226.531494]\n",
      "[Epoch 8/500] [Batch 85/250] [D loss: 50.074806] [G loss: -1229.671143]\n",
      "[Epoch 8/500] [Batch 90/250] [D loss: 48.454018] [G loss: -1220.705933]\n",
      "[Epoch 8/500] [Batch 95/250] [D loss: 48.069839] [G loss: -1207.466187]\n",
      "[Epoch 8/500] [Batch 100/250] [D loss: 47.742065] [G loss: -1189.244019]\n",
      "[Epoch 8/500] [Batch 105/250] [D loss: 40.330734] [G loss: -1158.969482]\n",
      "[Epoch 8/500] [Batch 110/250] [D loss: 40.834839] [G loss: -1131.740112]\n",
      "[Epoch 8/500] [Batch 115/250] [D loss: 33.570652] [G loss: -1091.836182]\n",
      "[Epoch 8/500] [Batch 120/250] [D loss: 34.113853] [G loss: -1055.471802]\n",
      "[Epoch 8/500] [Batch 125/250] [D loss: 29.161819] [G loss: -1009.262268]\n",
      "[Epoch 8/500] [Batch 130/250] [D loss: 23.100735] [G loss: -958.138550]\n",
      "[Epoch 8/500] [Batch 135/250] [D loss: 19.867714] [G loss: -906.147888]\n",
      "[Epoch 8/500] [Batch 140/250] [D loss: 13.400026] [G loss: -847.916626]\n",
      "[Epoch 8/500] [Batch 145/250] [D loss: 9.231834] [G loss: -788.491943]\n",
      "[Epoch 8/500] [Batch 150/250] [D loss: 7.015409] [G loss: -727.894348]\n",
      "[Epoch 8/500] [Batch 155/250] [D loss: 2.844653] [G loss: -663.065613]\n",
      "[Epoch 8/500] [Batch 160/250] [D loss: 0.891950] [G loss: -597.834778]\n",
      "[Epoch 8/500] [Batch 165/250] [D loss: -0.618196] [G loss: -531.285217]\n",
      "[Epoch 8/500] [Batch 170/250] [D loss: -2.802310] [G loss: -462.479279]\n",
      "[Epoch 8/500] [Batch 175/250] [D loss: -2.793845] [G loss: -394.728363]\n",
      "[Epoch 8/500] [Batch 180/250] [D loss: -3.277405] [G loss: -325.978058]\n",
      "[Epoch 8/500] [Batch 185/250] [D loss: -2.158777] [G loss: -258.851746]\n",
      "[Epoch 8/500] [Batch 190/250] [D loss: -0.336679] [G loss: -193.052383]\n",
      "[Epoch 8/500] [Batch 195/250] [D loss: 1.855164] [G loss: -128.657074]\n",
      "[Epoch 8/500] [Batch 200/250] [D loss: 4.610141] [G loss: -66.417145]\n",
      "[Epoch 8/500] [Batch 205/250] [D loss: 6.771203] [G loss: -6.643060]\n",
      "[Epoch 8/500] [Batch 210/250] [D loss: 6.708493] [G loss: 52.152206]\n",
      "[Epoch 8/500] [Batch 215/250] [D loss: 5.157895] [G loss: 111.629570]\n",
      "[Epoch 8/500] [Batch 220/250] [D loss: 3.621004] [G loss: 172.009338]\n",
      "[Epoch 8/500] [Batch 225/250] [D loss: 2.756567] [G loss: 232.818008]\n",
      "[Epoch 8/500] [Batch 230/250] [D loss: 2.219591] [G loss: 294.587341]\n",
      "[Epoch 8/500] [Batch 235/250] [D loss: 2.829150] [G loss: 356.330841]\n",
      "[Epoch 8/500] [Batch 240/250] [D loss: 3.889911] [G loss: 417.277557]\n",
      "[Epoch 8/500] [Batch 245/250] [D loss: 5.036415] [G loss: 477.413086]\n",
      "[Epoch 9/500] [Batch 0/250] [D loss: 7.896720] [G loss: 534.527893]\n",
      "[Epoch 9/500] [Batch 5/250] [D loss: 9.516911] [G loss: 591.006531]\n",
      "[Epoch 9/500] [Batch 10/250] [D loss: 14.307659] [G loss: 641.574219]\n",
      "[Epoch 9/500] [Batch 15/250] [D loss: 16.745567] [G loss: 691.625122]\n",
      "[Epoch 9/500] [Batch 20/250] [D loss: 18.092285] [G loss: 739.026001]\n",
      "[Epoch 9/500] [Batch 25/250] [D loss: 25.825773] [G loss: 775.921204]\n",
      "[Epoch 9/500] [Batch 30/250] [D loss: 26.577549] [G loss: 814.563354]\n",
      "[Epoch 9/500] [Batch 35/250] [D loss: 29.383974] [G loss: 846.241638]\n",
      "[Epoch 9/500] [Batch 40/250] [D loss: 33.146412] [G loss: 871.768433]\n",
      "[Epoch 9/500] [Batch 45/250] [D loss: 36.604321] [G loss: 892.350159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/500] [Batch 50/250] [D loss: 38.139580] [G loss: 909.389954]\n",
      "[Epoch 9/500] [Batch 55/250] [D loss: 40.542965] [G loss: 919.643982]\n",
      "[Epoch 9/500] [Batch 60/250] [D loss: 39.539295] [G loss: 927.985779]\n",
      "[Epoch 9/500] [Batch 65/250] [D loss: 40.673172] [G loss: 930.260437]\n",
      "[Epoch 9/500] [Batch 70/250] [D loss: 40.828182] [G loss: 928.670654]\n",
      "[Epoch 9/500] [Batch 75/250] [D loss: 39.641285] [G loss: 923.212097]\n",
      "[Epoch 9/500] [Batch 80/250] [D loss: 41.834957] [G loss: 909.520386]\n",
      "[Epoch 9/500] [Batch 85/250] [D loss: 39.992886] [G loss: 895.057373]\n",
      "[Epoch 9/500] [Batch 90/250] [D loss: 35.978111] [G loss: 878.481079]\n",
      "[Epoch 9/500] [Batch 95/250] [D loss: 34.466225] [G loss: 855.026001]\n",
      "[Epoch 9/500] [Batch 100/250] [D loss: 33.395618] [G loss: 827.003418]\n",
      "[Epoch 9/500] [Batch 105/250] [D loss: 30.880005] [G loss: 797.131226]\n",
      "[Epoch 9/500] [Batch 110/250] [D loss: 30.221182] [G loss: 762.091309]\n",
      "[Epoch 9/500] [Batch 115/250] [D loss: 24.913099] [G loss: 728.685547]\n",
      "[Epoch 9/500] [Batch 120/250] [D loss: 23.868391] [G loss: 688.395630]\n",
      "[Epoch 9/500] [Batch 125/250] [D loss: 20.549376] [G loss: 647.984802]\n",
      "[Epoch 9/500] [Batch 130/250] [D loss: 16.947039] [G loss: 605.979553]\n",
      "[Epoch 9/500] [Batch 135/250] [D loss: 15.720303] [G loss: 560.169067]\n",
      "[Epoch 9/500] [Batch 140/250] [D loss: 12.016151] [G loss: 515.556091]\n",
      "[Epoch 9/500] [Batch 145/250] [D loss: 9.382608] [G loss: 469.178101]\n",
      "[Epoch 9/500] [Batch 150/250] [D loss: 7.329412] [G loss: 421.597595]\n",
      "[Epoch 9/500] [Batch 155/250] [D loss: 7.151563] [G loss: 372.141663]\n",
      "[Epoch 9/500] [Batch 160/250] [D loss: 5.508079] [G loss: 324.253479]\n",
      "[Epoch 9/500] [Batch 165/250] [D loss: 5.503446] [G loss: 275.198334]\n",
      "[Epoch 9/500] [Batch 170/250] [D loss: 4.417589] [G loss: 227.712906]\n",
      "[Epoch 9/500] [Batch 175/250] [D loss: 4.941225] [G loss: 179.523926]\n",
      "[Epoch 9/500] [Batch 180/250] [D loss: 4.965119] [G loss: 132.718918]\n",
      "[Epoch 9/500] [Batch 185/250] [D loss: 5.542912] [G loss: 86.286919]\n",
      "[Epoch 9/500] [Batch 190/250] [D loss: 5.411660] [G loss: 41.096817]\n",
      "[Epoch 9/500] [Batch 195/250] [D loss: 5.056641] [G loss: -3.822021]\n",
      "[Epoch 9/500] [Batch 200/250] [D loss: 4.088697] [G loss: -48.888779]\n",
      "[Epoch 9/500] [Batch 205/250] [D loss: 2.401155] [G loss: -94.268295]\n",
      "[Epoch 9/500] [Batch 210/250] [D loss: -0.190161] [G loss: -139.583710]\n",
      "[Epoch 9/500] [Batch 215/250] [D loss: -1.825090] [G loss: -186.433121]\n",
      "[Epoch 9/500] [Batch 220/250] [D loss: -3.592163] [G loss: -233.270645]\n",
      "[Epoch 9/500] [Batch 225/250] [D loss: -4.830245] [G loss: -280.721680]\n",
      "[Epoch 9/500] [Batch 230/250] [D loss: -5.677396] [G loss: -328.226837]\n",
      "[Epoch 9/500] [Batch 235/250] [D loss: -5.365778] [G loss: -376.417114]\n",
      "[Epoch 9/500] [Batch 240/250] [D loss: -6.719689] [G loss: -422.204376]\n",
      "[Epoch 9/500] [Batch 245/250] [D loss: -4.620079] [G loss: -470.469452]\n",
      "[Epoch 10/500] [Batch 0/250] [D loss: -5.045072] [G loss: -514.823975]\n",
      "[Epoch 10/500] [Batch 5/250] [D loss: -3.537306] [G loss: -559.796387]\n",
      "[Epoch 10/500] [Batch 10/250] [D loss: -3.352011] [G loss: -601.904236]\n",
      "[Epoch 10/500] [Batch 15/250] [D loss: -1.757915] [G loss: -643.732788]\n",
      "[Epoch 10/500] [Batch 20/250] [D loss: 1.164394] [G loss: -684.886597]\n",
      "[Epoch 10/500] [Batch 25/250] [D loss: 4.405182] [G loss: -724.153503]\n",
      "[Epoch 10/500] [Batch 30/250] [D loss: 4.506868] [G loss: -758.031067]\n",
      "[Epoch 10/500] [Batch 35/250] [D loss: 6.077456] [G loss: -790.731689]\n",
      "[Epoch 10/500] [Batch 40/250] [D loss: 10.376148] [G loss: -823.771729]\n",
      "[Epoch 10/500] [Batch 45/250] [D loss: 13.471485] [G loss: -852.951721]\n",
      "[Epoch 10/500] [Batch 50/250] [D loss: 14.975082] [G loss: -877.842224]\n",
      "[Epoch 10/500] [Batch 55/250] [D loss: 18.529009] [G loss: -902.104797]\n",
      "[Epoch 10/500] [Batch 60/250] [D loss: 21.635168] [G loss: -922.979980]\n",
      "[Epoch 10/500] [Batch 65/250] [D loss: 20.257835] [G loss: -937.152771]\n",
      "[Epoch 10/500] [Batch 70/250] [D loss: 21.137661] [G loss: -950.471252]\n",
      "[Epoch 10/500] [Batch 75/250] [D loss: 25.344028] [G loss: -964.159119]\n",
      "[Epoch 10/500] [Batch 80/250] [D loss: 25.288723] [G loss: -970.813965]\n",
      "[Epoch 10/500] [Batch 85/250] [D loss: 26.091419] [G loss: -975.668518]\n",
      "[Epoch 10/500] [Batch 90/250] [D loss: 27.641582] [G loss: -978.413330]\n",
      "[Epoch 10/500] [Batch 95/250] [D loss: 25.777023] [G loss: -975.250061]\n",
      "[Epoch 10/500] [Batch 100/250] [D loss: 28.157669] [G loss: -973.042847]\n",
      "[Epoch 10/500] [Batch 105/250] [D loss: 27.565441] [G loss: -965.583008]\n",
      "[Epoch 10/500] [Batch 110/250] [D loss: 24.894558] [G loss: -953.087097]\n",
      "[Epoch 10/500] [Batch 115/250] [D loss: 26.133175] [G loss: -941.571228]\n",
      "[Epoch 10/500] [Batch 120/250] [D loss: 23.135622] [G loss: -923.470581]\n",
      "[Epoch 10/500] [Batch 125/250] [D loss: 20.441767] [G loss: -903.116882]\n",
      "[Epoch 10/500] [Batch 130/250] [D loss: 18.558399] [G loss: -880.906860]\n",
      "[Epoch 10/500] [Batch 135/250] [D loss: 17.310715] [G loss: -856.787842]\n",
      "[Epoch 10/500] [Batch 140/250] [D loss: 14.854141] [G loss: -829.251404]\n",
      "[Epoch 10/500] [Batch 145/250] [D loss: 14.326771] [G loss: -801.237000]\n",
      "[Epoch 10/500] [Batch 150/250] [D loss: 10.898411] [G loss: -768.372559]\n",
      "[Epoch 10/500] [Batch 155/250] [D loss: 10.138573] [G loss: -735.742249]\n",
      "[Epoch 10/500] [Batch 160/250] [D loss: 9.001631] [G loss: -701.154846]\n",
      "[Epoch 10/500] [Batch 165/250] [D loss: 6.523755] [G loss: -663.372009]\n",
      "[Epoch 10/500] [Batch 170/250] [D loss: 5.487251] [G loss: -624.896118]\n",
      "[Epoch 10/500] [Batch 175/250] [D loss: 2.241498] [G loss: -582.411194]\n",
      "[Epoch 10/500] [Batch 180/250] [D loss: 1.435459] [G loss: -540.120361]\n",
      "[Epoch 10/500] [Batch 185/250] [D loss: 1.013217] [G loss: -495.829041]\n",
      "[Epoch 10/500] [Batch 190/250] [D loss: -0.541848] [G loss: -448.845184]\n",
      "[Epoch 10/500] [Batch 195/250] [D loss: -0.158788] [G loss: -402.078186]\n",
      "[Epoch 10/500] [Batch 200/250] [D loss: -1.093063] [G loss: -352.882050]\n",
      "[Epoch 10/500] [Batch 205/250] [D loss: -1.250189] [G loss: -303.716522]\n",
      "[Epoch 10/500] [Batch 210/250] [D loss: -1.158967] [G loss: -254.757965]\n",
      "[Epoch 10/500] [Batch 215/250] [D loss: 0.510914] [G loss: -207.212006]\n",
      "[Epoch 10/500] [Batch 220/250] [D loss: 0.985171] [G loss: -158.896133]\n",
      "[Epoch 10/500] [Batch 225/250] [D loss: 2.066026] [G loss: -112.002754]\n",
      "[Epoch 10/500] [Batch 230/250] [D loss: 3.072981] [G loss: -65.803253]\n",
      "[Epoch 10/500] [Batch 235/250] [D loss: 3.660347] [G loss: -19.964193]\n",
      "[Epoch 10/500] [Batch 240/250] [D loss: 3.672548] [G loss: 25.912182]\n",
      "[Epoch 10/500] [Batch 245/250] [D loss: 3.377113] [G loss: 72.230606]\n",
      "[Epoch 11/500] [Batch 0/250] [D loss: 2.732345] [G loss: 119.641777]\n",
      "[Epoch 11/500] [Batch 5/250] [D loss: 2.251642] [G loss: 167.808548]\n",
      "[Epoch 11/500] [Batch 10/250] [D loss: 1.794796] [G loss: 217.140625]\n",
      "[Epoch 11/500] [Batch 15/250] [D loss: 1.681901] [G loss: 267.295197]\n",
      "[Epoch 11/500] [Batch 20/250] [D loss: 2.323552] [G loss: 317.211517]\n",
      "[Epoch 11/500] [Batch 25/250] [D loss: 2.147202] [G loss: 368.174713]\n",
      "[Epoch 11/500] [Batch 30/250] [D loss: 2.695407] [G loss: 418.012146]\n",
      "[Epoch 11/500] [Batch 35/250] [D loss: 4.499086] [G loss: 465.561798]\n",
      "[Epoch 11/500] [Batch 40/250] [D loss: 5.611581] [G loss: 511.685059]\n",
      "[Epoch 11/500] [Batch 45/250] [D loss: 6.348377] [G loss: 555.028931]\n",
      "[Epoch 11/500] [Batch 50/250] [D loss: 9.767717] [G loss: 591.764099]\n",
      "[Epoch 11/500] [Batch 55/250] [D loss: 11.434503] [G loss: 625.791138]\n",
      "[Epoch 11/500] [Batch 60/250] [D loss: 14.303089] [G loss: 653.675659]\n",
      "[Epoch 11/500] [Batch 65/250] [D loss: 14.827209] [G loss: 678.655090]\n",
      "[Epoch 11/500] [Batch 70/250] [D loss: 16.222973] [G loss: 697.595703]\n",
      "[Epoch 11/500] [Batch 75/250] [D loss: 17.290226] [G loss: 711.492798]\n",
      "[Epoch 11/500] [Batch 80/250] [D loss: 19.563007] [G loss: 719.021790]\n",
      "[Epoch 11/500] [Batch 85/250] [D loss: 19.764904] [G loss: 724.273132]\n",
      "[Epoch 11/500] [Batch 90/250] [D loss: 21.833858] [G loss: 722.731384]\n",
      "[Epoch 11/500] [Batch 95/250] [D loss: 21.715147] [G loss: 718.365295]\n",
      "[Epoch 11/500] [Batch 100/250] [D loss: 19.762299] [G loss: 711.456543]\n",
      "[Epoch 11/500] [Batch 105/250] [D loss: 21.123650] [G loss: 697.267822]\n",
      "[Epoch 11/500] [Batch 110/250] [D loss: 18.448143] [G loss: 683.273315]\n",
      "[Epoch 11/500] [Batch 115/250] [D loss: 18.660425] [G loss: 663.348083]\n",
      "[Epoch 11/500] [Batch 120/250] [D loss: 17.666990] [G loss: 642.358948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/500] [Batch 125/250] [D loss: 16.300587] [G loss: 618.929260]\n",
      "[Epoch 11/500] [Batch 130/250] [D loss: 15.530472] [G loss: 593.288513]\n",
      "[Epoch 11/500] [Batch 135/250] [D loss: 14.072131] [G loss: 566.715637]\n",
      "[Epoch 11/500] [Batch 140/250] [D loss: 12.888462] [G loss: 538.655823]\n",
      "[Epoch 11/500] [Batch 145/250] [D loss: 12.648497] [G loss: 508.426636]\n",
      "[Epoch 11/500] [Batch 150/250] [D loss: 11.198539] [G loss: 479.230255]\n",
      "[Epoch 11/500] [Batch 155/250] [D loss: 9.852489] [G loss: 449.630981]\n",
      "[Epoch 11/500] [Batch 160/250] [D loss: 10.128193] [G loss: 418.182770]\n",
      "[Epoch 11/500] [Batch 165/250] [D loss: 9.752403] [G loss: 387.595367]\n",
      "[Epoch 11/500] [Batch 170/250] [D loss: 9.362565] [G loss: 357.155884]\n",
      "[Epoch 11/500] [Batch 175/250] [D loss: 9.047735] [G loss: 326.827484]\n",
      "[Epoch 11/500] [Batch 180/250] [D loss: 8.467027] [G loss: 297.619049]\n",
      "[Epoch 11/500] [Batch 185/250] [D loss: 7.445475] [G loss: 269.324768]\n",
      "[Epoch 11/500] [Batch 190/250] [D loss: 8.422608] [G loss: 239.526016]\n",
      "[Epoch 11/500] [Batch 195/250] [D loss: 8.125269] [G loss: 211.443787]\n",
      "[Epoch 11/500] [Batch 200/250] [D loss: 8.617054] [G loss: 183.222168]\n",
      "[Epoch 11/500] [Batch 205/250] [D loss: 8.640182] [G loss: 156.248398]\n",
      "[Epoch 11/500] [Batch 210/250] [D loss: 8.497411] [G loss: 130.333130]\n",
      "[Epoch 11/500] [Batch 215/250] [D loss: 8.966835] [G loss: 104.727715]\n",
      "[Epoch 11/500] [Batch 220/250] [D loss: 9.037327] [G loss: 80.490044]\n",
      "[Epoch 11/500] [Batch 225/250] [D loss: 9.333580] [G loss: 56.975121]\n",
      "[Epoch 11/500] [Batch 230/250] [D loss: 9.595024] [G loss: 34.382149]\n",
      "[Epoch 11/500] [Batch 235/250] [D loss: 9.685320] [G loss: 12.564290]\n",
      "[Epoch 11/500] [Batch 240/250] [D loss: 9.507803] [G loss: -8.985798]\n",
      "[Epoch 11/500] [Batch 245/250] [D loss: 8.907808] [G loss: -30.806219]\n",
      "[Epoch 12/500] [Batch 0/250] [D loss: 7.882873] [G loss: -53.551941]\n",
      "[Epoch 12/500] [Batch 5/250] [D loss: 6.111819] [G loss: -77.250488]\n",
      "[Epoch 12/500] [Batch 10/250] [D loss: 4.632444] [G loss: -103.003212]\n",
      "[Epoch 12/500] [Batch 15/250] [D loss: 3.308807] [G loss: -130.678757]\n",
      "[Epoch 12/500] [Batch 20/250] [D loss: 1.054490] [G loss: -159.044052]\n",
      "[Epoch 12/500] [Batch 25/250] [D loss: -0.082749] [G loss: -190.174622]\n",
      "[Epoch 12/500] [Batch 30/250] [D loss: -2.097454] [G loss: -221.883179]\n",
      "[Epoch 12/500] [Batch 35/250] [D loss: -3.275258] [G loss: -255.988586]\n",
      "[Epoch 12/500] [Batch 40/250] [D loss: -3.862397] [G loss: -291.951599]\n",
      "[Epoch 12/500] [Batch 45/250] [D loss: -5.496244] [G loss: -327.926544]\n",
      "[Epoch 12/500] [Batch 50/250] [D loss: -5.921691] [G loss: -366.129791]\n",
      "[Epoch 12/500] [Batch 55/250] [D loss: -7.456244] [G loss: -403.737518]\n",
      "[Epoch 12/500] [Batch 60/250] [D loss: -6.049724] [G loss: -444.814453]\n",
      "[Epoch 12/500] [Batch 65/250] [D loss: -6.434300] [G loss: -483.888733]\n",
      "[Epoch 12/500] [Batch 70/250] [D loss: -6.206717] [G loss: -523.183838]\n",
      "[Epoch 12/500] [Batch 75/250] [D loss: -6.242768] [G loss: -560.882263]\n",
      "[Epoch 12/500] [Batch 80/250] [D loss: -3.591074] [G loss: -599.748535]\n",
      "[Epoch 12/500] [Batch 85/250] [D loss: -3.469603] [G loss: -633.637146]\n",
      "[Epoch 12/500] [Batch 90/250] [D loss: -2.876050] [G loss: -665.118286]\n",
      "[Epoch 12/500] [Batch 95/250] [D loss: 2.011347] [G loss: -697.420349]\n",
      "[Epoch 12/500] [Batch 100/250] [D loss: 0.460734] [G loss: -718.844543]\n",
      "[Epoch 12/500] [Batch 105/250] [D loss: 4.061835] [G loss: -740.962158]\n",
      "[Epoch 12/500] [Batch 110/250] [D loss: 4.568779] [G loss: -755.443420]\n",
      "[Epoch 12/500] [Batch 115/250] [D loss: 6.206396] [G loss: -766.063416]\n",
      "[Epoch 12/500] [Batch 120/250] [D loss: 6.378593] [G loss: -770.410767]\n",
      "[Epoch 12/500] [Batch 125/250] [D loss: 6.604485] [G loss: -769.621887]\n",
      "[Epoch 12/500] [Batch 130/250] [D loss: 7.384836] [G loss: -764.671753]\n",
      "[Epoch 12/500] [Batch 135/250] [D loss: 5.373272] [G loss: -752.282104]\n",
      "[Epoch 12/500] [Batch 140/250] [D loss: 8.009296] [G loss: -739.725403]\n",
      "[Epoch 12/500] [Batch 145/250] [D loss: 4.654100] [G loss: -717.757812]\n",
      "[Epoch 12/500] [Batch 150/250] [D loss: 5.311685] [G loss: -696.541626]\n",
      "[Epoch 12/500] [Batch 155/250] [D loss: 3.854243] [G loss: -669.504883]\n",
      "[Epoch 12/500] [Batch 160/250] [D loss: 2.861403] [G loss: -639.949158]\n",
      "[Epoch 12/500] [Batch 165/250] [D loss: 0.143937] [G loss: -606.327332]\n",
      "[Epoch 12/500] [Batch 170/250] [D loss: 0.816638] [G loss: -573.060486]\n",
      "[Epoch 12/500] [Batch 175/250] [D loss: -0.772048] [G loss: -535.312683]\n",
      "[Epoch 12/500] [Batch 180/250] [D loss: -1.786520] [G loss: -496.024414]\n",
      "[Epoch 12/500] [Batch 185/250] [D loss: -1.318287] [G loss: -456.369415]\n",
      "[Epoch 12/500] [Batch 190/250] [D loss: -2.051763] [G loss: -415.106873]\n",
      "[Epoch 12/500] [Batch 195/250] [D loss: -2.045888] [G loss: -373.906738]\n",
      "[Epoch 12/500] [Batch 200/250] [D loss: -2.263224] [G loss: -332.008942]\n",
      "[Epoch 12/500] [Batch 205/250] [D loss: -2.383311] [G loss: -290.574524]\n",
      "[Epoch 12/500] [Batch 210/250] [D loss: -1.860380] [G loss: -250.078690]\n",
      "[Epoch 12/500] [Batch 215/250] [D loss: -0.788449] [G loss: -211.111328]\n",
      "[Epoch 12/500] [Batch 220/250] [D loss: 0.295135] [G loss: -173.035156]\n",
      "[Epoch 12/500] [Batch 225/250] [D loss: 1.055186] [G loss: -135.620148]\n",
      "[Epoch 12/500] [Batch 230/250] [D loss: 1.971261] [G loss: -99.712646]\n",
      "[Epoch 12/500] [Batch 235/250] [D loss: 2.891362] [G loss: -65.734764]\n",
      "[Epoch 12/500] [Batch 240/250] [D loss: 3.721453] [G loss: -33.281551]\n",
      "[Epoch 12/500] [Batch 245/250] [D loss: 4.183880] [G loss: -1.550771]\n",
      "[Epoch 13/500] [Batch 0/250] [D loss: 4.683747] [G loss: 29.711782]\n",
      "[Epoch 13/500] [Batch 5/250] [D loss: 4.792500] [G loss: 59.875443]\n",
      "[Epoch 13/500] [Batch 10/250] [D loss: 4.748632] [G loss: 88.783455]\n",
      "[Epoch 13/500] [Batch 15/250] [D loss: 3.899666] [G loss: 118.412529]\n",
      "[Epoch 13/500] [Batch 20/250] [D loss: 3.329183] [G loss: 148.935196]\n",
      "[Epoch 13/500] [Batch 25/250] [D loss: 2.608048] [G loss: 180.807495]\n",
      "[Epoch 13/500] [Batch 30/250] [D loss: 1.841449] [G loss: 213.751144]\n",
      "[Epoch 13/500] [Batch 35/250] [D loss: 1.496902] [G loss: 247.190506]\n",
      "[Epoch 13/500] [Batch 40/250] [D loss: 1.316392] [G loss: 279.882385]\n",
      "[Epoch 13/500] [Batch 45/250] [D loss: 1.423951] [G loss: 312.032013]\n",
      "[Epoch 13/500] [Batch 50/250] [D loss: 0.842261] [G loss: 345.135376]\n",
      "[Epoch 13/500] [Batch 55/250] [D loss: 1.584978] [G loss: 376.208984]\n",
      "[Epoch 13/500] [Batch 60/250] [D loss: 1.674698] [G loss: 406.898590]\n",
      "[Epoch 13/500] [Batch 65/250] [D loss: 3.409007] [G loss: 435.152924]\n",
      "[Epoch 13/500] [Batch 70/250] [D loss: 3.602180] [G loss: 463.762573]\n",
      "[Epoch 13/500] [Batch 75/250] [D loss: 3.647228] [G loss: 491.817535]\n",
      "[Epoch 13/500] [Batch 80/250] [D loss: 5.018397] [G loss: 517.258728]\n",
      "[Epoch 13/500] [Batch 85/250] [D loss: 5.210236] [G loss: 542.352966]\n",
      "[Epoch 13/500] [Batch 90/250] [D loss: 6.223334] [G loss: 565.240601]\n",
      "[Epoch 13/500] [Batch 95/250] [D loss: 8.722530] [G loss: 585.176453]\n",
      "[Epoch 13/500] [Batch 100/250] [D loss: 8.515015] [G loss: 606.130737]\n",
      "[Epoch 13/500] [Batch 105/250] [D loss: 10.506870] [G loss: 622.995850]\n",
      "[Epoch 13/500] [Batch 110/250] [D loss: 10.189441] [G loss: 640.161682]\n",
      "[Epoch 13/500] [Batch 115/250] [D loss: 11.802726] [G loss: 653.458862]\n",
      "[Epoch 13/500] [Batch 120/250] [D loss: 11.440200] [G loss: 666.406372]\n",
      "[Epoch 13/500] [Batch 125/250] [D loss: 13.314518] [G loss: 674.781372]\n",
      "[Epoch 13/500] [Batch 130/250] [D loss: 13.547754] [G loss: 682.439819]\n",
      "[Epoch 13/500] [Batch 135/250] [D loss: 14.291119] [G loss: 687.251831]\n",
      "[Epoch 13/500] [Batch 140/250] [D loss: 14.438637] [G loss: 690.156555]\n",
      "[Epoch 13/500] [Batch 145/250] [D loss: 15.685574] [G loss: 689.502808]\n",
      "[Epoch 13/500] [Batch 150/250] [D loss: 14.878181] [G loss: 688.237854]\n",
      "[Epoch 13/500] [Batch 155/250] [D loss: 14.253797] [G loss: 684.464233]\n",
      "[Epoch 13/500] [Batch 160/250] [D loss: 12.795965] [G loss: 679.200562]\n",
      "[Epoch 13/500] [Batch 165/250] [D loss: 14.246106] [G loss: 668.422791]\n",
      "[Epoch 13/500] [Batch 170/250] [D loss: 14.994440] [G loss: 656.297119]\n",
      "[Epoch 13/500] [Batch 175/250] [D loss: 11.482709] [G loss: 645.980591]\n",
      "[Epoch 13/500] [Batch 180/250] [D loss: 12.514244] [G loss: 629.199341]\n",
      "[Epoch 13/500] [Batch 185/250] [D loss: 10.726034] [G loss: 613.092285]\n",
      "[Epoch 13/500] [Batch 190/250] [D loss: 10.799355] [G loss: 593.365051]\n",
      "[Epoch 13/500] [Batch 195/250] [D loss: 8.941216] [G loss: 573.678772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/500] [Batch 200/250] [D loss: 7.757856] [G loss: 551.642212]\n",
      "[Epoch 13/500] [Batch 205/250] [D loss: 7.277392] [G loss: 527.211548]\n",
      "[Epoch 13/500] [Batch 210/250] [D loss: 5.930305] [G loss: 502.242218]\n",
      "[Epoch 13/500] [Batch 215/250] [D loss: 6.473877] [G loss: 474.390808]\n",
      "[Epoch 13/500] [Batch 220/250] [D loss: 6.131089] [G loss: 446.053070]\n",
      "[Epoch 13/500] [Batch 225/250] [D loss: 4.132030] [G loss: 418.388184]\n",
      "[Epoch 13/500] [Batch 230/250] [D loss: 4.006530] [G loss: 388.234100]\n",
      "[Epoch 13/500] [Batch 235/250] [D loss: 3.130096] [G loss: 358.050079]\n",
      "[Epoch 13/500] [Batch 240/250] [D loss: 3.274602] [G loss: 326.382416]\n",
      "[Epoch 13/500] [Batch 245/250] [D loss: 3.286685] [G loss: 294.445251]\n",
      "[Epoch 14/500] [Batch 0/250] [D loss: 2.172831] [G loss: 263.526978]\n",
      "[Epoch 14/500] [Batch 5/250] [D loss: 2.164679] [G loss: 231.603134]\n",
      "[Epoch 14/500] [Batch 10/250] [D loss: 2.838851] [G loss: 199.146423]\n",
      "[Epoch 14/500] [Batch 15/250] [D loss: 3.041368] [G loss: 167.657501]\n",
      "[Epoch 14/500] [Batch 20/250] [D loss: 3.298833] [G loss: 137.120667]\n",
      "[Epoch 14/500] [Batch 25/250] [D loss: 4.179010] [G loss: 106.451622]\n",
      "[Epoch 14/500] [Batch 30/250] [D loss: 3.937359] [G loss: 77.167336]\n",
      "[Epoch 14/500] [Batch 35/250] [D loss: 4.084593] [G loss: 47.832294]\n",
      "[Epoch 14/500] [Batch 40/250] [D loss: 3.980212] [G loss: 18.675714]\n",
      "[Epoch 14/500] [Batch 45/250] [D loss: 3.615349] [G loss: -10.799496]\n",
      "[Epoch 14/500] [Batch 50/250] [D loss: 2.979138] [G loss: -41.012508]\n",
      "[Epoch 14/500] [Batch 55/250] [D loss: 2.028387] [G loss: -72.330322]\n",
      "[Epoch 14/500] [Batch 60/250] [D loss: 0.975771] [G loss: -104.978622]\n",
      "[Epoch 14/500] [Batch 65/250] [D loss: -0.333766] [G loss: -139.751389]\n",
      "[Epoch 14/500] [Batch 70/250] [D loss: -1.446940] [G loss: -176.610641]\n",
      "[Epoch 14/500] [Batch 75/250] [D loss: -2.534683] [G loss: -216.224365]\n",
      "[Epoch 14/500] [Batch 80/250] [D loss: -3.519057] [G loss: -256.861633]\n",
      "[Epoch 14/500] [Batch 85/250] [D loss: -3.956137] [G loss: -298.927460]\n",
      "[Epoch 14/500] [Batch 90/250] [D loss: -4.960611] [G loss: -340.933319]\n",
      "[Epoch 14/500] [Batch 95/250] [D loss: -5.015439] [G loss: -383.994537]\n",
      "[Epoch 14/500] [Batch 100/250] [D loss: -4.682826] [G loss: -427.189484]\n",
      "[Epoch 14/500] [Batch 105/250] [D loss: -3.822552] [G loss: -470.243378]\n",
      "[Epoch 14/500] [Batch 110/250] [D loss: -3.633086] [G loss: -512.064697]\n",
      "[Epoch 14/500] [Batch 115/250] [D loss: -1.595963] [G loss: -554.046448]\n",
      "[Epoch 14/500] [Batch 120/250] [D loss: -0.239973] [G loss: -592.984558]\n",
      "[Epoch 14/500] [Batch 125/250] [D loss: 2.083423] [G loss: -630.300354]\n",
      "[Epoch 14/500] [Batch 130/250] [D loss: 3.131033] [G loss: -663.308899]\n",
      "[Epoch 14/500] [Batch 135/250] [D loss: 3.114708] [G loss: -691.296570]\n",
      "[Epoch 14/500] [Batch 140/250] [D loss: 8.930908] [G loss: -721.455200]\n",
      "[Epoch 14/500] [Batch 145/250] [D loss: 7.275236] [G loss: -739.930786]\n",
      "[Epoch 14/500] [Batch 150/250] [D loss: 10.603157] [G loss: -759.302246]\n",
      "[Epoch 14/500] [Batch 155/250] [D loss: 9.955530] [G loss: -770.604187]\n",
      "[Epoch 14/500] [Batch 160/250] [D loss: 11.347071] [G loss: -779.656067]\n",
      "[Epoch 14/500] [Batch 165/250] [D loss: 10.391430] [G loss: -781.922485]\n",
      "[Epoch 14/500] [Batch 170/250] [D loss: 11.371220] [G loss: -782.195740]\n",
      "[Epoch 14/500] [Batch 175/250] [D loss: 13.198341] [G loss: -779.046875]\n",
      "[Epoch 14/500] [Batch 180/250] [D loss: 10.935406] [G loss: -767.947571]\n",
      "[Epoch 14/500] [Batch 185/250] [D loss: 11.843998] [G loss: -756.203735]\n",
      "[Epoch 14/500] [Batch 190/250] [D loss: 9.605951] [G loss: -739.360229]\n",
      "[Epoch 14/500] [Batch 195/250] [D loss: 9.693228] [G loss: -722.845215]\n",
      "[Epoch 14/500] [Batch 200/250] [D loss: 5.956500] [G loss: -699.527161]\n",
      "[Epoch 14/500] [Batch 205/250] [D loss: 6.952319] [G loss: -678.330933]\n",
      "[Epoch 14/500] [Batch 210/250] [D loss: 5.087995] [G loss: -652.135315]\n",
      "[Epoch 14/500] [Batch 215/250] [D loss: 4.851993] [G loss: -626.242188]\n",
      "[Epoch 14/500] [Batch 220/250] [D loss: 3.017629] [G loss: -596.642456]\n",
      "[Epoch 14/500] [Batch 225/250] [D loss: 0.964398] [G loss: -564.768799]\n",
      "[Epoch 14/500] [Batch 230/250] [D loss: 0.662224] [G loss: -532.908813]\n",
      "[Epoch 14/500] [Batch 235/250] [D loss: -0.635062] [G loss: -498.440277]\n",
      "[Epoch 14/500] [Batch 240/250] [D loss: -0.594454] [G loss: -463.975739]\n",
      "[Epoch 14/500] [Batch 245/250] [D loss: -1.637424] [G loss: -427.385468]\n",
      "[Epoch 15/500] [Batch 0/250] [D loss: -1.895459] [G loss: -390.585449]\n",
      "[Epoch 15/500] [Batch 5/250] [D loss: -2.642220] [G loss: -352.817444]\n",
      "[Epoch 15/500] [Batch 10/250] [D loss: -2.602055] [G loss: -315.492493]\n",
      "[Epoch 15/500] [Batch 15/250] [D loss: -2.194841] [G loss: -278.510132]\n",
      "[Epoch 15/500] [Batch 20/250] [D loss: -2.328498] [G loss: -241.072083]\n",
      "[Epoch 15/500] [Batch 25/250] [D loss: -2.020452] [G loss: -204.597382]\n",
      "[Epoch 15/500] [Batch 30/250] [D loss: -1.437535] [G loss: -169.461426]\n",
      "[Epoch 15/500] [Batch 35/250] [D loss: -0.979782] [G loss: -135.125992]\n",
      "[Epoch 15/500] [Batch 40/250] [D loss: -0.294936] [G loss: -101.808876]\n",
      "[Epoch 15/500] [Batch 45/250] [D loss: -0.070508] [G loss: -68.856293]\n",
      "[Epoch 15/500] [Batch 50/250] [D loss: 0.314947] [G loss: -36.805355]\n",
      "[Epoch 15/500] [Batch 55/250] [D loss: 0.577365] [G loss: -5.244139]\n",
      "[Epoch 15/500] [Batch 60/250] [D loss: 0.640185] [G loss: 25.985086]\n",
      "[Epoch 15/500] [Batch 65/250] [D loss: 0.641367] [G loss: 57.052902]\n",
      "[Epoch 15/500] [Batch 70/250] [D loss: 0.466800] [G loss: 88.361618]\n",
      "[Epoch 15/500] [Batch 75/250] [D loss: 0.115207] [G loss: 119.978233]\n",
      "[Epoch 15/500] [Batch 80/250] [D loss: 0.157067] [G loss: 151.378952]\n",
      "[Epoch 15/500] [Batch 85/250] [D loss: -0.390723] [G loss: 183.619370]\n",
      "[Epoch 15/500] [Batch 90/250] [D loss: -0.230267] [G loss: 215.436005]\n",
      "[Epoch 15/500] [Batch 95/250] [D loss: -0.356678] [G loss: 247.610275]\n",
      "[Epoch 15/500] [Batch 100/250] [D loss: -0.610598] [G loss: 279.905670]\n",
      "[Epoch 15/500] [Batch 105/250] [D loss: 0.108526] [G loss: 311.090240]\n",
      "[Epoch 15/500] [Batch 110/250] [D loss: 0.703901] [G loss: 342.046509]\n",
      "[Epoch 15/500] [Batch 115/250] [D loss: 0.410512] [G loss: 373.143463]\n",
      "[Epoch 15/500] [Batch 120/250] [D loss: 1.294004] [G loss: 402.091522]\n",
      "[Epoch 15/500] [Batch 125/250] [D loss: 2.158249] [G loss: 429.869843]\n",
      "[Epoch 15/500] [Batch 130/250] [D loss: 2.977706] [G loss: 456.192322]\n",
      "[Epoch 15/500] [Batch 135/250] [D loss: 4.066793] [G loss: 480.474091]\n",
      "[Epoch 15/500] [Batch 140/250] [D loss: 4.967218] [G loss: 502.713379]\n",
      "[Epoch 15/500] [Batch 145/250] [D loss: 6.092223] [G loss: 522.512634]\n",
      "[Epoch 15/500] [Batch 150/250] [D loss: 7.012192] [G loss: 539.776367]\n",
      "[Epoch 15/500] [Batch 155/250] [D loss: 7.701080] [G loss: 554.394348]\n",
      "[Epoch 15/500] [Batch 160/250] [D loss: 8.409159] [G loss: 565.849182]\n",
      "[Epoch 15/500] [Batch 165/250] [D loss: 8.334188] [G loss: 574.961853]\n",
      "[Epoch 15/500] [Batch 170/250] [D loss: 10.040155] [G loss: 578.892700]\n",
      "[Epoch 15/500] [Batch 175/250] [D loss: 9.655309] [G loss: 581.574646]\n",
      "[Epoch 15/500] [Batch 180/250] [D loss: 8.928050] [G loss: 581.267151]\n",
      "[Epoch 15/500] [Batch 185/250] [D loss: 10.011736] [G loss: 575.669861]\n",
      "[Epoch 15/500] [Batch 190/250] [D loss: 10.963256] [G loss: 567.062622]\n",
      "[Epoch 15/500] [Batch 195/250] [D loss: 9.796186] [G loss: 557.224182]\n",
      "[Epoch 15/500] [Batch 200/250] [D loss: 8.370385] [G loss: 544.702393]\n",
      "[Epoch 15/500] [Batch 205/250] [D loss: 9.164505] [G loss: 526.889404]\n",
      "[Epoch 15/500] [Batch 210/250] [D loss: 11.580898] [G loss: 504.814545]\n",
      "[Epoch 15/500] [Batch 215/250] [D loss: 8.491278] [G loss: 485.559875]\n",
      "[Epoch 15/500] [Batch 220/250] [D loss: 7.491141] [G loss: 461.945343]\n",
      "[Epoch 15/500] [Batch 225/250] [D loss: 6.915205] [G loss: 435.534454]\n",
      "[Epoch 15/500] [Batch 230/250] [D loss: 6.097804] [G loss: 408.000977]\n",
      "[Epoch 15/500] [Batch 235/250] [D loss: 6.234761] [G loss: 377.881195]\n",
      "[Epoch 15/500] [Batch 240/250] [D loss: 7.405228] [G loss: 346.010437]\n",
      "[Epoch 15/500] [Batch 245/250] [D loss: 6.578294] [G loss: 315.515839]\n",
      "[Epoch 16/500] [Batch 0/250] [D loss: 7.283327] [G loss: 282.356628]\n",
      "[Epoch 16/500] [Batch 5/250] [D loss: 6.066000] [G loss: 250.279709]\n",
      "[Epoch 16/500] [Batch 10/250] [D loss: 7.730804] [G loss: 214.863419]\n",
      "[Epoch 16/500] [Batch 15/250] [D loss: 7.514105] [G loss: 180.573837]\n",
      "[Epoch 16/500] [Batch 20/250] [D loss: 8.136017] [G loss: 144.769104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/500] [Batch 25/250] [D loss: 8.425225] [G loss: 108.629829]\n",
      "[Epoch 16/500] [Batch 30/250] [D loss: 8.316772] [G loss: 72.145523]\n",
      "[Epoch 16/500] [Batch 35/250] [D loss: 8.545118] [G loss: 34.557957]\n",
      "[Epoch 16/500] [Batch 40/250] [D loss: 8.027904] [G loss: -3.426916]\n",
      "[Epoch 16/500] [Batch 45/250] [D loss: 7.984835] [G loss: -42.885723]\n",
      "[Epoch 16/500] [Batch 50/250] [D loss: 7.437178] [G loss: -83.299530]\n",
      "[Epoch 16/500] [Batch 55/250] [D loss: 6.485192] [G loss: -124.869156]\n",
      "[Epoch 16/500] [Batch 60/250] [D loss: 5.559310] [G loss: -168.069977]\n",
      "[Epoch 16/500] [Batch 65/250] [D loss: 4.402359] [G loss: -212.516464]\n",
      "[Epoch 16/500] [Batch 70/250] [D loss: 3.473150] [G loss: -258.524475]\n",
      "[Epoch 16/500] [Batch 75/250] [D loss: 2.052891] [G loss: -305.108948]\n",
      "[Epoch 16/500] [Batch 80/250] [D loss: 1.468907] [G loss: -353.466522]\n",
      "[Epoch 16/500] [Batch 85/250] [D loss: -0.607627] [G loss: -400.764221]\n",
      "[Epoch 16/500] [Batch 90/250] [D loss: -0.776999] [G loss: -450.244141]\n",
      "[Epoch 16/500] [Batch 95/250] [D loss: -2.088937] [G loss: -498.308594]\n",
      "[Epoch 16/500] [Batch 100/250] [D loss: -0.378387] [G loss: -548.968445]\n",
      "[Epoch 16/500] [Batch 105/250] [D loss: -1.860107] [G loss: -595.126221]\n",
      "[Epoch 16/500] [Batch 110/250] [D loss: -0.060541] [G loss: -643.187378]\n",
      "[Epoch 16/500] [Batch 115/250] [D loss: 0.719736] [G loss: -688.187012]\n",
      "[Epoch 16/500] [Batch 120/250] [D loss: 2.318076] [G loss: -731.421448]\n",
      "[Epoch 16/500] [Batch 125/250] [D loss: 0.640598] [G loss: -768.086182]\n",
      "[Epoch 16/500] [Batch 130/250] [D loss: 4.743668] [G loss: -807.422546]\n",
      "[Epoch 16/500] [Batch 135/250] [D loss: 8.491211] [G loss: -842.770935]\n",
      "[Epoch 16/500] [Batch 140/250] [D loss: 11.241526] [G loss: -873.166321]\n",
      "[Epoch 16/500] [Batch 145/250] [D loss: 13.476467] [G loss: -898.789124]\n",
      "[Epoch 16/500] [Batch 150/250] [D loss: 13.731943] [G loss: -918.161560]\n",
      "[Epoch 16/500] [Batch 155/250] [D loss: 16.788193] [G loss: -936.330139]\n",
      "[Epoch 16/500] [Batch 160/250] [D loss: 16.267944] [G loss: -946.449219]\n",
      "[Epoch 16/500] [Batch 165/250] [D loss: 18.256680] [G loss: -954.660522]\n",
      "[Epoch 16/500] [Batch 170/250] [D loss: 20.452381] [G loss: -958.503601]\n",
      "[Epoch 16/500] [Batch 175/250] [D loss: 21.141556] [G loss: -956.223328]\n",
      "[Epoch 16/500] [Batch 180/250] [D loss: 22.491013] [G loss: -950.573120]\n",
      "[Epoch 16/500] [Batch 185/250] [D loss: 18.619850] [G loss: -935.430359]\n",
      "[Epoch 16/500] [Batch 190/250] [D loss: 21.655085] [G loss: -922.865784]\n",
      "[Epoch 16/500] [Batch 195/250] [D loss: 19.726950] [G loss: -901.482971]\n",
      "[Epoch 16/500] [Batch 200/250] [D loss: 16.455029] [G loss: -874.813782]\n",
      "[Epoch 16/500] [Batch 205/250] [D loss: 14.652834] [G loss: -846.400208]\n",
      "[Epoch 16/500] [Batch 210/250] [D loss: 12.865419] [G loss: -814.440674]\n",
      "[Epoch 16/500] [Batch 215/250] [D loss: 10.109997] [G loss: -778.132629]\n",
      "[Epoch 16/500] [Batch 220/250] [D loss: 8.703590] [G loss: -739.978333]\n",
      "[Epoch 16/500] [Batch 225/250] [D loss: 6.402825] [G loss: -698.417969]\n",
      "[Epoch 16/500] [Batch 230/250] [D loss: 4.454474] [G loss: -654.374146]\n",
      "[Epoch 16/500] [Batch 235/250] [D loss: 2.601072] [G loss: -608.217346]\n",
      "[Epoch 16/500] [Batch 240/250] [D loss: 0.858083] [G loss: -560.229126]\n",
      "[Epoch 16/500] [Batch 245/250] [D loss: -0.841332] [G loss: -510.383514]\n",
      "[Epoch 17/500] [Batch 0/250] [D loss: -1.316335] [G loss: -460.406555]\n",
      "[Epoch 17/500] [Batch 5/250] [D loss: -1.823073] [G loss: -409.374512]\n",
      "[Epoch 17/500] [Batch 10/250] [D loss: -2.378703] [G loss: -357.533051]\n",
      "[Epoch 17/500] [Batch 15/250] [D loss: -2.284817] [G loss: -305.972717]\n",
      "[Epoch 17/500] [Batch 20/250] [D loss: -1.693042] [G loss: -254.745621]\n",
      "[Epoch 17/500] [Batch 25/250] [D loss: -0.689777] [G loss: -204.401505]\n",
      "[Epoch 17/500] [Batch 30/250] [D loss: 0.199625] [G loss: -154.824432]\n",
      "[Epoch 17/500] [Batch 35/250] [D loss: 1.445803] [G loss: -106.887383]\n",
      "[Epoch 17/500] [Batch 40/250] [D loss: 2.123673] [G loss: -59.703293]\n",
      "[Epoch 17/500] [Batch 45/250] [D loss: 2.882511] [G loss: -13.762652]\n",
      "[Epoch 17/500] [Batch 50/250] [D loss: 3.087683] [G loss: 31.603796]\n",
      "[Epoch 17/500] [Batch 55/250] [D loss: 2.870182] [G loss: 76.918190]\n",
      "[Epoch 17/500] [Batch 60/250] [D loss: 2.516621] [G loss: 122.456894]\n",
      "[Epoch 17/500] [Batch 65/250] [D loss: 2.032161] [G loss: 168.416428]\n",
      "[Epoch 17/500] [Batch 70/250] [D loss: 1.507722] [G loss: 214.941895]\n",
      "[Epoch 17/500] [Batch 75/250] [D loss: 1.066415] [G loss: 261.812500]\n",
      "[Epoch 17/500] [Batch 80/250] [D loss: 1.382209] [G loss: 307.956512]\n",
      "[Epoch 17/500] [Batch 85/250] [D loss: 1.262677] [G loss: 354.473694]\n",
      "[Epoch 17/500] [Batch 90/250] [D loss: 2.545153] [G loss: 399.288635]\n",
      "[Epoch 17/500] [Batch 95/250] [D loss: 2.479043] [G loss: 444.327057]\n",
      "[Epoch 17/500] [Batch 100/250] [D loss: 4.493421] [G loss: 486.222382]\n",
      "[Epoch 17/500] [Batch 105/250] [D loss: 5.176395] [G loss: 527.742798]\n",
      "[Epoch 17/500] [Batch 110/250] [D loss: 8.041428] [G loss: 564.848083]\n",
      "[Epoch 17/500] [Batch 115/250] [D loss: 9.821869] [G loss: 600.676697]\n",
      "[Epoch 17/500] [Batch 120/250] [D loss: 11.302686] [G loss: 633.958008]\n",
      "[Epoch 17/500] [Batch 125/250] [D loss: 14.260988] [G loss: 662.233276]\n",
      "[Epoch 17/500] [Batch 130/250] [D loss: 15.872980] [G loss: 688.015137]\n",
      "[Epoch 17/500] [Batch 135/250] [D loss: 17.230881] [G loss: 709.354858]\n",
      "[Epoch 17/500] [Batch 140/250] [D loss: 18.346136] [G loss: 727.001465]\n",
      "[Epoch 17/500] [Batch 145/250] [D loss: 19.796017] [G loss: 739.639038]\n",
      "[Epoch 17/500] [Batch 150/250] [D loss: 21.021206] [G loss: 748.134338]\n",
      "[Epoch 17/500] [Batch 155/250] [D loss: 21.655449] [G loss: 752.881958]\n",
      "[Epoch 17/500] [Batch 160/250] [D loss: 20.903164] [G loss: 754.310059]\n",
      "[Epoch 17/500] [Batch 165/250] [D loss: 21.849323] [G loss: 749.740662]\n",
      "[Epoch 17/500] [Batch 170/250] [D loss: 20.921730] [G loss: 742.982727]\n",
      "[Epoch 17/500] [Batch 175/250] [D loss: 19.794529] [G loss: 732.010742]\n",
      "[Epoch 17/500] [Batch 180/250] [D loss: 20.224136] [G loss: 715.636047]\n",
      "[Epoch 17/500] [Batch 185/250] [D loss: 19.055170] [G loss: 697.756836]\n",
      "[Epoch 17/500] [Batch 190/250] [D loss: 17.425787] [G loss: 676.414734]\n",
      "[Epoch 17/500] [Batch 195/250] [D loss: 17.172413] [G loss: 650.986206]\n",
      "[Epoch 17/500] [Batch 200/250] [D loss: 16.027260] [G loss: 623.379211]\n",
      "[Epoch 17/500] [Batch 205/250] [D loss: 14.518722] [G loss: 593.040833]\n",
      "[Epoch 17/500] [Batch 210/250] [D loss: 11.560787] [G loss: 561.667664]\n",
      "[Epoch 17/500] [Batch 215/250] [D loss: 9.546193] [G loss: 527.165283]\n",
      "[Epoch 17/500] [Batch 220/250] [D loss: 10.956284] [G loss: 487.312225]\n",
      "[Epoch 17/500] [Batch 225/250] [D loss: 7.965514] [G loss: 450.868500]\n",
      "[Epoch 17/500] [Batch 230/250] [D loss: 6.701275] [G loss: 411.544861]\n",
      "[Epoch 17/500] [Batch 235/250] [D loss: 5.822723] [G loss: 370.768677]\n",
      "[Epoch 17/500] [Batch 240/250] [D loss: 4.122733] [G loss: 329.718597]\n",
      "[Epoch 17/500] [Batch 245/250] [D loss: 3.649119] [G loss: 286.961456]\n",
      "[Epoch 18/500] [Batch 0/250] [D loss: 2.673794] [G loss: 244.129532]\n",
      "[Epoch 18/500] [Batch 5/250] [D loss: 2.828522] [G loss: 199.944778]\n",
      "[Epoch 18/500] [Batch 10/250] [D loss: 2.531517] [G loss: 156.025421]\n",
      "[Epoch 18/500] [Batch 15/250] [D loss: 1.983531] [G loss: 112.025620]\n",
      "[Epoch 18/500] [Batch 20/250] [D loss: 1.588691] [G loss: 67.559364]\n",
      "[Epoch 18/500] [Batch 25/250] [D loss: 1.150052] [G loss: 22.661173]\n",
      "[Epoch 18/500] [Batch 30/250] [D loss: 0.442002] [G loss: -22.355621]\n",
      "[Epoch 18/500] [Batch 35/250] [D loss: 0.064471] [G loss: -68.418968]\n",
      "[Epoch 18/500] [Batch 40/250] [D loss: -0.579731] [G loss: -115.028404]\n",
      "[Epoch 18/500] [Batch 45/250] [D loss: -1.872320] [G loss: -161.505386]\n",
      "[Epoch 18/500] [Batch 50/250] [D loss: -1.638145] [G loss: -210.078888]\n",
      "[Epoch 18/500] [Batch 55/250] [D loss: -3.215487] [G loss: -256.896423]\n",
      "[Epoch 18/500] [Batch 60/250] [D loss: -1.881176] [G loss: -306.672974]\n",
      "[Epoch 18/500] [Batch 65/250] [D loss: -0.832335] [G loss: -355.639221]\n",
      "[Epoch 18/500] [Batch 70/250] [D loss: -1.630928] [G loss: -401.938507]\n",
      "[Epoch 18/500] [Batch 75/250] [D loss: 0.531618] [G loss: -449.488068]\n",
      "[Epoch 18/500] [Batch 80/250] [D loss: 1.953088] [G loss: -494.432922]\n",
      "[Epoch 18/500] [Batch 85/250] [D loss: 3.740954] [G loss: -538.053223]\n",
      "[Epoch 18/500] [Batch 90/250] [D loss: 4.684581] [G loss: -578.742126]\n",
      "[Epoch 18/500] [Batch 95/250] [D loss: 8.412651] [G loss: -619.731018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/500] [Batch 100/250] [D loss: 8.402599] [G loss: -654.252747]\n",
      "[Epoch 18/500] [Batch 105/250] [D loss: 12.070439] [G loss: -689.552002]\n",
      "[Epoch 18/500] [Batch 110/250] [D loss: 12.855333] [G loss: -720.071106]\n",
      "[Epoch 18/500] [Batch 115/250] [D loss: 14.565430] [G loss: -746.852478]\n",
      "[Epoch 18/500] [Batch 120/250] [D loss: 17.935595] [G loss: -772.189758]\n",
      "[Epoch 18/500] [Batch 125/250] [D loss: 18.931620] [G loss: -791.007568]\n",
      "[Epoch 18/500] [Batch 130/250] [D loss: 21.642246] [G loss: -807.355530]\n",
      "[Epoch 18/500] [Batch 135/250] [D loss: 21.715052] [G loss: -817.146057]\n",
      "[Epoch 18/500] [Batch 140/250] [D loss: 23.222876] [G loss: -824.848877]\n",
      "[Epoch 18/500] [Batch 145/250] [D loss: 24.063042] [G loss: -827.468872]\n",
      "[Epoch 18/500] [Batch 150/250] [D loss: 24.382217] [G loss: -826.062378]\n",
      "[Epoch 18/500] [Batch 155/250] [D loss: 22.885365] [G loss: -818.577637]\n",
      "[Epoch 18/500] [Batch 160/250] [D loss: 23.501842] [G loss: -809.846619]\n",
      "[Epoch 18/500] [Batch 165/250] [D loss: 22.982716] [G loss: -796.286865]\n",
      "[Epoch 18/500] [Batch 170/250] [D loss: 19.141993] [G loss: -775.927429]\n",
      "[Epoch 18/500] [Batch 175/250] [D loss: 18.441513] [G loss: -755.441284]\n",
      "[Epoch 18/500] [Batch 180/250] [D loss: 17.169451] [G loss: -731.149841]\n",
      "[Epoch 18/500] [Batch 185/250] [D loss: 15.911769] [G loss: -704.063110]\n",
      "[Epoch 18/500] [Batch 190/250] [D loss: 12.558021] [G loss: -671.964966]\n",
      "[Epoch 18/500] [Batch 195/250] [D loss: 11.418541] [G loss: -639.676636]\n",
      "[Epoch 18/500] [Batch 200/250] [D loss: 9.904453] [G loss: -604.914124]\n",
      "[Epoch 18/500] [Batch 205/250] [D loss: 8.290648] [G loss: -567.961304]\n",
      "[Epoch 18/500] [Batch 210/250] [D loss: 6.537185] [G loss: -529.022217]\n",
      "[Epoch 18/500] [Batch 215/250] [D loss: 4.758265] [G loss: -488.319489]\n",
      "[Epoch 18/500] [Batch 220/250] [D loss: 4.049430] [G loss: -447.367218]\n",
      "[Epoch 18/500] [Batch 225/250] [D loss: 2.896223] [G loss: -404.765350]\n",
      "[Epoch 18/500] [Batch 230/250] [D loss: 2.479315] [G loss: -361.950806]\n",
      "[Epoch 18/500] [Batch 235/250] [D loss: 2.189216] [G loss: -318.766022]\n",
      "[Epoch 18/500] [Batch 240/250] [D loss: 1.580915] [G loss: -275.345337]\n",
      "[Epoch 18/500] [Batch 245/250] [D loss: 1.388727] [G loss: -232.136169]\n",
      "[Epoch 19/500] [Batch 0/250] [D loss: 1.807601] [G loss: -189.680679]\n",
      "[Epoch 19/500] [Batch 5/250] [D loss: 2.141288] [G loss: -147.967056]\n",
      "[Epoch 19/500] [Batch 10/250] [D loss: 2.484226] [G loss: -107.198463]\n",
      "[Epoch 19/500] [Batch 15/250] [D loss: 2.761480] [G loss: -67.046249]\n",
      "[Epoch 19/500] [Batch 20/250] [D loss: 3.145813] [G loss: -27.720860]\n",
      "[Epoch 19/500] [Batch 25/250] [D loss: 3.244255] [G loss: 11.227101]\n",
      "[Epoch 19/500] [Batch 30/250] [D loss: 3.137319] [G loss: 49.989780]\n",
      "[Epoch 19/500] [Batch 35/250] [D loss: 2.842337] [G loss: 88.773125]\n",
      "[Epoch 19/500] [Batch 40/250] [D loss: 2.297076] [G loss: 127.921684]\n",
      "[Epoch 19/500] [Batch 45/250] [D loss: 1.989444] [G loss: 167.053741]\n",
      "[Epoch 19/500] [Batch 50/250] [D loss: 1.305190] [G loss: 206.901108]\n",
      "[Epoch 19/500] [Batch 55/250] [D loss: 0.941272] [G loss: 246.795059]\n",
      "[Epoch 19/500] [Batch 60/250] [D loss: 0.303148] [G loss: 287.262634]\n",
      "[Epoch 19/500] [Batch 65/250] [D loss: 0.301847] [G loss: 327.190887]\n",
      "[Epoch 19/500] [Batch 70/250] [D loss: 0.268222] [G loss: 366.925781]\n",
      "[Epoch 19/500] [Batch 75/250] [D loss: 1.430848] [G loss: 404.967194]\n",
      "[Epoch 19/500] [Batch 80/250] [D loss: 1.880896] [G loss: 443.385132]\n",
      "[Epoch 19/500] [Batch 85/250] [D loss: 2.881461] [G loss: 480.181305]\n",
      "[Epoch 19/500] [Batch 90/250] [D loss: 3.668625] [G loss: 515.650391]\n",
      "[Epoch 19/500] [Batch 95/250] [D loss: 5.061950] [G loss: 548.832581]\n",
      "[Epoch 19/500] [Batch 100/250] [D loss: 6.420739] [G loss: 580.057556]\n",
      "[Epoch 19/500] [Batch 105/250] [D loss: 7.991668] [G loss: 608.417419]\n",
      "[Epoch 19/500] [Batch 110/250] [D loss: 8.671806] [G loss: 634.982117]\n",
      "[Epoch 19/500] [Batch 115/250] [D loss: 9.627352] [G loss: 658.352234]\n",
      "[Epoch 19/500] [Batch 120/250] [D loss: 11.715980] [G loss: 677.246338]\n",
      "[Epoch 19/500] [Batch 125/250] [D loss: 11.795719] [G loss: 694.738281]\n",
      "[Epoch 19/500] [Batch 130/250] [D loss: 12.414407] [G loss: 708.697510]\n",
      "[Epoch 19/500] [Batch 135/250] [D loss: 13.834918] [G loss: 718.044373]\n",
      "[Epoch 19/500] [Batch 140/250] [D loss: 14.680348] [G loss: 724.319824]\n",
      "[Epoch 19/500] [Batch 145/250] [D loss: 15.088659] [G loss: 727.552734]\n",
      "[Epoch 19/500] [Batch 150/250] [D loss: 15.680950] [G loss: 727.065247]\n",
      "[Epoch 19/500] [Batch 155/250] [D loss: 14.420635] [G loss: 724.851868]\n",
      "[Epoch 19/500] [Batch 160/250] [D loss: 14.426244] [G loss: 718.315125]\n",
      "[Epoch 19/500] [Batch 165/250] [D loss: 13.168394] [G loss: 709.915466]\n",
      "[Epoch 19/500] [Batch 170/250] [D loss: 12.033556] [G loss: 698.083496]\n",
      "[Epoch 19/500] [Batch 175/250] [D loss: 11.388120] [G loss: 683.176147]\n",
      "[Epoch 19/500] [Batch 180/250] [D loss: 11.977887] [G loss: 663.980896]\n",
      "[Epoch 19/500] [Batch 185/250] [D loss: 10.287821] [G loss: 644.546326]\n",
      "[Epoch 19/500] [Batch 190/250] [D loss: 8.747133] [G loss: 622.315735]\n",
      "[Epoch 19/500] [Batch 195/250] [D loss: 7.931565] [G loss: 597.257019]\n",
      "[Epoch 19/500] [Batch 200/250] [D loss: 5.932189] [G loss: 571.254089]\n",
      "[Epoch 19/500] [Batch 205/250] [D loss: 5.140519] [G loss: 542.139099]\n",
      "[Epoch 19/500] [Batch 210/250] [D loss: 4.499784] [G loss: 511.412598]\n",
      "[Epoch 19/500] [Batch 215/250] [D loss: 3.294873] [G loss: 479.896271]\n",
      "[Epoch 19/500] [Batch 220/250] [D loss: 2.378109] [G loss: 446.571411]\n",
      "[Epoch 19/500] [Batch 225/250] [D loss: 1.034709] [G loss: 412.438507]\n",
      "[Epoch 19/500] [Batch 230/250] [D loss: 0.000637] [G loss: 376.951935]\n",
      "[Epoch 19/500] [Batch 235/250] [D loss: 0.139922] [G loss: 339.586365]\n",
      "[Epoch 19/500] [Batch 240/250] [D loss: -0.652644] [G loss: 302.776794]\n",
      "[Epoch 19/500] [Batch 245/250] [D loss: -0.642730] [G loss: 264.855927]\n",
      "[Epoch 20/500] [Batch 0/250] [D loss: -1.232802] [G loss: 227.176712]\n",
      "[Epoch 20/500] [Batch 5/250] [D loss: -1.232544] [G loss: 188.677795]\n",
      "[Epoch 20/500] [Batch 10/250] [D loss: -1.651826] [G loss: 150.525101]\n",
      "[Epoch 20/500] [Batch 15/250] [D loss: -1.990464] [G loss: 112.253433]\n",
      "[Epoch 20/500] [Batch 20/250] [D loss: -1.682096] [G loss: 73.331642]\n",
      "[Epoch 20/500] [Batch 25/250] [D loss: -2.077567] [G loss: 34.657291]\n",
      "[Epoch 20/500] [Batch 30/250] [D loss: -2.527390] [G loss: -4.357071]\n",
      "[Epoch 20/500] [Batch 35/250] [D loss: -2.693352] [G loss: -43.859535]\n",
      "[Epoch 20/500] [Batch 40/250] [D loss: -2.934037] [G loss: -83.618553]\n",
      "[Epoch 20/500] [Batch 45/250] [D loss: -3.072425] [G loss: -123.772758]\n",
      "[Epoch 20/500] [Batch 50/250] [D loss: -3.538676] [G loss: -164.045685]\n",
      "[Epoch 20/500] [Batch 55/250] [D loss: -3.372867] [G loss: -204.675400]\n",
      "[Epoch 20/500] [Batch 60/250] [D loss: -3.101372] [G loss: -245.529663]\n",
      "[Epoch 20/500] [Batch 65/250] [D loss: -2.770845] [G loss: -285.910095]\n",
      "[Epoch 20/500] [Batch 70/250] [D loss: -1.662704] [G loss: -326.949554]\n",
      "[Epoch 20/500] [Batch 75/250] [D loss: -1.688295] [G loss: -366.030762]\n",
      "[Epoch 20/500] [Batch 80/250] [D loss: -0.295775] [G loss: -405.291626]\n",
      "[Epoch 20/500] [Batch 85/250] [D loss: 0.887800] [G loss: -442.860199]\n",
      "[Epoch 20/500] [Batch 90/250] [D loss: 2.746936] [G loss: -479.116394]\n",
      "[Epoch 20/500] [Batch 95/250] [D loss: 3.488170] [G loss: -512.119812]\n",
      "[Epoch 20/500] [Batch 100/250] [D loss: 5.229149] [G loss: -543.397461]\n",
      "[Epoch 20/500] [Batch 105/250] [D loss: 7.909674] [G loss: -573.439636]\n",
      "[Epoch 20/500] [Batch 110/250] [D loss: 9.183141] [G loss: -599.219360]\n",
      "[Epoch 20/500] [Batch 115/250] [D loss: 10.973866] [G loss: -622.062378]\n",
      "[Epoch 20/500] [Batch 120/250] [D loss: 13.026796] [G loss: -642.059448]\n",
      "[Epoch 20/500] [Batch 125/250] [D loss: 14.708887] [G loss: -659.113342]\n",
      "[Epoch 20/500] [Batch 130/250] [D loss: 16.214039] [G loss: -672.611633]\n",
      "[Epoch 20/500] [Batch 135/250] [D loss: 16.250433] [G loss: -681.431396]\n",
      "[Epoch 20/500] [Batch 140/250] [D loss: 17.035898] [G loss: -687.777466]\n",
      "[Epoch 20/500] [Batch 145/250] [D loss: 17.398491] [G loss: -690.813660]\n",
      "[Epoch 20/500] [Batch 150/250] [D loss: 16.936665] [G loss: -689.545776]\n",
      "[Epoch 20/500] [Batch 155/250] [D loss: 15.950745] [G loss: -685.292236]\n",
      "[Epoch 20/500] [Batch 160/250] [D loss: 15.972052] [G loss: -678.545837]\n",
      "[Epoch 20/500] [Batch 165/250] [D loss: 14.998752] [G loss: -668.085632]\n",
      "[Epoch 20/500] [Batch 170/250] [D loss: 14.273562] [G loss: -655.128357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/500] [Batch 175/250] [D loss: 11.265419] [G loss: -637.566528]\n",
      "[Epoch 20/500] [Batch 180/250] [D loss: 11.479801] [G loss: -620.890198]\n",
      "[Epoch 20/500] [Batch 185/250] [D loss: 10.498545] [G loss: -600.880615]\n",
      "[Epoch 20/500] [Batch 190/250] [D loss: 8.319132] [G loss: -577.946411]\n",
      "[Epoch 20/500] [Batch 195/250] [D loss: 7.801656] [G loss: -555.111877]\n",
      "[Epoch 20/500] [Batch 200/250] [D loss: 6.669035] [G loss: -529.820923]\n",
      "[Epoch 20/500] [Batch 205/250] [D loss: 5.184715] [G loss: -503.159241]\n",
      "[Epoch 20/500] [Batch 210/250] [D loss: 3.982026] [G loss: -475.661865]\n",
      "[Epoch 20/500] [Batch 215/250] [D loss: 3.055005] [G loss: -447.409821]\n",
      "[Epoch 20/500] [Batch 220/250] [D loss: 1.477794] [G loss: -417.716675]\n",
      "[Epoch 20/500] [Batch 225/250] [D loss: 1.302143] [G loss: -388.961914]\n",
      "[Epoch 20/500] [Batch 230/250] [D loss: 0.357796] [G loss: -358.832642]\n",
      "[Epoch 20/500] [Batch 235/250] [D loss: -0.284384] [G loss: -328.899292]\n",
      "[Epoch 20/500] [Batch 240/250] [D loss: -0.600223] [G loss: -299.224335]\n",
      "[Epoch 20/500] [Batch 245/250] [D loss: -0.877473] [G loss: -269.866974]\n",
      "[Epoch 21/500] [Batch 0/250] [D loss: -1.410768] [G loss: -240.835632]\n",
      "[Epoch 21/500] [Batch 5/250] [D loss: -1.459876] [G loss: -212.704727]\n",
      "[Epoch 21/500] [Batch 10/250] [D loss: -1.292618] [G loss: -185.441208]\n",
      "[Epoch 21/500] [Batch 15/250] [D loss: -1.374358] [G loss: -158.652832]\n",
      "[Epoch 21/500] [Batch 20/250] [D loss: -1.220360] [G loss: -132.793839]\n",
      "[Epoch 21/500] [Batch 25/250] [D loss: -0.994349] [G loss: -107.846664]\n",
      "[Epoch 21/500] [Batch 30/250] [D loss: -0.853502] [G loss: -83.667809]\n",
      "[Epoch 21/500] [Batch 35/250] [D loss: -0.817790] [G loss: -60.265648]\n",
      "[Epoch 21/500] [Batch 40/250] [D loss: -0.779243] [G loss: -37.637508]\n",
      "[Epoch 21/500] [Batch 45/250] [D loss: -0.749264] [G loss: -15.655858]\n",
      "[Epoch 21/500] [Batch 50/250] [D loss: -0.860362] [G loss: 5.846641]\n",
      "[Epoch 21/500] [Batch 55/250] [D loss: -1.103740] [G loss: 26.980516]\n",
      "[Epoch 21/500] [Batch 60/250] [D loss: -1.357542] [G loss: 47.764961]\n",
      "[Epoch 21/500] [Batch 65/250] [D loss: -1.832319] [G loss: 68.526825]\n",
      "[Epoch 21/500] [Batch 70/250] [D loss: -1.871252] [G loss: 88.771606]\n",
      "[Epoch 21/500] [Batch 75/250] [D loss: -2.532206] [G loss: 109.519867]\n",
      "[Epoch 21/500] [Batch 80/250] [D loss: -2.493665] [G loss: 129.550568]\n",
      "[Epoch 21/500] [Batch 85/250] [D loss: -3.090498] [G loss: 150.002548]\n",
      "[Epoch 21/500] [Batch 90/250] [D loss: -3.185078] [G loss: 170.035675]\n",
      "[Epoch 21/500] [Batch 95/250] [D loss: -3.483897] [G loss: 190.312027]\n",
      "[Epoch 21/500] [Batch 100/250] [D loss: -4.116412] [G loss: 210.981186]\n",
      "[Epoch 21/500] [Batch 105/250] [D loss: -4.007148] [G loss: 230.681091]\n",
      "[Epoch 21/500] [Batch 110/250] [D loss: -4.802481] [G loss: 251.082260]\n",
      "[Epoch 21/500] [Batch 115/250] [D loss: -4.416214] [G loss: 269.741730]\n",
      "[Epoch 21/500] [Batch 120/250] [D loss: -5.292295] [G loss: 289.200806]\n",
      "[Epoch 21/500] [Batch 125/250] [D loss: -4.835026] [G loss: 306.465546]\n",
      "[Epoch 21/500] [Batch 130/250] [D loss: -4.279260] [G loss: 323.000183]\n",
      "[Epoch 21/500] [Batch 135/250] [D loss: -4.383274] [G loss: 339.319183]\n",
      "[Epoch 21/500] [Batch 140/250] [D loss: -4.048293] [G loss: 354.096008]\n",
      "[Epoch 21/500] [Batch 145/250] [D loss: -3.359849] [G loss: 367.357849]\n",
      "[Epoch 21/500] [Batch 150/250] [D loss: -2.487398] [G loss: 379.047852]\n",
      "[Epoch 21/500] [Batch 155/250] [D loss: -1.974463] [G loss: 389.536285]\n",
      "[Epoch 21/500] [Batch 160/250] [D loss: -0.697762] [G loss: 397.391541]\n",
      "[Epoch 21/500] [Batch 165/250] [D loss: 0.581055] [G loss: 403.561523]\n",
      "[Epoch 21/500] [Batch 170/250] [D loss: 1.037550] [G loss: 408.216827]\n",
      "[Epoch 21/500] [Batch 175/250] [D loss: 1.864144] [G loss: 410.305298]\n",
      "[Epoch 21/500] [Batch 180/250] [D loss: 2.925411] [G loss: 410.091858]\n",
      "[Epoch 21/500] [Batch 185/250] [D loss: 4.677542] [G loss: 406.638000]\n",
      "[Epoch 21/500] [Batch 190/250] [D loss: 6.595052] [G loss: 400.532379]\n",
      "[Epoch 21/500] [Batch 195/250] [D loss: 7.077699] [G loss: 393.317383]\n",
      "[Epoch 21/500] [Batch 200/250] [D loss: 9.014606] [G loss: 382.249512]\n",
      "[Epoch 21/500] [Batch 205/250] [D loss: 11.090237] [G loss: 368.533112]\n",
      "[Epoch 21/500] [Batch 210/250] [D loss: 12.287973] [G loss: 353.069031]\n",
      "[Epoch 21/500] [Batch 215/250] [D loss: 14.507498] [G loss: 333.828705]\n",
      "[Epoch 21/500] [Batch 220/250] [D loss: 15.848324] [G loss: 313.292603]\n",
      "[Epoch 21/500] [Batch 225/250] [D loss: 16.850637] [G loss: 290.595306]\n",
      "[Epoch 21/500] [Batch 230/250] [D loss: 17.609219] [G loss: 266.056213]\n",
      "[Epoch 21/500] [Batch 235/250] [D loss: 18.980875] [G loss: 240.245209]\n",
      "[Epoch 21/500] [Batch 240/250] [D loss: 18.820568] [G loss: 214.414871]\n",
      "[Epoch 21/500] [Batch 245/250] [D loss: 18.432724] [G loss: 186.958511]\n",
      "[Epoch 22/500] [Batch 0/250] [D loss: 18.279499] [G loss: 158.066788]\n",
      "[Epoch 22/500] [Batch 5/250] [D loss: 17.837242] [G loss: 127.368919]\n",
      "[Epoch 22/500] [Batch 10/250] [D loss: 16.735626] [G loss: 95.081383]\n",
      "[Epoch 22/500] [Batch 15/250] [D loss: 15.502295] [G loss: 60.326557]\n",
      "[Epoch 22/500] [Batch 20/250] [D loss: 14.165555] [G loss: 23.651232]\n",
      "[Epoch 22/500] [Batch 25/250] [D loss: 12.494705] [G loss: -14.371801]\n",
      "[Epoch 22/500] [Batch 30/250] [D loss: 11.103629] [G loss: -54.248528]\n",
      "[Epoch 22/500] [Batch 35/250] [D loss: 9.138021] [G loss: -95.049995]\n",
      "[Epoch 22/500] [Batch 40/250] [D loss: 7.396463] [G loss: -137.037796]\n",
      "[Epoch 22/500] [Batch 45/250] [D loss: 5.389933] [G loss: -179.734482]\n",
      "[Epoch 22/500] [Batch 50/250] [D loss: 4.516708] [G loss: -224.014389]\n",
      "[Epoch 22/500] [Batch 55/250] [D loss: 3.259661] [G loss: -268.114105]\n",
      "[Epoch 22/500] [Batch 60/250] [D loss: 2.424203] [G loss: -311.888702]\n",
      "[Epoch 22/500] [Batch 65/250] [D loss: 1.582392] [G loss: -354.213501]\n",
      "[Epoch 22/500] [Batch 70/250] [D loss: 0.361192] [G loss: -394.434082]\n",
      "[Epoch 22/500] [Batch 75/250] [D loss: 1.147270] [G loss: -434.286774]\n",
      "[Epoch 22/500] [Batch 80/250] [D loss: 1.000307] [G loss: -470.760651]\n",
      "[Epoch 22/500] [Batch 85/250] [D loss: 1.363719] [G loss: -503.767456]\n",
      "[Epoch 22/500] [Batch 90/250] [D loss: 1.791090] [G loss: -532.896851]\n",
      "[Epoch 22/500] [Batch 95/250] [D loss: 3.304314] [G loss: -558.046753]\n",
      "[Epoch 22/500] [Batch 100/250] [D loss: 4.161953] [G loss: -577.744812]\n",
      "[Epoch 22/500] [Batch 105/250] [D loss: 5.069771] [G loss: -592.213684]\n",
      "[Epoch 22/500] [Batch 110/250] [D loss: 5.093699] [G loss: -600.452148]\n",
      "[Epoch 22/500] [Batch 115/250] [D loss: 6.975204] [G loss: -605.239624]\n",
      "[Epoch 22/500] [Batch 120/250] [D loss: 6.893539] [G loss: -602.726929]\n",
      "[Epoch 22/500] [Batch 125/250] [D loss: 7.974852] [G loss: -596.385986]\n",
      "[Epoch 22/500] [Batch 130/250] [D loss: 7.719162] [G loss: -583.563110]\n",
      "[Epoch 22/500] [Batch 135/250] [D loss: 7.463346] [G loss: -566.120667]\n",
      "[Epoch 22/500] [Batch 140/250] [D loss: 8.205725] [G loss: -544.841492]\n",
      "[Epoch 22/500] [Batch 145/250] [D loss: 6.516459] [G loss: -517.027588]\n",
      "[Epoch 22/500] [Batch 150/250] [D loss: 6.177111] [G loss: -486.718842]\n",
      "[Epoch 22/500] [Batch 155/250] [D loss: 5.575628] [G loss: -452.327057]\n",
      "[Epoch 22/500] [Batch 160/250] [D loss: 5.650348] [G loss: -415.334991]\n",
      "[Epoch 22/500] [Batch 165/250] [D loss: 4.722499] [G loss: -374.168182]\n",
      "[Epoch 22/500] [Batch 170/250] [D loss: 4.483082] [G loss: -331.034393]\n",
      "[Epoch 22/500] [Batch 175/250] [D loss: 4.339396] [G loss: -285.657990]\n",
      "[Epoch 22/500] [Batch 180/250] [D loss: 5.182851] [G loss: -238.920303]\n",
      "[Epoch 22/500] [Batch 185/250] [D loss: 4.729349] [G loss: -189.510498]\n",
      "[Epoch 22/500] [Batch 190/250] [D loss: 4.715108] [G loss: -139.124390]\n",
      "[Epoch 22/500] [Batch 195/250] [D loss: 5.710824] [G loss: -89.134926]\n",
      "[Epoch 22/500] [Batch 200/250] [D loss: 6.586030] [G loss: -38.245266]\n",
      "[Epoch 22/500] [Batch 205/250] [D loss: 7.669325] [G loss: 12.657898]\n",
      "[Epoch 22/500] [Batch 210/250] [D loss: 8.207373] [G loss: 63.347244]\n",
      "[Epoch 22/500] [Batch 215/250] [D loss: 9.633478] [G loss: 112.997299]\n",
      "[Epoch 22/500] [Batch 220/250] [D loss: 10.986760] [G loss: 161.587524]\n",
      "[Epoch 22/500] [Batch 225/250] [D loss: 12.482306] [G loss: 208.737595]\n",
      "[Epoch 22/500] [Batch 230/250] [D loss: 13.051769] [G loss: 253.723587]\n",
      "[Epoch 22/500] [Batch 235/250] [D loss: 14.068036] [G loss: 296.132599]\n",
      "[Epoch 22/500] [Batch 240/250] [D loss: 15.418077] [G loss: 336.296478]\n",
      "[Epoch 22/500] [Batch 245/250] [D loss: 17.621334] [G loss: 372.749084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/500] [Batch 0/250] [D loss: 18.696548] [G loss: 407.163544]\n",
      "[Epoch 23/500] [Batch 5/250] [D loss: 20.365963] [G loss: 437.375641]\n",
      "[Epoch 23/500] [Batch 10/250] [D loss: 22.217279] [G loss: 463.437225]\n",
      "[Epoch 23/500] [Batch 15/250] [D loss: 22.630314] [G loss: 486.535614]\n",
      "[Epoch 23/500] [Batch 20/250] [D loss: 23.449387] [G loss: 504.901672]\n",
      "[Epoch 23/500] [Batch 25/250] [D loss: 24.403479] [G loss: 518.580811]\n",
      "[Epoch 23/500] [Batch 30/250] [D loss: 25.785126] [G loss: 527.975220]\n",
      "[Epoch 23/500] [Batch 35/250] [D loss: 26.231459] [G loss: 532.945923]\n",
      "[Epoch 23/500] [Batch 40/250] [D loss: 25.404852] [G loss: 534.744019]\n",
      "[Epoch 23/500] [Batch 45/250] [D loss: 25.101208] [G loss: 531.361877]\n",
      "[Epoch 23/500] [Batch 50/250] [D loss: 24.254957] [G loss: 524.408020]\n",
      "[Epoch 23/500] [Batch 55/250] [D loss: 24.306072] [G loss: 512.293762]\n",
      "[Epoch 23/500] [Batch 60/250] [D loss: 22.241339] [G loss: 498.259338]\n",
      "[Epoch 23/500] [Batch 65/250] [D loss: 21.594561] [G loss: 479.189636]\n",
      "[Epoch 23/500] [Batch 70/250] [D loss: 20.132647] [G loss: 457.534821]\n",
      "[Epoch 23/500] [Batch 75/250] [D loss: 18.850801] [G loss: 432.540161]\n",
      "[Epoch 23/500] [Batch 80/250] [D loss: 17.191675] [G loss: 405.089844]\n",
      "[Epoch 23/500] [Batch 85/250] [D loss: 15.657236] [G loss: 374.988403]\n",
      "[Epoch 23/500] [Batch 90/250] [D loss: 14.067522] [G loss: 342.538696]\n",
      "[Epoch 23/500] [Batch 95/250] [D loss: 12.278614] [G loss: 308.389496]\n",
      "[Epoch 23/500] [Batch 100/250] [D loss: 10.845559] [G loss: 272.270905]\n",
      "[Epoch 23/500] [Batch 105/250] [D loss: 10.108835] [G loss: 234.198898]\n",
      "[Epoch 23/500] [Batch 110/250] [D loss: 9.024759] [G loss: 196.197845]\n",
      "[Epoch 23/500] [Batch 115/250] [D loss: 7.909087] [G loss: 157.156143]\n",
      "[Epoch 23/500] [Batch 120/250] [D loss: 6.811476] [G loss: 117.314018]\n",
      "[Epoch 23/500] [Batch 125/250] [D loss: 6.262871] [G loss: 76.748375]\n",
      "[Epoch 23/500] [Batch 130/250] [D loss: 5.209703] [G loss: 37.927223]\n",
      "[Epoch 23/500] [Batch 135/250] [D loss: 5.864772] [G loss: -1.476520]\n",
      "[Epoch 23/500] [Batch 140/250] [D loss: 4.537148] [G loss: -40.039185]\n",
      "[Epoch 23/500] [Batch 145/250] [D loss: 4.173054] [G loss: -77.821274]\n",
      "[Epoch 23/500] [Batch 150/250] [D loss: 3.966002] [G loss: -115.085594]\n",
      "[Epoch 23/500] [Batch 155/250] [D loss: 4.284187] [G loss: -151.832291]\n",
      "[Epoch 23/500] [Batch 160/250] [D loss: 4.167376] [G loss: -187.993164]\n",
      "[Epoch 23/500] [Batch 165/250] [D loss: 4.399055] [G loss: -223.253738]\n",
      "[Epoch 23/500] [Batch 170/250] [D loss: 4.282619] [G loss: -256.975769]\n",
      "[Epoch 23/500] [Batch 175/250] [D loss: 4.924463] [G loss: -289.702667]\n",
      "[Epoch 23/500] [Batch 180/250] [D loss: 4.949678] [G loss: -320.505249]\n",
      "[Epoch 23/500] [Batch 185/250] [D loss: 5.206269] [G loss: -349.332428]\n",
      "[Epoch 23/500] [Batch 190/250] [D loss: 5.470703] [G loss: -376.455658]\n",
      "[Epoch 23/500] [Batch 195/250] [D loss: 5.624626] [G loss: -401.203674]\n",
      "[Epoch 23/500] [Batch 200/250] [D loss: 5.631639] [G loss: -423.288940]\n",
      "[Epoch 23/500] [Batch 205/250] [D loss: 5.985293] [G loss: -443.308502]\n",
      "[Epoch 23/500] [Batch 210/250] [D loss: 5.791615] [G loss: -460.153229]\n",
      "[Epoch 23/500] [Batch 215/250] [D loss: 6.454101] [G loss: -475.214355]\n",
      "[Epoch 23/500] [Batch 220/250] [D loss: 6.020901] [G loss: -486.459808]\n",
      "[Epoch 23/500] [Batch 225/250] [D loss: 6.494600] [G loss: -495.925629]\n",
      "[Epoch 23/500] [Batch 230/250] [D loss: 6.396757] [G loss: -501.719635]\n",
      "[Epoch 23/500] [Batch 235/250] [D loss: 6.413822] [G loss: -505.260651]\n",
      "[Epoch 23/500] [Batch 240/250] [D loss: 5.658328] [G loss: -505.408752]\n",
      "[Epoch 23/500] [Batch 245/250] [D loss: 5.145804] [G loss: -502.888977]\n",
      "[Epoch 24/500] [Batch 0/250] [D loss: 5.108084] [G loss: -498.277924]\n",
      "[Epoch 24/500] [Batch 5/250] [D loss: 3.858506] [G loss: -489.744873]\n",
      "[Epoch 24/500] [Batch 10/250] [D loss: 3.731953] [G loss: -480.045959]\n",
      "[Epoch 24/500] [Batch 15/250] [D loss: 3.452423] [G loss: -468.403381]\n",
      "[Epoch 24/500] [Batch 20/250] [D loss: 2.404447] [G loss: -453.868011]\n",
      "[Epoch 24/500] [Batch 25/250] [D loss: 1.579904] [G loss: -437.457367]\n",
      "[Epoch 24/500] [Batch 30/250] [D loss: 1.369187] [G loss: -420.112518]\n",
      "[Epoch 24/500] [Batch 35/250] [D loss: 0.296590] [G loss: -400.174988]\n",
      "[Epoch 24/500] [Batch 40/250] [D loss: -0.166409] [G loss: -379.548676]\n",
      "[Epoch 24/500] [Batch 45/250] [D loss: -0.382416] [G loss: -358.034821]\n",
      "[Epoch 24/500] [Batch 50/250] [D loss: -0.460942] [G loss: -335.730621]\n",
      "[Epoch 24/500] [Batch 55/250] [D loss: -0.487350] [G loss: -312.945404]\n",
      "[Epoch 24/500] [Batch 60/250] [D loss: -0.397523] [G loss: -289.954651]\n",
      "[Epoch 24/500] [Batch 65/250] [D loss: -0.311235] [G loss: -267.074768]\n",
      "[Epoch 24/500] [Batch 70/250] [D loss: -0.232644] [G loss: -244.658859]\n",
      "[Epoch 24/500] [Batch 75/250] [D loss: 0.238374] [G loss: -224.105896]\n",
      "[Epoch 24/500] [Batch 80/250] [D loss: 0.704456] [G loss: -204.898407]\n",
      "[Epoch 24/500] [Batch 85/250] [D loss: 0.924067] [G loss: -186.802872]\n",
      "[Epoch 24/500] [Batch 90/250] [D loss: 1.391949] [G loss: -170.619064]\n",
      "[Epoch 24/500] [Batch 95/250] [D loss: 1.814112] [G loss: -156.315872]\n",
      "[Epoch 24/500] [Batch 100/250] [D loss: 2.170220] [G loss: -144.152588]\n",
      "[Epoch 24/500] [Batch 105/250] [D loss: 2.233742] [G loss: -134.056137]\n",
      "[Epoch 24/500] [Batch 110/250] [D loss: 2.418017] [G loss: -126.547485]\n",
      "[Epoch 24/500] [Batch 115/250] [D loss: 2.398175] [G loss: -121.238647]\n",
      "[Epoch 24/500] [Batch 120/250] [D loss: 2.030290] [G loss: -118.011757]\n",
      "[Epoch 24/500] [Batch 125/250] [D loss: 2.054702] [G loss: -117.383682]\n",
      "[Epoch 24/500] [Batch 130/250] [D loss: 1.763225] [G loss: -118.598900]\n",
      "[Epoch 24/500] [Batch 135/250] [D loss: 1.428465] [G loss: -121.758690]\n",
      "[Epoch 24/500] [Batch 140/250] [D loss: 0.678889] [G loss: -126.289490]\n",
      "[Epoch 24/500] [Batch 145/250] [D loss: 0.480933] [G loss: -132.847519]\n",
      "[Epoch 24/500] [Batch 150/250] [D loss: -0.252642] [G loss: -140.222183]\n",
      "[Epoch 24/500] [Batch 155/250] [D loss: -0.244501] [G loss: -149.488510]\n",
      "[Epoch 24/500] [Batch 160/250] [D loss: -0.710485] [G loss: -159.252533]\n",
      "[Epoch 24/500] [Batch 165/250] [D loss: -0.748498] [G loss: -170.235382]\n",
      "[Epoch 24/500] [Batch 170/250] [D loss: -0.758564] [G loss: -181.869202]\n",
      "[Epoch 24/500] [Batch 175/250] [D loss: -0.787181] [G loss: -193.840683]\n",
      "[Epoch 24/500] [Batch 180/250] [D loss: -0.327451] [G loss: -206.513214]\n",
      "[Epoch 24/500] [Batch 185/250] [D loss: -0.023683] [G loss: -219.129044]\n",
      "[Epoch 24/500] [Batch 190/250] [D loss: -0.064049] [G loss: -231.140854]\n",
      "[Epoch 24/500] [Batch 195/250] [D loss: 1.008952] [G loss: -243.853882]\n",
      "[Epoch 24/500] [Batch 200/250] [D loss: 1.640612] [G loss: -255.471786]\n",
      "[Epoch 24/500] [Batch 205/250] [D loss: 2.772357] [G loss: -266.699768]\n",
      "[Epoch 24/500] [Batch 210/250] [D loss: 3.259680] [G loss: -276.279633]\n",
      "[Epoch 24/500] [Batch 215/250] [D loss: 4.921302] [G loss: -285.518616]\n",
      "[Epoch 24/500] [Batch 220/250] [D loss: 6.048333] [G loss: -292.884918]\n",
      "[Epoch 24/500] [Batch 225/250] [D loss: 6.324792] [G loss: -298.009460]\n",
      "[Epoch 24/500] [Batch 230/250] [D loss: 7.335344] [G loss: -301.819855]\n",
      "[Epoch 24/500] [Batch 235/250] [D loss: 8.791191] [G loss: -304.528412]\n",
      "[Epoch 24/500] [Batch 240/250] [D loss: 9.652063] [G loss: -304.460205]\n",
      "[Epoch 24/500] [Batch 245/250] [D loss: 10.618262] [G loss: -302.491974]\n",
      "[Epoch 25/500] [Batch 0/250] [D loss: 11.346261] [G loss: -298.346802]\n",
      "[Epoch 25/500] [Batch 5/250] [D loss: 11.798470] [G loss: -291.775299]\n",
      "[Epoch 25/500] [Batch 10/250] [D loss: 12.338120] [G loss: -283.547150]\n",
      "[Epoch 25/500] [Batch 15/250] [D loss: 12.476168] [G loss: -272.928070]\n",
      "[Epoch 25/500] [Batch 20/250] [D loss: 13.244576] [G loss: -261.084778]\n",
      "[Epoch 25/500] [Batch 25/250] [D loss: 12.983019] [G loss: -246.662628]\n",
      "[Epoch 25/500] [Batch 30/250] [D loss: 13.941120] [G loss: -231.950714]\n",
      "[Epoch 25/500] [Batch 35/250] [D loss: 13.224968] [G loss: -214.570267]\n",
      "[Epoch 25/500] [Batch 40/250] [D loss: 13.763561] [G loss: -196.476730]\n",
      "[Epoch 25/500] [Batch 45/250] [D loss: 13.425894] [G loss: -177.277710]\n",
      "[Epoch 25/500] [Batch 50/250] [D loss: 13.166363] [G loss: -157.423889]\n",
      "[Epoch 25/500] [Batch 55/250] [D loss: 12.967485] [G loss: -136.880508]\n",
      "[Epoch 25/500] [Batch 60/250] [D loss: 12.651960] [G loss: -115.901947]\n",
      "[Epoch 25/500] [Batch 65/250] [D loss: 12.345041] [G loss: -94.636192]\n",
      "[Epoch 25/500] [Batch 70/250] [D loss: 12.027065] [G loss: -73.215729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/500] [Batch 75/250] [D loss: 11.643518] [G loss: -51.917294]\n",
      "[Epoch 25/500] [Batch 80/250] [D loss: 10.512949] [G loss: -30.802246]\n",
      "[Epoch 25/500] [Batch 85/250] [D loss: 8.284092] [G loss: -10.403492]\n",
      "[Epoch 25/500] [Batch 90/250] [D loss: 8.175785] [G loss: 9.316833]\n",
      "[Epoch 25/500] [Batch 95/250] [D loss: 7.739910] [G loss: 28.345058]\n",
      "[Epoch 25/500] [Batch 100/250] [D loss: 7.325931] [G loss: 46.407200]\n",
      "[Epoch 25/500] [Batch 105/250] [D loss: 6.793611] [G loss: 63.648388]\n",
      "[Epoch 25/500] [Batch 110/250] [D loss: 6.378135] [G loss: 79.563286]\n",
      "[Epoch 25/500] [Batch 115/250] [D loss: 5.953534] [G loss: 94.229858]\n",
      "[Epoch 25/500] [Batch 120/250] [D loss: 5.420074] [G loss: 107.510834]\n",
      "[Epoch 25/500] [Batch 125/250] [D loss: 5.112176] [G loss: 119.528992]\n",
      "[Epoch 25/500] [Batch 130/250] [D loss: 4.454015] [G loss: 130.299438]\n",
      "[Epoch 25/500] [Batch 135/250] [D loss: 4.198717] [G loss: 139.562256]\n",
      "[Epoch 25/500] [Batch 140/250] [D loss: 3.838957] [G loss: 147.487427]\n",
      "[Epoch 25/500] [Batch 145/250] [D loss: 3.576697] [G loss: 153.995361]\n",
      "[Epoch 25/500] [Batch 150/250] [D loss: 2.799586] [G loss: 159.781174]\n",
      "[Epoch 25/500] [Batch 155/250] [D loss: 2.566971] [G loss: 163.814651]\n",
      "[Epoch 25/500] [Batch 160/250] [D loss: 2.388875] [G loss: 166.893082]\n",
      "[Epoch 25/500] [Batch 165/250] [D loss: 2.138056] [G loss: 168.829300]\n",
      "[Epoch 25/500] [Batch 170/250] [D loss: 1.650381] [G loss: 170.184036]\n",
      "[Epoch 25/500] [Batch 175/250] [D loss: 1.360420] [G loss: 170.355621]\n",
      "[Epoch 25/500] [Batch 180/250] [D loss: 1.155350] [G loss: 169.565353]\n",
      "[Epoch 25/500] [Batch 185/250] [D loss: 0.992982] [G loss: 168.095718]\n",
      "[Epoch 25/500] [Batch 190/250] [D loss: 0.382666] [G loss: 166.438751]\n",
      "[Epoch 25/500] [Batch 195/250] [D loss: 0.558428] [G loss: 163.654282]\n",
      "[Epoch 25/500] [Batch 200/250] [D loss: 0.225789] [G loss: 160.814941]\n",
      "[Epoch 25/500] [Batch 205/250] [D loss: 0.026683] [G loss: 157.560013]\n",
      "[Epoch 25/500] [Batch 210/250] [D loss: -0.349539] [G loss: 154.072021]\n",
      "[Epoch 25/500] [Batch 215/250] [D loss: -0.365545] [G loss: 150.001511]\n",
      "[Epoch 25/500] [Batch 220/250] [D loss: -0.843789] [G loss: 146.035568]\n",
      "[Epoch 25/500] [Batch 225/250] [D loss: -0.876883] [G loss: 141.355804]\n",
      "[Epoch 25/500] [Batch 230/250] [D loss: -1.436516] [G loss: 136.700363]\n",
      "[Epoch 25/500] [Batch 235/250] [D loss: -1.783778] [G loss: 131.385788]\n",
      "[Epoch 25/500] [Batch 240/250] [D loss: -1.885628] [G loss: 125.241882]\n",
      "[Epoch 25/500] [Batch 245/250] [D loss: -2.491707] [G loss: 118.946289]\n",
      "[Epoch 26/500] [Batch 0/250] [D loss: -2.743587] [G loss: 111.606987]\n",
      "[Epoch 26/500] [Batch 5/250] [D loss: -3.106472] [G loss: 103.691422]\n",
      "[Epoch 26/500] [Batch 10/250] [D loss: -3.262589] [G loss: 94.704903]\n",
      "[Epoch 26/500] [Batch 15/250] [D loss: -3.440216] [G loss: 84.741798]\n",
      "[Epoch 26/500] [Batch 20/250] [D loss: -3.826889] [G loss: 73.956924]\n",
      "[Epoch 26/500] [Batch 25/250] [D loss: -3.900352] [G loss: 61.916279]\n",
      "[Epoch 26/500] [Batch 30/250] [D loss: -3.811233] [G loss: 48.920883]\n",
      "[Epoch 26/500] [Batch 35/250] [D loss: -3.752914] [G loss: 34.992439]\n",
      "[Epoch 26/500] [Batch 40/250] [D loss: -3.430456] [G loss: 20.014004]\n",
      "[Epoch 26/500] [Batch 45/250] [D loss: -3.153283] [G loss: 4.338006]\n",
      "[Epoch 26/500] [Batch 50/250] [D loss: -2.588152] [G loss: -11.984244]\n",
      "[Epoch 26/500] [Batch 55/250] [D loss: -2.161901] [G loss: -28.637487]\n",
      "[Epoch 26/500] [Batch 60/250] [D loss: -1.670372] [G loss: -45.344540]\n",
      "[Epoch 26/500] [Batch 65/250] [D loss: -0.887290] [G loss: -61.858543]\n",
      "[Epoch 26/500] [Batch 70/250] [D loss: 0.105581] [G loss: -77.696617]\n",
      "[Epoch 26/500] [Batch 75/250] [D loss: 0.800708] [G loss: -92.421608]\n",
      "[Epoch 26/500] [Batch 80/250] [D loss: 2.045324] [G loss: -105.843330]\n",
      "[Epoch 26/500] [Batch 85/250] [D loss: 2.570395] [G loss: -117.399101]\n",
      "[Epoch 26/500] [Batch 90/250] [D loss: 3.773064] [G loss: -127.415916]\n",
      "[Epoch 26/500] [Batch 95/250] [D loss: 4.658463] [G loss: -134.960617]\n",
      "[Epoch 26/500] [Batch 100/250] [D loss: 5.503676] [G loss: -140.280197]\n",
      "[Epoch 26/500] [Batch 105/250] [D loss: 6.201502] [G loss: -143.345901]\n",
      "[Epoch 26/500] [Batch 110/250] [D loss: 6.819798] [G loss: -143.772949]\n",
      "[Epoch 26/500] [Batch 115/250] [D loss: 7.007471] [G loss: -141.133392]\n",
      "[Epoch 26/500] [Batch 120/250] [D loss: 7.127219] [G loss: -136.365463]\n",
      "[Epoch 26/500] [Batch 125/250] [D loss: 7.142756] [G loss: -128.823593]\n",
      "[Epoch 26/500] [Batch 130/250] [D loss: 7.358445] [G loss: -119.118126]\n",
      "[Epoch 26/500] [Batch 135/250] [D loss: 7.160772] [G loss: -107.335587]\n",
      "[Epoch 26/500] [Batch 140/250] [D loss: 7.014141] [G loss: -93.495766]\n",
      "[Epoch 26/500] [Batch 145/250] [D loss: 6.460624] [G loss: -77.844421]\n",
      "[Epoch 26/500] [Batch 150/250] [D loss: 6.114284] [G loss: -60.741436]\n",
      "[Epoch 26/500] [Batch 155/250] [D loss: 5.733938] [G loss: -42.520000]\n",
      "[Epoch 26/500] [Batch 160/250] [D loss: 5.131067] [G loss: -23.488674]\n",
      "[Epoch 26/500] [Batch 165/250] [D loss: 4.434400] [G loss: -3.809164]\n",
      "[Epoch 26/500] [Batch 170/250] [D loss: 3.977052] [G loss: 16.263294]\n",
      "[Epoch 26/500] [Batch 175/250] [D loss: 3.598129] [G loss: 36.114109]\n",
      "[Epoch 26/500] [Batch 180/250] [D loss: 3.090421] [G loss: 55.571751]\n",
      "[Epoch 26/500] [Batch 185/250] [D loss: 2.653199] [G loss: 74.484444]\n",
      "[Epoch 26/500] [Batch 190/250] [D loss: 2.148070] [G loss: 92.601006]\n",
      "[Epoch 26/500] [Batch 195/250] [D loss: 2.303868] [G loss: 109.320114]\n",
      "[Epoch 26/500] [Batch 200/250] [D loss: 1.540256] [G loss: 124.995621]\n",
      "[Epoch 26/500] [Batch 205/250] [D loss: 1.826923] [G loss: 138.297714]\n",
      "[Epoch 26/500] [Batch 210/250] [D loss: 1.214296] [G loss: 150.567581]\n",
      "[Epoch 26/500] [Batch 215/250] [D loss: 0.922286] [G loss: 160.798431]\n",
      "[Epoch 26/500] [Batch 220/250] [D loss: 0.576613] [G loss: 169.070114]\n",
      "[Epoch 26/500] [Batch 225/250] [D loss: 0.418225] [G loss: 175.467209]\n",
      "[Epoch 26/500] [Batch 230/250] [D loss: 0.614903] [G loss: 179.293427]\n"
     ]
    }
   ],
   "source": [
    "# old logger\n",
    "#from utils import Logger\n",
    "#logger = Logger(model_name='wGANGP', data_name='mol224')\n",
    "#num_batches = len(dataloader)\n",
    "n_epochs=500\n",
    "batches_done = 0\n",
    "### new logger\n",
    "#configure(\"runs/test1\",flush_secs=5)\n",
    "\n",
    "#batch_size=100\n",
    "#print(latent_dim)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "        # Generate a batch of images\n",
    "        fake_imgs = generator(z)\n",
    "        # Real images\n",
    "        real_validity = discriminator(real_imgs)\n",
    "        # Fake images\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "        # Adversarial loss\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        optimizer_G.zero_grad()\n",
    "        # Train the generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator(z)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            # Train on fake images\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "            #logger.log(d_loss, g_loss, epoch, batches_done, num_batches)\n",
    "            if batches_done % sample_interval/10 ==0:\n",
    "                log_value('g_loss', g_loss, batches_done)\n",
    "                log_value('d_loss', d_loss, batches_done)\n",
    "            \n",
    "            if batches_done % sample_interval == 0:\n",
    "                save_image(fake_imgs.data[:25], \"molpics500px/%d_b.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "            batches_done += n_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.10.0 at http://minilearn:6006 (Press CTRL+C to quit)\n",
      "\u001b[33mW0913 15:35:36.738302 Thread-1 application.py:276] path /[[_dataImageSrc]] not found, sending 404\n",
      "\u001b[0mW0913 15:35:36.738301 139841509275392 application.py:276] path /[[_dataImageSrc]] not found, sending 404\n",
      "\u001b[33mW0913 15:35:36.786560 Thread-1 application.py:276] path /[[_imageURL]] not found, sending 404\n",
      "\u001b[0mW0913 15:35:36.786560 139841509275392 application.py:276] path /[[_imageURL]] not found, sending 404\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#tensorboard\n",
    "!python -m tensorboard.main --logdir runs\n",
    "#tensorboard --help\n",
    "#tensorboard --logdir=runs\n",
    "\n",
    "#logger.d_loss.data.cpu().numpy()\n",
    "#logger.display_status(epoch, n_epochs, batches_done, num_batches, d_loss, g_loss, d_pred_real, d_pred_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/home/jgmeyer2/vangan/gans/models\",exist_ok=True)\n",
    "PATH = \"/home/jgmeyer2/vangan/gans/models/g5k.model\"\n",
    "modelid=\"5k\"\n",
    "\n",
    "\n",
    "state_g = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': generator.state_dict(),\n",
    "    'optimizer': optimizer_G.state_dict()\n",
    "    }\n",
    "torch.save(state_g, PATH+\"g\"+modelid+\".model\")\n",
    "\n",
    "state_d = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': discriminator.state_dict(),\n",
    "    'optimizer': optimizer_D.state_dict()\n",
    "    }\n",
    "torch.save(state_d, PATH+\"d\"+modelid+\".model\")\n",
    "print(\"saved models @\")\n",
    "print(epoch)\n",
    "\n",
    "\n",
    "#def save_model(net, optim, ckpt_fname):\n",
    "#    state_dict = net.module.state_dict()\n",
    "#    for key in state_dict.keys():\n",
    "#        state_dict[key] = state_dict[key].cpu()\n",
    "#        torch.save({\n",
    "#            'epoch': epoch,                                                                                                                                                                                     \n",
    "#            'state_dict': state_dict,                                                                                                                                                                                \n",
    "#            'optimizer': optim},                                                                                                                                                                                     \n",
    "#            ckpt_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(state['state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "model.load_state_dict(state['state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "state = torch.load(filepath)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vangan",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
