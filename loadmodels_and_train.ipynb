{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from tensorboard_logger import configure, log_value\n",
    "import tensorboard\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "#import utils import Logger\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transforms_=None):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.files = sorted(glob.glob('%s/*.*' % folder_path))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    \n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(cuda)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(100, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "    \n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 250, 250)\n"
     ]
    }
   ],
   "source": [
    "n_epochs=100 #number of epochs of training\n",
    "batch_size=5 #size of the batches\n",
    "lr=0.0002 #adam: learning rate\n",
    "b1=0.5  #\"adam: decay of first order momentum of gradient\")\n",
    "b2=0.999 #adam: decay of first order momentum of gradient\")\n",
    "n_cpu=8\n",
    "latent_dim=100\n",
    "img_size=250\n",
    "channels=3\n",
    "n_critic=5\n",
    "clip_value=0.01\n",
    "sample_interval=1000\n",
    "img_shape = (channels, img_size, img_size)\n",
    "crop_size = 400\n",
    "print(img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "folder_path = \"./zinc100k500pxRe250pxJPG/\"\n",
    "transforms_ = [ transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "dataloader = DataLoader(ImageDataset(folder_path, transforms_=transforms_),\n",
    "                        batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Models and Load Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10\n",
    "\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# If cuda, use cuda\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    \n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b2, b2))\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_g = torch.load(\"/home/jgmeyer2/vangan/gans/models/g250px_jpegs.model\")\n",
    "state_d = torch.load(\"/home/jgmeyer2/vangan/gans/models/d250px_jpegs.model\")\n",
    "\n",
    "\n",
    "generator.load_state_dict(state_g['state_dict'])\n",
    "optimizer_G.load_state_dict(state_g['optimizer'])\n",
    "\n",
    "discriminator.load_state_dict(state_d['state_dict'])\n",
    "optimizer_D.load_state_dict(state_d['optimizer'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INitialize logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure(\"runs/test5\",flush_secs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[Epoch 0/1] [Batch 0/2000] [D loss: 8.087671] [G loss: 0.065366] [5batch time: 0.931179]\n",
      "[Epoch 0/1] [Batch 5/2000] [D loss: -567.965088] [G loss: -0.170554] [5batch time: 0.898888]\n",
      "[Epoch 0/1] [Batch 10/2000] [D loss: -1505.326172] [G loss: -1.529939] [5batch time: 0.907479]\n",
      "[Epoch 0/1] [Batch 15/2000] [D loss: -2695.595703] [G loss: -5.341569] [5batch time: 0.882582]\n",
      "[Epoch 0/1] [Batch 20/2000] [D loss: -3933.389893] [G loss: -13.375514] [5batch time: 0.905233]\n",
      "[Epoch 0/1] [Batch 25/2000] [D loss: -4838.215820] [G loss: -31.952581] [5batch time: 0.907908]\n",
      "[Epoch 0/1] [Batch 30/2000] [D loss: -4806.338867] [G loss: -53.683540] [5batch time: 0.897358]\n",
      "[Epoch 0/1] [Batch 35/2000] [D loss: -3855.025391] [G loss: -93.181923] [5batch time: 0.898592]\n",
      "[Epoch 0/1] [Batch 40/2000] [D loss: -3222.952148] [G loss: -126.084206] [5batch time: 0.910570]\n",
      "[Epoch 0/1] [Batch 45/2000] [D loss: -3320.944336] [G loss: -140.084290] [5batch time: 0.908895]\n",
      "[Epoch 0/1] [Batch 50/2000] [D loss: -3752.233398] [G loss: -167.550171] [5batch time: 0.890086]\n",
      "[Epoch 0/1] [Batch 55/2000] [D loss: -4287.375000] [G loss: -181.621048] [5batch time: 0.902865]\n",
      "[Epoch 0/1] [Batch 60/2000] [D loss: -4666.519531] [G loss: -182.654510] [5batch time: 0.908464]\n",
      "[Epoch 0/1] [Batch 65/2000] [D loss: -4816.155273] [G loss: -187.270859] [5batch time: 0.905732]\n",
      "[Epoch 0/1] [Batch 70/2000] [D loss: -4765.415039] [G loss: -176.874756] [5batch time: 0.906876]\n",
      "[Epoch 0/1] [Batch 75/2000] [D loss: -4617.509766] [G loss: -176.368149] [5batch time: 0.928914]\n",
      "[Epoch 0/1] [Batch 80/2000] [D loss: -4388.181641] [G loss: -168.703476] [5batch time: 0.901798]\n",
      "[Epoch 0/1] [Batch 85/2000] [D loss: -4162.368164] [G loss: -171.354095] [5batch time: 0.913978]\n",
      "[Epoch 0/1] [Batch 90/2000] [D loss: -3949.555176] [G loss: -180.134171] [5batch time: 0.898050]\n",
      "[Epoch 0/1] [Batch 95/2000] [D loss: -3819.467285] [G loss: -170.768539] [5batch time: 0.909470]\n",
      "[Epoch 0/1] [Batch 100/2000] [D loss: -3728.830078] [G loss: -182.856979] [5batch time: 0.915868]\n",
      "[Epoch 0/1] [Batch 105/2000] [D loss: -3682.945068] [G loss: -193.197998] [5batch time: 0.915810]\n",
      "[Epoch 0/1] [Batch 110/2000] [D loss: -3674.039062] [G loss: -207.882843] [5batch time: 0.900365]\n",
      "[Epoch 0/1] [Batch 115/2000] [D loss: -3714.028320] [G loss: -218.877121] [5batch time: 0.916026]\n",
      "[Epoch 0/1] [Batch 120/2000] [D loss: -3790.500000] [G loss: -243.804245] [5batch time: 0.904774]\n",
      "[Epoch 0/1] [Batch 125/2000] [D loss: -3900.333496] [G loss: -280.681213] [5batch time: 0.899441]\n",
      "[Epoch 0/1] [Batch 130/2000] [D loss: -4003.872803] [G loss: -329.275452] [5batch time: 0.884509]\n",
      "[Epoch 0/1] [Batch 135/2000] [D loss: -4124.529297] [G loss: -377.894989] [5batch time: 0.910881]\n",
      "[Epoch 0/1] [Batch 140/2000] [D loss: -4222.996582] [G loss: -432.046143] [5batch time: 0.899121]\n",
      "[Epoch 0/1] [Batch 145/2000] [D loss: -4299.395508] [G loss: -510.984833] [5batch time: 0.899594]\n",
      "[Epoch 0/1] [Batch 150/2000] [D loss: -4336.913574] [G loss: -591.983093] [5batch time: 0.921681]\n",
      "[Epoch 0/1] [Batch 155/2000] [D loss: -4302.324219] [G loss: -687.929688] [5batch time: 0.900257]\n",
      "[Epoch 0/1] [Batch 160/2000] [D loss: -4211.139160] [G loss: -801.254395] [5batch time: 0.908595]\n",
      "[Epoch 0/1] [Batch 165/2000] [D loss: -4021.302734] [G loss: -961.276306] [5batch time: 0.901757]\n",
      "[Epoch 0/1] [Batch 170/2000] [D loss: -3834.159180] [G loss: -1097.123413] [5batch time: 0.888119]\n",
      "[Epoch 0/1] [Batch 175/2000] [D loss: -3554.525879] [G loss: -1283.319214] [5batch time: 0.906224]\n",
      "[Epoch 0/1] [Batch 180/2000] [D loss: -3399.774902] [G loss: -1355.931030] [5batch time: 0.894480]\n",
      "[Epoch 0/1] [Batch 185/2000] [D loss: -3094.637695] [G loss: -1595.862427] [5batch time: 0.901523]\n",
      "[Epoch 0/1] [Batch 190/2000] [D loss: -2922.548340] [G loss: -1761.670898] [5batch time: 0.891373]\n",
      "[Epoch 0/1] [Batch 195/2000] [D loss: -2956.208496] [G loss: -1737.166870] [5batch time: 0.895524]\n",
      "[Epoch 0/1] [Batch 200/2000] [D loss: -2819.947754] [G loss: -1936.430298] [5batch time: 0.908670]\n",
      "[Epoch 0/1] [Batch 205/2000] [D loss: -2616.482422] [G loss: -2206.944580] [5batch time: 0.892370]\n",
      "[Epoch 0/1] [Batch 210/2000] [D loss: -2814.814941] [G loss: -2035.151245] [5batch time: 0.893826]\n",
      "[Epoch 0/1] [Batch 215/2000] [D loss: -2753.379883] [G loss: -2188.678223] [5batch time: 0.902070]\n",
      "[Epoch 0/1] [Batch 220/2000] [D loss: -2966.380859] [G loss: -2027.093018] [5batch time: 0.894176]\n",
      "[Epoch 0/1] [Batch 225/2000] [D loss: -2735.254395] [G loss: -2253.105469] [5batch time: 0.907329]\n",
      "[Epoch 0/1] [Batch 230/2000] [D loss: -2835.577881] [G loss: -2135.913330] [5batch time: 0.883066]\n",
      "[Epoch 0/1] [Batch 235/2000] [D loss: -2981.329590] [G loss: -1908.570557] [5batch time: 0.891228]\n",
      "[Epoch 0/1] [Batch 240/2000] [D loss: -2750.702393] [G loss: -2050.338135] [5batch time: 0.906420]\n",
      "[Epoch 0/1] [Batch 245/2000] [D loss: -2660.954102] [G loss: -2022.438843] [5batch time: 0.907823]\n",
      "[Epoch 0/1] [Batch 250/2000] [D loss: -2681.236572] [G loss: -1858.239502] [5batch time: 0.895156]\n",
      "[Epoch 0/1] [Batch 255/2000] [D loss: -2630.995605] [G loss: -1742.454102] [5batch time: 0.921566]\n",
      "[Epoch 0/1] [Batch 260/2000] [D loss: -2530.828125] [G loss: -1680.156616] [5batch time: 0.892326]\n",
      "[Epoch 0/1] [Batch 265/2000] [D loss: -2309.046387] [G loss: -1712.551392] [5batch time: 0.880534]\n",
      "[Epoch 0/1] [Batch 270/2000] [D loss: -2106.662354] [G loss: -1741.259155] [5batch time: 0.892790]\n",
      "[Epoch 0/1] [Batch 275/2000] [D loss: -2021.935059] [G loss: -1671.117432] [5batch time: 0.918525]\n",
      "[Epoch 0/1] [Batch 280/2000] [D loss: -2030.842651] [G loss: -1495.804077] [5batch time: 0.903704]\n",
      "[Epoch 0/1] [Batch 285/2000] [D loss: -1797.390869] [G loss: -1585.812012] [5batch time: 0.914979]\n",
      "[Epoch 0/1] [Batch 290/2000] [D loss: -1775.936035] [G loss: -1472.338989] [5batch time: 0.910902]\n",
      "[Epoch 0/1] [Batch 295/2000] [D loss: -1693.279053] [G loss: -1448.322144] [5batch time: 0.912865]\n",
      "[Epoch 0/1] [Batch 300/2000] [D loss: -1674.091919] [G loss: -1370.861084] [5batch time: 0.915954]\n",
      "[Epoch 0/1] [Batch 305/2000] [D loss: -1667.592285] [G loss: -1287.536011] [5batch time: 0.906090]\n",
      "[Epoch 0/1] [Batch 310/2000] [D loss: -1339.797852] [G loss: -1559.936890] [5batch time: 0.896697]\n",
      "[Epoch 0/1] [Batch 315/2000] [D loss: -1649.105347] [G loss: -1199.607422] [5batch time: 0.899353]\n",
      "[Epoch 0/1] [Batch 320/2000] [D loss: -1198.416504] [G loss: -1621.770752] [5batch time: 0.907250]\n",
      "[Epoch 0/1] [Batch 325/2000] [D loss: -1402.381104] [G loss: -1388.761597] [5batch time: 0.910884]\n",
      "[Epoch 0/1] [Batch 330/2000] [D loss: -1126.820435] [G loss: -1659.738281] [5batch time: 0.903966]\n",
      "[Epoch 0/1] [Batch 335/2000] [D loss: -1568.397095] [G loss: -1214.507690] [5batch time: 0.892401]\n",
      "[Epoch 0/1] [Batch 340/2000] [D loss: -1048.105957] [G loss: -1752.890015] [5batch time: 0.898587]\n",
      "[Epoch 0/1] [Batch 345/2000] [D loss: -858.407593] [G loss: -1961.677002] [5batch time: 0.891009]\n",
      "[Epoch 0/1] [Batch 350/2000] [D loss: -1420.031982] [G loss: -1423.827393] [5batch time: 0.895917]\n",
      "[Epoch 0/1] [Batch 355/2000] [D loss: -1313.449951] [G loss: -1569.538452] [5batch time: 0.905269]\n",
      "[Epoch 0/1] [Batch 360/2000] [D loss: -1028.340454] [G loss: -1897.748047] [5batch time: 0.888248]\n",
      "[Epoch 0/1] [Batch 365/2000] [D loss: -789.686523] [G loss: -2179.471680] [5batch time: 0.890491]\n",
      "[Epoch 0/1] [Batch 370/2000] [D loss: -1013.461914] [G loss: -1997.470337] [5batch time: 0.897978]\n",
      "[Epoch 0/1] [Batch 375/2000] [D loss: -778.174805] [G loss: -2299.291748] [5batch time: 0.905015]\n",
      "[Epoch 0/1] [Batch 380/2000] [D loss: -1077.101318] [G loss: -2056.878906] [5batch time: 0.897050]\n",
      "[Epoch 0/1] [Batch 385/2000] [D loss: -794.867920] [G loss: -2395.300537] [5batch time: 0.893153]\n",
      "[Epoch 0/1] [Batch 390/2000] [D loss: -725.256958] [G loss: -2523.104736] [5batch time: 0.899393]\n",
      "[Epoch 0/1] [Batch 395/2000] [D loss: -936.320190] [G loss: -2377.859131] [5batch time: 0.897671]\n",
      "[Epoch 0/1] [Batch 400/2000] [D loss: -448.513428] [G loss: -2915.781250] [5batch time: 0.905419]\n",
      "[Epoch 0/1] [Batch 405/2000] [D loss: -812.058105] [G loss: -2606.101807] [5batch time: 0.904226]\n",
      "[Epoch 0/1] [Batch 410/2000] [D loss: -734.133606] [G loss: -2728.624268] [5batch time: 0.888238]\n",
      "[Epoch 0/1] [Batch 415/2000] [D loss: -289.175537] [G loss: -3220.574951] [5batch time: 0.893399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 420/2000] [D loss: -347.932861] [G loss: -3203.039795] [5batch time: 0.896833]\n",
      "[Epoch 0/1] [Batch 425/2000] [D loss: -612.530945] [G loss: -2956.669189] [5batch time: 0.896049]\n",
      "[Epoch 0/1] [Batch 430/2000] [D loss: -102.558044] [G loss: -3495.974365] [5batch time: 0.892218]\n",
      "[Epoch 0/1] [Batch 435/2000] [D loss: -461.666077] [G loss: -3138.668945] [5batch time: 0.874080]\n",
      "[Epoch 0/1] [Batch 440/2000] [D loss: -312.091492] [G loss: -3286.340332] [5batch time: 0.898537]\n",
      "[Epoch 0/1] [Batch 445/2000] [D loss: -599.583862] [G loss: -2986.123047] [5batch time: 0.901865]\n",
      "[Epoch 0/1] [Batch 450/2000] [D loss: 425.260193] [G loss: -4002.648438] [5batch time: 0.907014]\n",
      "[Epoch 0/1] [Batch 455/2000] [D loss: 432.650757] [G loss: -3973.274658] [5batch time: 0.909390]\n",
      "[Epoch 0/1] [Batch 460/2000] [D loss: -605.986938] [G loss: -2915.540039] [5batch time: 0.909810]\n",
      "[Epoch 0/1] [Batch 465/2000] [D loss: -368.296997] [G loss: -3119.831543] [5batch time: 0.892771]\n",
      "[Epoch 0/1] [Batch 470/2000] [D loss: -347.881348] [G loss: -3096.938477] [5batch time: 0.899680]\n",
      "[Epoch 0/1] [Batch 475/2000] [D loss: 99.303772] [G loss: -3493.703125] [5batch time: 0.915905]\n",
      "[Epoch 0/1] [Batch 480/2000] [D loss: 424.329712] [G loss: -3756.931396] [5batch time: 0.914282]\n",
      "[Epoch 0/1] [Batch 485/2000] [D loss: 210.333496] [G loss: -3487.638428] [5batch time: 0.917708]\n",
      "[Epoch 0/1] [Batch 490/2000] [D loss: 59.057190] [G loss: -3264.510986] [5batch time: 0.891853]\n",
      "[Epoch 0/1] [Batch 495/2000] [D loss: -284.905884] [G loss: -2851.551758] [5batch time: 0.919019]\n",
      "[Epoch 0/1] [Batch 500/2000] [D loss: -67.680359] [G loss: -2993.140869] [5batch time: 0.914307]\n",
      "[Epoch 0/1] [Batch 505/2000] [D loss: -121.543213] [G loss: -2863.645264] [5batch time: 0.906117]\n",
      "[Epoch 0/1] [Batch 510/2000] [D loss: -5.350281] [G loss: -2892.853027] [5batch time: 0.891749]\n",
      "[Epoch 0/1] [Batch 515/2000] [D loss: 190.536804] [G loss: -2999.570068] [5batch time: 0.893891]\n",
      "[Epoch 0/1] [Batch 520/2000] [D loss: -22.024902] [G loss: -2703.729980] [5batch time: 0.903927]\n",
      "[Epoch 0/1] [Batch 525/2000] [D loss: 19.864105] [G loss: -2653.088867] [5batch time: 0.901371]\n",
      "[Epoch 0/1] [Batch 530/2000] [D loss: -90.573700] [G loss: -2449.389648] [5batch time: 0.898651]\n",
      "[Epoch 0/1] [Batch 535/2000] [D loss: -172.889679] [G loss: -2275.146973] [5batch time: 0.906680]\n",
      "[Epoch 0/1] [Batch 540/2000] [D loss: 181.079834] [G loss: -2536.128174] [5batch time: 0.919713]\n",
      "[Epoch 0/1] [Batch 545/2000] [D loss: -31.671066] [G loss: -2228.645752] [5batch time: 0.880490]\n",
      "[Epoch 0/1] [Batch 550/2000] [D loss: -108.783478] [G loss: -2054.472900] [5batch time: 0.897531]\n",
      "[Epoch 0/1] [Batch 555/2000] [D loss: -303.586243] [G loss: -1770.758057] [5batch time: 0.881754]\n",
      "[Epoch 0/1] [Batch 560/2000] [D loss: -92.528275] [G loss: -1886.557007] [5batch time: 0.893153]\n",
      "[Epoch 0/1] [Batch 565/2000] [D loss: -62.213837] [G loss: -1822.944336] [5batch time: 0.899706]\n",
      "[Epoch 0/1] [Batch 570/2000] [D loss: -36.075317] [G loss: -1753.879883] [5batch time: 0.900171]\n",
      "[Epoch 0/1] [Batch 575/2000] [D loss: -219.175140] [G loss: -1478.224121] [5batch time: 0.879982]\n",
      "[Epoch 0/1] [Batch 580/2000] [D loss: -41.103867] [G loss: -1563.508545] [5batch time: 0.910795]\n",
      "[Epoch 0/1] [Batch 585/2000] [D loss: -234.908478] [G loss: -1278.167358] [5batch time: 0.890555]\n",
      "[Epoch 0/1] [Batch 590/2000] [D loss: 16.761684] [G loss: -1436.158569] [5batch time: 0.912028]\n",
      "[Epoch 0/1] [Batch 595/2000] [D loss: -234.843781] [G loss: -1095.358032] [5batch time: 0.903461]\n",
      "[Epoch 0/1] [Batch 600/2000] [D loss: -101.499336] [G loss: -1136.653320] [5batch time: 0.910965]\n",
      "[Epoch 0/1] [Batch 605/2000] [D loss: -55.297520] [G loss: -1092.485229] [5batch time: 0.913466]\n",
      "[Epoch 0/1] [Batch 610/2000] [D loss: -13.434410] [G loss: -1045.572266] [5batch time: 0.919825]\n",
      "[Epoch 0/1] [Batch 615/2000] [D loss: -45.289257] [G loss: -926.567566] [5batch time: 0.913597]\n",
      "[Epoch 0/1] [Batch 620/2000] [D loss: -105.522537] [G loss: -781.639343] [5batch time: 0.911621]\n",
      "[Epoch 0/1] [Batch 625/2000] [D loss: -27.873100] [G loss: -774.030762] [5batch time: 0.884811]\n",
      "[Epoch 0/1] [Batch 630/2000] [D loss: -74.648361] [G loss: -646.639282] [5batch time: 0.895694]\n",
      "[Epoch 0/1] [Batch 635/2000] [D loss: -21.413620] [G loss: -617.862488] [5batch time: 0.901314]\n",
      "[Epoch 0/1] [Batch 640/2000] [D loss: -95.364883] [G loss: -470.968750] [5batch time: 0.924639]\n",
      "[Epoch 0/1] [Batch 645/2000] [D loss: -10.518617] [G loss: -482.838928] [5batch time: 0.903042]\n",
      "[Epoch 0/1] [Batch 650/2000] [D loss: -78.698227] [G loss: -349.742218] [5batch time: 0.894924]\n",
      "[Epoch 0/1] [Batch 655/2000] [D loss: -47.194752] [G loss: -319.603271] [5batch time: 0.888066]\n",
      "[Epoch 0/1] [Batch 660/2000] [D loss: -42.760098] [G loss: -266.591156] [5batch time: 0.881325]\n",
      "[Epoch 0/1] [Batch 665/2000] [D loss: -44.899807] [G loss: -213.952560] [5batch time: 0.903475]\n",
      "[Epoch 0/1] [Batch 670/2000] [D loss: -32.310314] [G loss: -180.324493] [5batch time: 0.906941]\n",
      "[Epoch 0/1] [Batch 675/2000] [D loss: -13.105985] [G loss: -158.819107] [5batch time: 0.904388]\n",
      "[Epoch 0/1] [Batch 680/2000] [D loss: -20.030865] [G loss: -118.619530] [5batch time: 0.917257]\n",
      "[Epoch 0/1] [Batch 685/2000] [D loss: -6.812413] [G loss: -104.788445] [5batch time: 0.871516]\n",
      "[Epoch 0/1] [Batch 690/2000] [D loss: -13.944500] [G loss: -76.916794] [5batch time: 0.897256]\n",
      "[Epoch 0/1] [Batch 695/2000] [D loss: -9.566437] [G loss: -65.015366] [5batch time: 0.894542]\n",
      "[Epoch 0/1] [Batch 700/2000] [D loss: -8.765315] [G loss: -55.611122] [5batch time: 0.906675]\n",
      "[Epoch 0/1] [Batch 705/2000] [D loss: -2.180256] [G loss: -56.391903] [5batch time: 0.901848]\n",
      "[Epoch 0/1] [Batch 710/2000] [D loss: -10.498968] [G loss: -44.669949] [5batch time: 0.905142]\n",
      "[Epoch 0/1] [Batch 715/2000] [D loss: -5.225575] [G loss: -51.659737] [5batch time: 0.901948]\n",
      "[Epoch 0/1] [Batch 720/2000] [D loss: -2.653491] [G loss: -58.239330] [5batch time: 0.918104]\n",
      "[Epoch 0/1] [Batch 725/2000] [D loss: -10.707307] [G loss: -55.316547] [5batch time: 0.903680]\n",
      "[Epoch 0/1] [Batch 730/2000] [D loss: -7.113584] [G loss: -66.050850] [5batch time: 0.886100]\n",
      "[Epoch 0/1] [Batch 735/2000] [D loss: -1.153261] [G loss: -82.198631] [5batch time: 0.920312]\n",
      "[Epoch 0/1] [Batch 740/2000] [D loss: -2.489624] [G loss: -93.377831] [5batch time: 0.901933]\n",
      "[Epoch 0/1] [Batch 745/2000] [D loss: -8.250562] [G loss: -102.859840] [5batch time: 0.903836]\n",
      "[Epoch 0/1] [Batch 750/2000] [D loss: 0.029378] [G loss: -127.924355] [5batch time: 0.924909]\n",
      "[Epoch 0/1] [Batch 755/2000] [D loss: -7.573085] [G loss: -137.619247] [5batch time: 0.920081]\n",
      "[Epoch 0/1] [Batch 760/2000] [D loss: -12.390324] [G loss: -150.672760] [5batch time: 0.908278]\n",
      "[Epoch 0/1] [Batch 765/2000] [D loss: -18.616415] [G loss: -164.655106] [5batch time: 0.904327]\n",
      "[Epoch 0/1] [Batch 770/2000] [D loss: -13.518002] [G loss: -191.564438] [5batch time: 0.895089]\n",
      "[Epoch 0/1] [Batch 775/2000] [D loss: -10.893183] [G loss: -217.724716] [5batch time: 0.915789]\n",
      "[Epoch 0/1] [Batch 780/2000] [D loss: -14.304744] [G loss: -238.595566] [5batch time: 0.918258]\n",
      "[Epoch 0/1] [Batch 785/2000] [D loss: -21.144491] [G loss: -259.577606] [5batch time: 0.905576]\n",
      "[Epoch 0/1] [Batch 790/2000] [D loss: -15.266152] [G loss: -295.587860] [5batch time: 0.893297]\n",
      "[Epoch 0/1] [Batch 795/2000] [D loss: -27.581797] [G loss: -313.267181] [5batch time: 0.897977]\n",
      "[Epoch 0/1] [Batch 800/2000] [D loss: -13.401299] [G loss: -358.942535] [5batch time: 0.900664]\n",
      "[Epoch 0/1] [Batch 805/2000] [D loss: -59.847767] [G loss: -344.760834] [5batch time: 0.911623]\n",
      "[Epoch 0/1] [Batch 810/2000] [D loss: -47.580322] [G loss: -391.645996] [5batch time: 0.905253]\n",
      "[Epoch 0/1] [Batch 815/2000] [D loss: -57.966675] [G loss: -416.589569] [5batch time: 0.890836]\n",
      "[Epoch 0/1] [Batch 820/2000] [D loss: -83.866470] [G loss: -428.231049] [5batch time: 0.890992]\n",
      "[Epoch 0/1] [Batch 825/2000] [D loss: -32.682835] [G loss: -520.157227] [5batch time: 0.905314]\n",
      "[Epoch 0/1] [Batch 830/2000] [D loss: -80.501961] [G loss: -513.490662] [5batch time: 0.897961]\n",
      "[Epoch 0/1] [Batch 835/2000] [D loss: -38.690266] [G loss: -598.396973] [5batch time: 0.911627]\n",
      "[Epoch 0/1] [Batch 840/2000] [D loss: -42.035797] [G loss: -637.344666] [5batch time: 0.894798]\n",
      "[Epoch 0/1] [Batch 845/2000] [D loss: -2.536963] [G loss: -722.169861] [5batch time: 0.896559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 850/2000] [D loss: -50.385334] [G loss: -718.950806] [5batch time: 0.907542]\n",
      "[Epoch 0/1] [Batch 855/2000] [D loss: -103.970139] [G loss: -712.428040] [5batch time: 0.887082]\n",
      "[Epoch 0/1] [Batch 860/2000] [D loss: -20.397224] [G loss: -842.537048] [5batch time: 0.893337]\n",
      "[Epoch 0/1] [Batch 865/2000] [D loss: -37.454174] [G loss: -872.766785] [5batch time: 0.893759]\n",
      "[Epoch 0/1] [Batch 870/2000] [D loss: -70.321899] [G loss: -885.544434] [5batch time: 0.902352]\n",
      "[Epoch 0/1] [Batch 875/2000] [D loss: -41.195652] [G loss: -960.776550] [5batch time: 0.908836]\n",
      "[Epoch 0/1] [Batch 880/2000] [D loss: -131.632477] [G loss: -916.584106] [5batch time: 0.889803]\n",
      "[Epoch 0/1] [Batch 885/2000] [D loss: -94.097504] [G loss: -1003.889832] [5batch time: 0.904941]\n",
      "[Epoch 0/1] [Batch 890/2000] [D loss: -21.544722] [G loss: -1121.779297] [5batch time: 0.895882]\n",
      "[Epoch 0/1] [Batch 895/2000] [D loss: -13.370022] [G loss: -1176.402710] [5batch time: 0.902519]\n",
      "[Epoch 0/1] [Batch 900/2000] [D loss: -51.462173] [G loss: -1179.915894] [5batch time: 0.891059]\n",
      "[Epoch 0/1] [Batch 905/2000] [D loss: 39.959595] [G loss: -1316.776611] [5batch time: 0.914201]\n",
      "[Epoch 0/1] [Batch 910/2000] [D loss: -18.131409] [G loss: -1302.019897] [5batch time: 0.904488]\n",
      "[Epoch 0/1] [Batch 915/2000] [D loss: -62.979198] [G loss: -1292.924194] [5batch time: 0.914811]\n",
      "[Epoch 0/1] [Batch 920/2000] [D loss: -165.786926] [G loss: -1222.587158] [5batch time: 0.898997]\n",
      "[Epoch 0/1] [Batch 925/2000] [D loss: -35.058777] [G loss: -1388.032471] [5batch time: 0.904334]\n",
      "[Epoch 0/1] [Batch 930/2000] [D loss: 20.574409] [G loss: -1474.050171] [5batch time: 0.893539]\n",
      "[Epoch 0/1] [Batch 935/2000] [D loss: -54.344849] [G loss: -1420.379517] [5batch time: 0.895204]\n",
      "[Epoch 0/1] [Batch 940/2000] [D loss: -54.408180] [G loss: -1443.261108] [5batch time: 0.891500]\n",
      "[Epoch 0/1] [Batch 945/2000] [D loss: -102.576859] [G loss: -1410.465210] [5batch time: 0.916699]\n",
      "[Epoch 0/1] [Batch 950/2000] [D loss: -30.394646] [G loss: -1499.326782] [5batch time: 0.913316]\n",
      "[Epoch 0/1] [Batch 955/2000] [D loss: -13.834969] [G loss: -1522.877930] [5batch time: 0.903322]\n",
      "[Epoch 0/1] [Batch 960/2000] [D loss: -8.733238] [G loss: -1532.835327] [5batch time: 0.906432]\n",
      "[Epoch 0/1] [Batch 965/2000] [D loss: 16.240913] [G loss: -1553.430908] [5batch time: 0.895494]\n",
      "[Epoch 0/1] [Batch 970/2000] [D loss: 12.419106] [G loss: -1549.123901] [5batch time: 0.884711]\n",
      "[Epoch 0/1] [Batch 975/2000] [D loss: 35.390121] [G loss: -1557.836304] [5batch time: 0.879420]\n",
      "[Epoch 0/1] [Batch 980/2000] [D loss: -81.508667] [G loss: -1426.591064] [5batch time: 0.901877]\n",
      "[Epoch 0/1] [Batch 985/2000] [D loss: -0.940331] [G loss: -1492.421143] [5batch time: 0.901183]\n",
      "[Epoch 0/1] [Batch 990/2000] [D loss: 54.534691] [G loss: -1523.628540] [5batch time: 0.896505]\n",
      "[Epoch 0/1] [Batch 995/2000] [D loss: -102.568573] [G loss: -1344.067261] [5batch time: 0.896799]\n",
      "[Epoch 0/1] [Batch 1000/2000] [D loss: 54.482513] [G loss: -1472.653320] [5batch time: 1.018561]\n",
      "[Epoch 0/1] [Batch 1005/2000] [D loss: -69.783707] [G loss: -1316.381958] [5batch time: 0.901615]\n",
      "[Epoch 0/1] [Batch 1010/2000] [D loss: 7.315918] [G loss: -1354.068481] [5batch time: 0.879939]\n",
      "[Epoch 0/1] [Batch 1015/2000] [D loss: 49.875298] [G loss: -1359.249146] [5batch time: 0.904347]\n",
      "[Epoch 0/1] [Batch 1020/2000] [D loss: -39.373955] [G loss: -1225.603394] [5batch time: 0.879440]\n",
      "[Epoch 0/1] [Batch 1025/2000] [D loss: -8.829529] [G loss: -1207.894775] [5batch time: 0.882475]\n",
      "[Epoch 0/1] [Batch 1030/2000] [D loss: -31.174141] [G loss: -1139.845093] [5batch time: 0.883150]\n",
      "[Epoch 0/1] [Batch 1035/2000] [D loss: 24.779865] [G loss: -1143.786499] [5batch time: 0.904255]\n",
      "[Epoch 0/1] [Batch 1040/2000] [D loss: -77.983017] [G loss: -991.216248] [5batch time: 0.871963]\n",
      "[Epoch 0/1] [Batch 1045/2000] [D loss: 17.463326] [G loss: -1031.301758] [5batch time: 0.885132]\n",
      "[Epoch 0/1] [Batch 1050/2000] [D loss: -151.564774] [G loss: -812.231628] [5batch time: 0.905472]\n",
      "[Epoch 0/1] [Batch 1055/2000] [D loss: -16.387352] [G loss: -893.211548] [5batch time: 0.905700]\n",
      "[Epoch 0/1] [Batch 1060/2000] [D loss: 2.090899] [G loss: -858.692993] [5batch time: 0.898001]\n",
      "[Epoch 0/1] [Batch 1065/2000] [D loss: -36.221649] [G loss: -765.587585] [5batch time: 0.911167]\n",
      "[Epoch 0/1] [Batch 1070/2000] [D loss: 0.151119] [G loss: -745.865234] [5batch time: 0.904407]\n",
      "[Epoch 0/1] [Batch 1075/2000] [D loss: -52.064663] [G loss: -636.038269] [5batch time: 0.903549]\n",
      "[Epoch 0/1] [Batch 1080/2000] [D loss: -48.148323] [G loss: -580.197571] [5batch time: 0.894509]\n",
      "[Epoch 0/1] [Batch 1085/2000] [D loss: -31.042828] [G loss: -534.926453] [5batch time: 0.915200]\n",
      "[Epoch 0/1] [Batch 1090/2000] [D loss: -57.465092] [G loss: -444.319397] [5batch time: 0.901873]\n",
      "[Epoch 0/1] [Batch 1095/2000] [D loss: -9.686668] [G loss: -432.811249] [5batch time: 0.915207]\n",
      "[Epoch 0/1] [Batch 1100/2000] [D loss: -13.741685] [G loss: -366.359558] [5batch time: 0.909081]\n",
      "[Epoch 0/1] [Batch 1105/2000] [D loss: -20.298023] [G loss: -298.862762] [5batch time: 0.916965]\n",
      "[Epoch 0/1] [Batch 1110/2000] [D loss: -7.347049] [G loss: -250.414673] [5batch time: 0.913932]\n",
      "[Epoch 0/1] [Batch 1115/2000] [D loss: -17.653711] [G loss: -180.601105] [5batch time: 0.920814]\n",
      "[Epoch 0/1] [Batch 1120/2000] [D loss: -12.081748] [G loss: -128.547638] [5batch time: 0.921641]\n",
      "[Epoch 0/1] [Batch 1125/2000] [D loss: -14.269381] [G loss: -70.404610] [5batch time: 0.918701]\n",
      "[Epoch 0/1] [Batch 1130/2000] [D loss: -11.152356] [G loss: -20.899931] [5batch time: 0.917914]\n",
      "[Epoch 0/1] [Batch 1135/2000] [D loss: -8.251951] [G loss: 28.464224] [5batch time: 0.910291]\n",
      "[Epoch 0/1] [Batch 1140/2000] [D loss: 3.341818] [G loss: 64.312187] [5batch time: 0.889509]\n",
      "[Epoch 0/1] [Batch 1145/2000] [D loss: -3.970255] [G loss: 118.883354] [5batch time: 0.885573]\n",
      "[Epoch 0/1] [Batch 1150/2000] [D loss: -4.133856] [G loss: 160.392441] [5batch time: 0.896386]\n",
      "[Epoch 0/1] [Batch 1155/2000] [D loss: -8.248851] [G loss: 206.114365] [5batch time: 0.909494]\n",
      "[Epoch 0/1] [Batch 1160/2000] [D loss: -3.425524] [G loss: 237.996628] [5batch time: 0.882907]\n",
      "[Epoch 0/1] [Batch 1165/2000] [D loss: -3.484145] [G loss: 272.674164] [5batch time: 0.905996]\n",
      "[Epoch 0/1] [Batch 1170/2000] [D loss: 3.361165] [G loss: 293.432220] [5batch time: 0.887918]\n",
      "[Epoch 0/1] [Batch 1175/2000] [D loss: -2.684827] [G loss: 326.024323] [5batch time: 0.884748]\n",
      "[Epoch 0/1] [Batch 1180/2000] [D loss: 1.565068] [G loss: 341.571838] [5batch time: 0.875665]\n",
      "[Epoch 0/1] [Batch 1185/2000] [D loss: 8.457026] [G loss: 347.781189] [5batch time: 0.909359]\n",
      "[Epoch 0/1] [Batch 1190/2000] [D loss: 5.748767] [G loss: 357.939056] [5batch time: 0.887108]\n",
      "[Epoch 0/1] [Batch 1195/2000] [D loss: 20.361179] [G loss: 349.302582] [5batch time: 0.895560]\n",
      "[Epoch 0/1] [Batch 1200/2000] [D loss: 10.714446] [G loss: 357.105896] [5batch time: 0.893380]\n",
      "[Epoch 0/1] [Batch 1205/2000] [D loss: 6.065480] [G loss: 354.548401] [5batch time: 0.886323]\n",
      "[Epoch 0/1] [Batch 1210/2000] [D loss: -8.648037] [G loss: 360.908173] [5batch time: 0.899170]\n",
      "[Epoch 0/1] [Batch 1215/2000] [D loss: -1.692233] [G loss: 343.281494] [5batch time: 0.914659]\n",
      "[Epoch 0/1] [Batch 1220/2000] [D loss: -3.457977] [G loss: 323.482544] [5batch time: 0.887953]\n",
      "[Epoch 0/1] [Batch 1225/2000] [D loss: 1.966361] [G loss: 296.136078] [5batch time: 0.883902]\n",
      "[Epoch 0/1] [Batch 1230/2000] [D loss: -5.024627] [G loss: 277.012299] [5batch time: 0.912043]\n",
      "[Epoch 0/1] [Batch 1235/2000] [D loss: 76.436714] [G loss: 165.509766] [5batch time: 0.897317]\n",
      "[Epoch 0/1] [Batch 1240/2000] [D loss: 2.306478] [G loss: 198.445114] [5batch time: 0.883311]\n",
      "[Epoch 0/1] [Batch 1245/2000] [D loss: 4.551142] [G loss: 157.587997] [5batch time: 0.904524]\n",
      "[Epoch 0/1] [Batch 1250/2000] [D loss: 2.944483] [G loss: 115.337601] [5batch time: 0.892328]\n",
      "[Epoch 0/1] [Batch 1255/2000] [D loss: -7.025728] [G loss: 82.326385] [5batch time: 0.909110]\n",
      "[Epoch 0/1] [Batch 1260/2000] [D loss: 14.571806] [G loss: 19.327078] [5batch time: 0.910559]\n",
      "[Epoch 0/1] [Batch 1265/2000] [D loss: -8.833218] [G loss: -8.985461] [5batch time: 0.897721]\n",
      "[Epoch 0/1] [Batch 1270/2000] [D loss: -9.450585] [G loss: -54.104828] [5batch time: 0.886376]\n",
      "[Epoch 0/1] [Batch 1275/2000] [D loss: -10.190859] [G loss: -104.359978] [5batch time: 0.921049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 1280/2000] [D loss: -9.199570] [G loss: -153.148483] [5batch time: 0.904542]\n",
      "[Epoch 0/1] [Batch 1285/2000] [D loss: -15.171153] [G loss: -195.885452] [5batch time: 0.902480]\n",
      "[Epoch 0/1] [Batch 1290/2000] [D loss: -16.968481] [G loss: -243.492844] [5batch time: 0.880398]\n",
      "[Epoch 0/1] [Batch 1295/2000] [D loss: -39.509613] [G loss: -271.964020] [5batch time: 0.892226]\n",
      "[Epoch 0/1] [Batch 1300/2000] [D loss: -7.695563] [G loss: -358.232880] [5batch time: 0.900847]\n",
      "[Epoch 0/1] [Batch 1305/2000] [D loss: -12.231634] [G loss: -406.617004] [5batch time: 0.929706]\n",
      "[Epoch 0/1] [Batch 1310/2000] [D loss: -15.365938] [G loss: -459.221039] [5batch time: 0.878980]\n",
      "[Epoch 0/1] [Batch 1315/2000] [D loss: -9.917288] [G loss: -517.554504] [5batch time: 0.904765]\n",
      "[Epoch 0/1] [Batch 1320/2000] [D loss: -21.670959] [G loss: -557.828674] [5batch time: 0.883117]\n",
      "[Epoch 0/1] [Batch 1325/2000] [D loss: -15.388021] [G loss: -614.793152] [5batch time: 0.906604]\n",
      "[Epoch 0/1] [Batch 1330/2000] [D loss: -30.938599] [G loss: -648.687256] [5batch time: 0.902766]\n",
      "[Epoch 0/1] [Batch 1335/2000] [D loss: -23.864397] [G loss: -706.829346] [5batch time: 0.880973]\n",
      "[Epoch 0/1] [Batch 1340/2000] [D loss: -10.587844] [G loss: -769.056152] [5batch time: 0.880670]\n",
      "[Epoch 0/1] [Batch 1345/2000] [D loss: -38.074646] [G loss: -787.253540] [5batch time: 0.898961]\n",
      "[Epoch 0/1] [Batch 1350/2000] [D loss: 12.158484] [G loss: -883.751770] [5batch time: 0.895763]\n",
      "[Epoch 0/1] [Batch 1355/2000] [D loss: 13.499390] [G loss: -928.641052] [5batch time: 0.891637]\n",
      "[Epoch 0/1] [Batch 1360/2000] [D loss: 14.255138] [G loss: -970.316833] [5batch time: 0.913212]\n",
      "[Epoch 0/1] [Batch 1365/2000] [D loss: 7.987993] [G loss: -1002.392395] [5batch time: 0.904898]\n",
      "[Epoch 0/1] [Batch 1370/2000] [D loss: -36.512276] [G loss: -991.333618] [5batch time: 0.910412]\n",
      "[Epoch 0/1] [Batch 1375/2000] [D loss: -21.800890] [G loss: -1039.895264] [5batch time: 0.891495]\n",
      "[Epoch 0/1] [Batch 1380/2000] [D loss: -76.982109] [G loss: -1014.931458] [5batch time: 0.886869]\n",
      "[Epoch 0/1] [Batch 1385/2000] [D loss: -7.290504] [G loss: -1116.578003] [5batch time: 0.906828]\n",
      "[Epoch 0/1] [Batch 1390/2000] [D loss: -91.248734] [G loss: -1057.288452] [5batch time: 0.914501]\n",
      "[Epoch 0/1] [Batch 1395/2000] [D loss: 30.410671] [G loss: -1205.382812] [5batch time: 0.914748]\n",
      "[Epoch 0/1] [Batch 1400/2000] [D loss: 22.698425] [G loss: -1215.343872] [5batch time: 0.926618]\n",
      "[Epoch 0/1] [Batch 1405/2000] [D loss: -199.261642] [G loss: -1011.810974] [5batch time: 0.918980]\n",
      "[Epoch 0/1] [Batch 1410/2000] [D loss: 46.430443] [G loss: -1274.099976] [5batch time: 0.898467]\n",
      "[Epoch 0/1] [Batch 1415/2000] [D loss: 43.743156] [G loss: -1287.952759] [5batch time: 0.912174]\n",
      "[Epoch 0/1] [Batch 1420/2000] [D loss: 4.821194] [G loss: -1257.464722] [5batch time: 0.909533]\n",
      "[Epoch 0/1] [Batch 1425/2000] [D loss: -9.975342] [G loss: -1251.466919] [5batch time: 0.903502]\n",
      "[Epoch 0/1] [Batch 1430/2000] [D loss: -16.375881] [G loss: -1251.092773] [5batch time: 0.898127]\n",
      "[Epoch 0/1] [Batch 1435/2000] [D loss: 21.643520] [G loss: -1291.135376] [5batch time: 0.909364]\n",
      "[Epoch 0/1] [Batch 1440/2000] [D loss: -3.690536] [G loss: -1265.521118] [5batch time: 0.896516]\n",
      "[Epoch 0/1] [Batch 1445/2000] [D loss: 40.926262] [G loss: -1308.850952] [5batch time: 0.889675]\n",
      "[Epoch 0/1] [Batch 1450/2000] [D loss: 45.838821] [G loss: -1309.364502] [5batch time: 0.886951]\n",
      "[Epoch 0/1] [Batch 1455/2000] [D loss: 14.040447] [G loss: -1267.459961] [5batch time: 0.920402]\n",
      "[Epoch 0/1] [Batch 1460/2000] [D loss: -111.257111] [G loss: -1130.786987] [5batch time: 0.894755]\n",
      "[Epoch 0/1] [Batch 1465/2000] [D loss: 36.199558] [G loss: -1263.319580] [5batch time: 0.901055]\n",
      "[Epoch 0/1] [Batch 1470/2000] [D loss: -0.914265] [G loss: -1209.729858] [5batch time: 0.896040]\n",
      "[Epoch 0/1] [Batch 1475/2000] [D loss: 0.338200] [G loss: -1190.393677] [5batch time: 0.912651]\n",
      "[Epoch 0/1] [Batch 1480/2000] [D loss: -10.037426] [G loss: -1157.226929] [5batch time: 0.905204]\n",
      "[Epoch 0/1] [Batch 1485/2000] [D loss: -41.799343] [G loss: -1103.376953] [5batch time: 0.899879]\n",
      "[Epoch 0/1] [Batch 1490/2000] [D loss: 29.692472] [G loss: -1148.596558] [5batch time: 0.891207]\n",
      "[Epoch 0/1] [Batch 1495/2000] [D loss: -40.818443] [G loss: -1050.457642] [5batch time: 0.908706]\n",
      "[Epoch 0/1] [Batch 1500/2000] [D loss: -25.225235] [G loss: -1033.819458] [5batch time: 0.887510]\n",
      "[Epoch 0/1] [Batch 1505/2000] [D loss: -20.674339] [G loss: -1008.544556] [5batch time: 0.902883]\n",
      "[Epoch 0/1] [Batch 1510/2000] [D loss: 14.830435] [G loss: -1011.152527] [5batch time: 0.895806]\n",
      "[Epoch 0/1] [Batch 1515/2000] [D loss: -25.602291] [G loss: -937.264343] [5batch time: 0.905169]\n",
      "[Epoch 0/1] [Batch 1520/2000] [D loss: -39.025799] [G loss: -888.819153] [5batch time: 0.893618]\n",
      "[Epoch 0/1] [Batch 1525/2000] [D loss: -41.045219] [G loss: -847.266724] [5batch time: 0.902572]\n",
      "[Epoch 0/1] [Batch 1530/2000] [D loss: -12.765892] [G loss: -838.067810] [5batch time: 0.893951]\n",
      "[Epoch 0/1] [Batch 1535/2000] [D loss: 0.356852] [G loss: -811.792847] [5batch time: 0.889109]\n",
      "[Epoch 0/1] [Batch 1540/2000] [D loss: 9.669319] [G loss: -778.509949] [5batch time: 0.878846]\n",
      "[Epoch 0/1] [Batch 1545/2000] [D loss: -13.590738] [G loss: -715.711914] [5batch time: 0.903927]\n",
      "[Epoch 0/1] [Batch 1550/2000] [D loss: -7.665977] [G loss: -677.574585] [5batch time: 0.892877]\n",
      "[Epoch 0/1] [Batch 1555/2000] [D loss: -4.060595] [G loss: -635.769653] [5batch time: 0.900103]\n",
      "[Epoch 0/1] [Batch 1560/2000] [D loss: -9.816649] [G loss: -586.757812] [5batch time: 0.904264]\n",
      "[Epoch 0/1] [Batch 1565/2000] [D loss: -20.262808] [G loss: -531.822205] [5batch time: 0.887777]\n",
      "[Epoch 0/1] [Batch 1570/2000] [D loss: -12.135571] [G loss: -495.498016] [5batch time: 0.907542]\n",
      "[Epoch 0/1] [Batch 1575/2000] [D loss: -4.025439] [G loss: -459.885742] [5batch time: 0.883116]\n",
      "[Epoch 0/1] [Batch 1580/2000] [D loss: -9.726798] [G loss: -411.660309] [5batch time: 0.890098]\n",
      "[Epoch 0/1] [Batch 1585/2000] [D loss: -3.415404] [G loss: -375.364655] [5batch time: 0.895323]\n",
      "[Epoch 0/1] [Batch 1590/2000] [D loss: -8.762723] [G loss: -328.917389] [5batch time: 0.918665]\n",
      "[Epoch 0/1] [Batch 1595/2000] [D loss: -0.815592] [G loss: -296.252411] [5batch time: 0.893747]\n",
      "[Epoch 0/1] [Batch 1600/2000] [D loss: -9.181648] [G loss: -249.277664] [5batch time: 0.886399]\n",
      "[Epoch 0/1] [Batch 1605/2000] [D loss: -1.846859] [G loss: -219.703247] [5batch time: 0.901693]\n",
      "[Epoch 0/1] [Batch 1610/2000] [D loss: -7.553492] [G loss: -177.996628] [5batch time: 0.906869]\n",
      "[Epoch 0/1] [Batch 1615/2000] [D loss: -2.417455] [G loss: -149.057907] [5batch time: 0.895906]\n",
      "[Epoch 0/1] [Batch 1620/2000] [D loss: -1.397006] [G loss: -118.537071] [5batch time: 0.901943]\n",
      "[Epoch 0/1] [Batch 1625/2000] [D loss: 1.950750] [G loss: -92.054512] [5batch time: 0.884924]\n",
      "[Epoch 0/1] [Batch 1630/2000] [D loss: 4.229481] [G loss: -66.746750] [5batch time: 0.910392]\n",
      "[Epoch 0/1] [Batch 1635/2000] [D loss: 4.935879] [G loss: -41.421936] [5batch time: 0.902321]\n",
      "[Epoch 0/1] [Batch 1640/2000] [D loss: 5.008990] [G loss: -18.227671] [5batch time: 0.914126]\n",
      "[Epoch 0/1] [Batch 1645/2000] [D loss: 5.258856] [G loss: 3.478166] [5batch time: 0.901063]\n",
      "[Epoch 0/1] [Batch 1650/2000] [D loss: 5.340750] [G loss: 24.254505] [5batch time: 0.898546]\n",
      "[Epoch 0/1] [Batch 1655/2000] [D loss: 4.770493] [G loss: 45.338753] [5batch time: 0.881896]\n",
      "[Epoch 0/1] [Batch 1660/2000] [D loss: 4.939093] [G loss: 65.231712] [5batch time: 0.896684]\n",
      "[Epoch 0/1] [Batch 1665/2000] [D loss: 19.083231] [G loss: 70.423767] [5batch time: 0.886679]\n",
      "[Epoch 0/1] [Batch 1670/2000] [D loss: 2.491491] [G loss: 106.561089] [5batch time: 0.921374]\n",
      "[Epoch 0/1] [Batch 1675/2000] [D loss: 4.437916] [G loss: 123.062950] [5batch time: 0.878592]\n",
      "[Epoch 0/1] [Batch 1680/2000] [D loss: 2.216766] [G loss: 144.114227] [5batch time: 0.901809]\n",
      "[Epoch 0/1] [Batch 1685/2000] [D loss: 1.793432] [G loss: 161.617630] [5batch time: 0.896630]\n",
      "[Epoch 0/1] [Batch 1690/2000] [D loss: 1.203562] [G loss: 180.183014] [5batch time: 0.898061]\n",
      "[Epoch 0/1] [Batch 1695/2000] [D loss: -0.201593] [G loss: 198.851135] [5batch time: 0.884345]\n",
      "[Epoch 0/1] [Batch 1700/2000] [D loss: 0.575180] [G loss: 216.077103] [5batch time: 0.897071]\n",
      "[Epoch 0/1] [Batch 1705/2000] [D loss: -2.480932] [G loss: 237.782089] [5batch time: 0.880553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 1710/2000] [D loss: 19.121912] [G loss: 232.357529] [5batch time: 0.898061]\n",
      "[Epoch 0/1] [Batch 1715/2000] [D loss: 6.124456] [G loss: 259.982849] [5batch time: 0.872641]\n",
      "[Epoch 0/1] [Batch 1720/2000] [D loss: 1.529225] [G loss: 280.949707] [5batch time: 0.903325]\n",
      "[Epoch 0/1] [Batch 1725/2000] [D loss: 0.978690] [G loss: 297.282379] [5batch time: 0.915385]\n",
      "[Epoch 0/1] [Batch 1730/2000] [D loss: -0.912380] [G loss: 312.043549] [5batch time: 0.904616]\n",
      "[Epoch 0/1] [Batch 1735/2000] [D loss: -0.413338] [G loss: 325.781799] [5batch time: 0.906282]\n",
      "[Epoch 0/1] [Batch 1740/2000] [D loss: 12.197227] [G loss: 324.784760] [5batch time: 0.900841]\n",
      "[Epoch 0/1] [Batch 1745/2000] [D loss: 2.199239] [G loss: 344.365631] [5batch time: 0.878794]\n",
      "[Epoch 0/1] [Batch 1750/2000] [D loss: 9.074334] [G loss: 345.891785] [5batch time: 0.911001]\n",
      "[Epoch 0/1] [Batch 1755/2000] [D loss: -2.444327] [G loss: 363.500916] [5batch time: 0.899207]\n",
      "[Epoch 0/1] [Batch 1760/2000] [D loss: 11.595289] [G loss: 357.015137] [5batch time: 0.909745]\n",
      "[Epoch 0/1] [Batch 1765/2000] [D loss: 7.449687] [G loss: 365.248260] [5batch time: 0.896341]\n",
      "[Epoch 0/1] [Batch 1770/2000] [D loss: 39.591850] [G loss: 334.524780] [5batch time: 0.898875]\n",
      "[Epoch 0/1] [Batch 1775/2000] [D loss: -8.286444] [G loss: 384.349365] [5batch time: 0.902810]\n",
      "[Epoch 0/1] [Batch 1780/2000] [D loss: -2.331542] [G loss: 377.197266] [5batch time: 0.888852]\n",
      "[Epoch 0/1] [Batch 1785/2000] [D loss: 1.272203] [G loss: 369.545502] [5batch time: 0.910611]\n",
      "[Epoch 0/1] [Batch 1790/2000] [D loss: -7.175740] [G loss: 373.510956] [5batch time: 0.906211]\n",
      "[Epoch 0/1] [Batch 1795/2000] [D loss: 3.388201] [G loss: 359.945770] [5batch time: 0.870844]\n",
      "[Epoch 0/1] [Batch 1800/2000] [D loss: -4.441690] [G loss: 363.262207] [5batch time: 0.896423]\n",
      "[Epoch 0/1] [Batch 1805/2000] [D loss: 2.373601] [G loss: 349.738251] [5batch time: 0.910738]\n",
      "[Epoch 0/1] [Batch 1810/2000] [D loss: -8.178957] [G loss: 350.551361] [5batch time: 0.891289]\n",
      "[Epoch 0/1] [Batch 1815/2000] [D loss: 1.446504] [G loss: 328.912750] [5batch time: 0.894428]\n",
      "[Epoch 0/1] [Batch 1820/2000] [D loss: -0.371802] [G loss: 317.441864] [5batch time: 0.878794]\n",
      "[Epoch 0/1] [Batch 1825/2000] [D loss: 2.598742] [G loss: 298.052917] [5batch time: 0.880345]\n",
      "[Epoch 0/1] [Batch 1830/2000] [D loss: 7.320882] [G loss: 275.778656] [5batch time: 0.904377]\n",
      "[Epoch 0/1] [Batch 1835/2000] [D loss: -9.101180] [G loss: 271.807220] [5batch time: 0.892120]\n",
      "[Epoch 0/1] [Batch 1840/2000] [D loss: -2.412003] [G loss: 242.629623] [5batch time: 0.886153]\n",
      "[Epoch 0/1] [Batch 1845/2000] [D loss: -3.184499] [G loss: 220.458130] [5batch time: 0.889904]\n",
      "[Epoch 0/1] [Batch 1850/2000] [D loss: -8.740150] [G loss: 197.369797] [5batch time: 0.895119]\n",
      "[Epoch 0/1] [Batch 1855/2000] [D loss: -5.534642] [G loss: 166.716476] [5batch time: 0.894626]\n",
      "[Epoch 0/1] [Batch 1860/2000] [D loss: -2.593742] [G loss: 133.469696] [5batch time: 0.896152]\n",
      "[Epoch 0/1] [Batch 1865/2000] [D loss: -6.140476] [G loss: 106.313469] [5batch time: 0.916910]\n",
      "[Epoch 0/1] [Batch 1870/2000] [D loss: 5.963122] [G loss: 62.657040] [5batch time: 0.906026]\n",
      "[Epoch 0/1] [Batch 1875/2000] [D loss: -5.544048] [G loss: 37.488964] [5batch time: 0.911705]\n",
      "[Epoch 0/1] [Batch 1880/2000] [D loss: 1.094941] [G loss: -4.154036] [5batch time: 0.901197]\n",
      "[Epoch 0/1] [Batch 1885/2000] [D loss: -3.306621] [G loss: -38.965679] [5batch time: 0.873335]\n",
      "[Epoch 0/1] [Batch 1890/2000] [D loss: -4.582373] [G loss: -75.080620] [5batch time: 0.871105]\n",
      "[Epoch 0/1] [Batch 1895/2000] [D loss: -8.300694] [G loss: -114.685242] [5batch time: 0.890452]\n",
      "[Epoch 0/1] [Batch 1900/2000] [D loss: -7.095629] [G loss: -156.022858] [5batch time: 0.903146]\n",
      "[Epoch 0/1] [Batch 1905/2000] [D loss: -6.810951] [G loss: -199.608337] [5batch time: 0.882348]\n",
      "[Epoch 0/1] [Batch 1910/2000] [D loss: -9.809966] [G loss: -241.561935] [5batch time: 0.899690]\n",
      "[Epoch 0/1] [Batch 1915/2000] [D loss: -5.799212] [G loss: -288.546692] [5batch time: 0.906207]\n",
      "[Epoch 0/1] [Batch 1920/2000] [D loss: -13.314764] [G loss: -323.192474] [5batch time: 0.890126]\n",
      "[Epoch 0/1] [Batch 1925/2000] [D loss: -83.137276] [G loss: -296.135223] [5batch time: 0.889934]\n",
      "[Epoch 0/1] [Batch 1930/2000] [D loss: -34.928352] [G loss: -392.040741] [5batch time: 0.894175]\n",
      "[Epoch 0/1] [Batch 1935/2000] [D loss: -0.942394] [G loss: -472.248291] [5batch time: 0.902151]\n",
      "[Epoch 0/1] [Batch 1940/2000] [D loss: -14.672336] [G loss: -502.753967] [5batch time: 0.872839]\n",
      "[Epoch 0/1] [Batch 1945/2000] [D loss: 0.483396] [G loss: -561.233032] [5batch time: 0.888226]\n",
      "[Epoch 0/1] [Batch 1950/2000] [D loss: -3.832113] [G loss: -601.552795] [5batch time: 0.898275]\n",
      "[Epoch 0/1] [Batch 1955/2000] [D loss: -17.171667] [G loss: -629.091797] [5batch time: 0.897342]\n",
      "[Epoch 0/1] [Batch 1960/2000] [D loss: 8.185043] [G loss: -696.305542] [5batch time: 0.897267]\n",
      "[Epoch 0/1] [Batch 1965/2000] [D loss: 11.312706] [G loss: -733.492065] [5batch time: 0.888059]\n",
      "[Epoch 0/1] [Batch 1970/2000] [D loss: 12.400299] [G loss: -769.216980] [5batch time: 0.913311]\n",
      "[Epoch 0/1] [Batch 1975/2000] [D loss: -13.937389] [G loss: -773.339294] [5batch time: 0.896848]\n",
      "[Epoch 0/1] [Batch 1980/2000] [D loss: 17.353712] [G loss: -832.305664] [5batch time: 0.882438]\n",
      "[Epoch 0/1] [Batch 1985/2000] [D loss: 22.769047] [G loss: -861.060181] [5batch time: 0.885117]\n",
      "[Epoch 0/1] [Batch 1990/2000] [D loss: 7.169907] [G loss: -866.336914] [5batch time: 0.885898]\n",
      "[Epoch 0/1] [Batch 1995/2000] [D loss: -26.465942] [G loss: -851.325989] [5batch time: 0.884593]\n",
      "average time per picture = 0.897970400546257\n",
      "minutes per 100,000 pictures = 1496.6173342437617\n"
     ]
    }
   ],
   "source": [
    "# old logger\n",
    "#from utils import Logger\n",
    "\n",
    "#logger = Logger(model_name='wgangp1t1', data_name='200px')\n",
    "#num_batches = len(dataloader)\n",
    "n_epochs=1\n",
    "batches_done = 0\n",
    "### new logger\n",
    "start = time.time()\n",
    "batchtimes=[float()]\n",
    "\n",
    "#batch_size=100\n",
    "#print(latent_dim)\n",
    "\n",
    "print(batch_size)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "        ## monitor time\n",
    "       \n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "        # Generate a batch of images\n",
    "        fake_imgs = generator(z)\n",
    "        # Real images\n",
    "        real_validity = discriminator(real_imgs)\n",
    "        # Fake images\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "        # Adversarial loss\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        optimizer_G.zero_grad()\n",
    "        # Train the generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator(z)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            # Train on fake images\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "\n",
    "\n",
    "            #logger.log(d_loss, g_loss, epoch, batches_done, num_batches)\n",
    "            if batches_done % sample_interval/10 ==0:\n",
    "                log_value('g_loss', g_loss, batches_done)\n",
    "                log_value('d_loss', d_loss, batches_done)\n",
    "            \n",
    "            if batches_done % sample_interval == 0:\n",
    "                save_image(fake_imgs.data[:25], \"molpics500px/%d_d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "            end = time.time()\n",
    "            batchtime = end - start\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [5batch time: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), batchtime)\n",
    "            )\n",
    "            batchtimes.append(batchtime*batch_size/5)\n",
    "            start = time.time()\n",
    "            \n",
    "            batches_done += n_critic\n",
    "print(\"average time per picture = \" +str(np.mean(batchtimes)))\n",
    "print(\"minutes per 100,000 pictures = \"+str((np.mean(batchtimes)*100000)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try pre-resized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "path = \"./zinc100k500px/\"\n",
    "dirs = os.listdir( path )\n",
    "dirs = dirs[0:10000]\n",
    "#print(dirs)[1]\n",
    "newpath = \"./zinc100k500pxRe250px/\"\n",
    "final_size = 250\n",
    "\n",
    "def resize(final_size, newpath):\n",
    "    for item in dirs:\n",
    "        if os.path.isfile(path+item):\n",
    "            im = Image.open(path+item)\n",
    "            f, e = os.path.splitext(item)\n",
    "            \n",
    "            basename= (item.split(\"/\")[-1]).split(\".\")[0]\n",
    "            \n",
    "            imResize = im.resize((final_size,final_size), Image.ANTIALIAS)\n",
    "            new_im = Image.new(\"RGB\", (final_size, final_size))\n",
    "            new_im.paste(imResize)\n",
    "            #new_im.paste(im, ((final_size-new_image_size[0])//2, (final_size-new_image_size[1])//2))\n",
    "            #new_im.save(f + 'resized.jpg', 'JPEG', quality=90)\n",
    "\n",
    "            new_im.save(newpath + basename + 'resized.png', 'PNG', quality=95)\n",
    "\n",
    "resize(250,newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "path = \"./zinc100k500pxRe250px/\"\n",
    "\n",
    "dirs = os.listdir( path )\n",
    "dirs = dirs[0:10000]\n",
    "#print(dirs)[1]\n",
    "newpath = \"./zinc100k500pxRe250pxJPG/\"\n",
    "os.makedirs(newpath, exist_ok=True)\n",
    "\n",
    "def png2jpg(path, newpath):\n",
    "    for item in dirs:\n",
    "        if os.path.isfile(path+item):\n",
    "            im = Image.open(path+item)\n",
    "            f, e = os.path.splitext(item)\n",
    "            \n",
    "            basename= (item.split(\"/\")[-1]).split(\".\")[0]\n",
    "            new_im = Image.new(\"RGB\", (250, 250))\n",
    "            new_im.paste(im)\n",
    "            #new_im.paste(im, ((final_size-new_image_size[0])//2, (final_size-new_image_size[1])//2))\n",
    "            #new_im.save(f + 'resized.jpg', 'JPEG', quality=90)\n",
    "\n",
    "            new_im.save(newpath + basename + 'resized.jpeg', 'JPEG', quality=95)\n",
    "\n",
    "png2jpg(path, newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train one epoch for testing speeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(dataloader, batch_size, n_epochs):\n",
    "    start = time.time()\n",
    "    batches_done=0\n",
    "    batchtimes=[float()]\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, imgs in enumerate(dataloader):\n",
    "            ## monitor time\n",
    "\n",
    "            # Configure input\n",
    "            real_imgs = Variable(imgs.type(Tensor))\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_D.zero_grad()\n",
    "            # Sample noise as generator input\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator(z)\n",
    "            # Real images\n",
    "            real_validity = discriminator(real_imgs)\n",
    "            # Fake images\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "            # Adversarial loss\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            optimizer_G.zero_grad()\n",
    "            # Train the generator every n_critic steps\n",
    "            if i % n_critic == 0:\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "                # Generate a batch of images\n",
    "                fake_imgs = generator(z)\n",
    "                # Loss measures generator's ability to fool the discriminator\n",
    "                # Train on fake images\n",
    "                fake_validity = discriminator(fake_imgs)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "\n",
    "\n",
    "                #logger.log(d_loss, g_loss, epoch, batches_done, num_batches)\n",
    "                if batches_done % sample_interval/10 ==0:\n",
    "                    log_value('g_loss', g_loss, batches_done)\n",
    "                    log_value('d_loss', d_loss, batches_done)\n",
    "\n",
    "                if batches_done % sample_interval/100 == 0:\n",
    "                    save_image(fake_imgs.data[:25], \"molpics500px/%d_d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "                end = time.time()\n",
    "                batchtime = (end - start)/5\n",
    "\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [batch time: %f]\"\n",
    "                    % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), batchtime)\n",
    "                )\n",
    "                batchtimes.append(batchtime/batch_size)\n",
    "                start = time.time()\n",
    "\n",
    "                batches_done += n_critic\n",
    "    print(\"average time per picture = \" +str(np.mean(batchtimes)))\n",
    "    print(\"minutes per 100,000 pictures = \"+str((np.mean(batchtimes)*100000)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 0/167] [D loss: 15.302143] [G loss: -908.854858] [batch time: 0.247740]\n",
      "[Epoch 0/1] [Batch 5/167] [D loss: 17.402771] [G loss: -921.105347] [batch time: 0.189697]\n",
      "[Epoch 0/1] [Batch 10/167] [D loss: 21.201168] [G loss: -931.035400] [batch time: 0.189470]\n",
      "[Epoch 0/1] [Batch 15/167] [D loss: 16.733734] [G loss: -929.514221] [batch time: 0.193110]\n",
      "[Epoch 0/1] [Batch 20/167] [D loss: 15.244930] [G loss: -927.200256] [batch time: 0.190559]\n",
      "[Epoch 0/1] [Batch 25/167] [D loss: 12.640652] [G loss: -918.174927] [batch time: 0.189335]\n",
      "[Epoch 0/1] [Batch 30/167] [D loss: 14.900623] [G loss: -912.032715] [batch time: 0.188965]\n",
      "[Epoch 0/1] [Batch 35/167] [D loss: 18.028090] [G loss: -904.062317] [batch time: 0.188347]\n",
      "[Epoch 0/1] [Batch 40/167] [D loss: 18.058678] [G loss: -889.497314] [batch time: 0.187090]\n",
      "[Epoch 0/1] [Batch 45/167] [D loss: 13.704739] [G loss: -868.281372] [batch time: 0.186964]\n",
      "[Epoch 0/1] [Batch 50/167] [D loss: 10.296509] [G loss: -844.734985] [batch time: 0.188739]\n",
      "[Epoch 0/1] [Batch 55/167] [D loss: 11.396332] [G loss: -823.183472] [batch time: 0.190690]\n",
      "[Epoch 0/1] [Batch 60/167] [D loss: 4.286787] [G loss: -790.889709] [batch time: 0.189419]\n",
      "[Epoch 0/1] [Batch 65/167] [D loss: -0.831886] [G loss: -757.310669] [batch time: 0.190422]\n",
      "[Epoch 0/1] [Batch 70/167] [D loss: 3.159870] [G loss: -730.909973] [batch time: 0.195039]\n",
      "[Epoch 0/1] [Batch 75/167] [D loss: -1.650226] [G loss: -693.459106] [batch time: 0.189579]\n",
      "[Epoch 0/1] [Batch 80/167] [D loss: 2.882196] [G loss: -663.440796] [batch time: 0.189951]\n",
      "[Epoch 0/1] [Batch 85/167] [D loss: -0.929801] [G loss: -623.121765] [batch time: 0.191351]\n",
      "[Epoch 0/1] [Batch 90/167] [D loss: -3.854639] [G loss: -581.761597] [batch time: 0.188628]\n",
      "[Epoch 0/1] [Batch 95/167] [D loss: -2.997277] [G loss: -542.987427] [batch time: 0.189914]\n",
      "[Epoch 0/1] [Batch 100/167] [D loss: -3.968028] [G loss: -501.759186] [batch time: 0.189550]\n",
      "[Epoch 0/1] [Batch 105/167] [D loss: -6.519568] [G loss: -457.366547] [batch time: 0.192250]\n",
      "[Epoch 0/1] [Batch 110/167] [D loss: -6.665513] [G loss: -415.442505] [batch time: 0.189330]\n",
      "[Epoch 0/1] [Batch 115/167] [D loss: -4.799684] [G loss: -374.956299] [batch time: 0.191431]\n",
      "[Epoch 0/1] [Batch 120/167] [D loss: -5.348018] [G loss: -331.264252] [batch time: 0.190176]\n",
      "[Epoch 0/1] [Batch 125/167] [D loss: -5.553428] [G loss: -288.403198] [batch time: 0.191079]\n",
      "[Epoch 0/1] [Batch 130/167] [D loss: -4.689130] [G loss: -246.074265] [batch time: 0.191064]\n",
      "[Epoch 0/1] [Batch 135/167] [D loss: -5.238605] [G loss: -203.936874] [batch time: 0.189781]\n",
      "[Epoch 0/1] [Batch 140/167] [D loss: -5.124134] [G loss: -162.298462] [batch time: 0.189982]\n",
      "[Epoch 0/1] [Batch 145/167] [D loss: -3.978929] [G loss: -122.442940] [batch time: 0.189822]\n",
      "[Epoch 0/1] [Batch 150/167] [D loss: -3.500855] [G loss: -82.787521] [batch time: 0.190278]\n",
      "[Epoch 0/1] [Batch 155/167] [D loss: -2.827742] [G loss: -44.189377] [batch time: 0.190395]\n",
      "[Epoch 0/1] [Batch 160/167] [D loss: -2.288236] [G loss: -6.440554] [batch time: 0.189477]\n",
      "[Epoch 0/1] [Batch 165/167] [D loss: -1.589252] [G loss: 29.970081] [batch time: 0.190102]\n",
      "average time per picture = 0.0031046329907008577\n",
      "minutes per 100,000 pictures = 5.174388317834763\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./zinc100k500pxRe250pxJPG/\"\n",
    "batch_size = 60\n",
    "transforms_ = [ transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "dataloader = DataLoader(ImageDataset(folder_path, transforms_=transforms_),\n",
    "                        batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "train(dataloader, batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/home/jgmeyer2/vangan/gans/models\",exist_ok=True)\n",
    "PATH = \"/home/jgmeyer2/vangan/gans/models/\"\n",
    "modelid=\"250px_jpegs\"\n",
    "\n",
    "\n",
    "state_g = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': generator.state_dict(),\n",
    "    'optimizer': optimizer_G.state_dict()\n",
    "    }\n",
    "torch.save(state_g, PATH+\"g\"+modelid+\".model\")\n",
    "\n",
    "state_d = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': discriminator.state_dict(),\n",
    "    'optimizer': optimizer_D.state_dict()\n",
    "    }\n",
    "torch.save(state_d, PATH+\"d\"+modelid+\".model\")\n",
    "print(\"saved models @\")\n",
    "print(epoch)\n",
    "\n",
    "\n",
    "#def save_model(net, optim, ckpt_fname):\n",
    "#    state_dict = net.module.state_dict()\n",
    "#    for key in state_dict.keys():\n",
    "#        state_dict[key] = state_dict[key].cpu()\n",
    "#        torch.save({\n",
    "#            'epoch': epoch,                                                                                                                                                                                     \n",
    "#            'state_dict': state_dict,                                                                                                                                                                                \n",
    "#            'optimizer': optim},                                                                                                                                                                                     \n",
    "#            ckpt_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vangan",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
